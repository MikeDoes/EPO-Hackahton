<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP18773296B1" file="EP18773296NWB1.xml" lang="en" country="EP" doc-number="3676149" kind="B1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSK..HRIS..MTNORS..SM..................</B001EP><B003EP>*</B003EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  2100000/0</B007EP></eptags></B000><B100><B110>3676149</B110><B120><B121>EUROPEAN PATENT SPECIFICATION</B121></B120><B130>B1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>18773296.1</B210><B220><date>20180820</date></B220><B240><B241><date>20200228</date></B241></B240><B250>en</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>201762550796 P</B310><B320><date>20170828</date></B320><B330><ctry>US</ctry></B330><B310>201715700211</B310><B320><date>20170911</date></B320><B330><ctry>US</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20200708</date><bnum>202028</bnum></B430><B450><date>20211006</date><bnum>202140</bnum></B450><B452EP><date>20210416</date></B452EP></B400><B500><B510EP><classification-ipcr sequence="1"><text>B60W  50/14        20200101AFI20190308BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>B60W  50/00        20060101ALI20190308BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>B60K2370/119       20190501 LA20190517BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>B60W  50/0097      20130101 LI20181106BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>B60W2050/146       20130101 LA20190228BHEP        </text></classification-cpc><classification-cpc sequence="4"><text>B60K2370/16        20190501 LA20190503BHEP        </text></classification-cpc><classification-cpc sequence="5"><text>B60W  50/14        20130101 LI20181106BHEP        </text></classification-cpc><classification-cpc sequence="6"><text>B60K2370/175       20190501 LA20190530BHEP        </text></classification-cpc><classification-cpc sequence="7"><text>B60K2370/191       20190501 LA20190523BHEP        </text></classification-cpc><classification-cpc sequence="8"><text>B60K2370/193       20190501 LA20190503BHEP        </text></classification-cpc><classification-cpc sequence="9"><text>B60K2370/186       20190501 LA20190503BHEP        </text></classification-cpc><classification-cpc sequence="10"><text>B60K2370/166       20190501 LA20190524BHEP        </text></classification-cpc><classification-cpc sequence="11"><text>B60W2554/80        20200201 LA20200203RHEP        </text></classification-cpc><classification-cpc sequence="12"><text>B60W2555/60        20200201 LA20200203RHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>SYSTEME UND VERFAHREN ZUR KOMMUNIKATIONSABSICHT EINES AUTONOMEN FAHRZEUGS</B542><B541>en</B541><B542>SYSTEMS AND METHODS FOR COMMUNICATING INTENT OF AN AUTONOMOUS VEHICLE</B542><B541>fr</B541><B542>SYSTÈMES ET PROCÉDÉS D'INTENTION DE COMMUNICATION D'UN VÉHICULE AUTONOME</B542></B540><B560><B561><text>EP-A1- 3 050 771</text></B561><B561><text>WO-A1-2016/109829</text></B561><B561><text>US-A1- 2013 179 023</text></B561><B561><text>US-A1- 2014 222 277</text></B561><B561><text>US-B1- 8 676 431</text></B561></B560></B500><B600><B620EP><parent><cdoc><dnum><anum>21193381.7</anum></dnum><date>20210826</date></cdoc></parent></B620EP></B600><B700><B720><B721><snm>NIX, Molly Castle</snm><adr><str>c/o UATC, LLC
1455 Market Street
4th Floor</str><city>San Francisco, California 94103</city><ctry>US</ctry></adr></B721><B721><snm>CHIN, Sean</snm><adr><str>c/o UATC, LLC
1455 Market Street
4th Floor</str><city>San Francisco, California 94103</city><ctry>US</ctry></adr></B721></B720><B730><B731><snm>Uber Technologies, Inc.</snm><iid>101647588</iid><irf>PN837110EP</irf><adr><str>1455 Market Street, 4th Floor</str><city>San Francisco, California 94103</city><ctry>US</ctry></adr></B731></B730><B740><B741><snm>Grant, David Michael</snm><iid>101479810</iid><adr><str>Marks &amp; Clerk LLP 
15 Fetter Lane</str><city>London EC4A 1BW</city><ctry>GB</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B860><B861><dnum><anum>US2018047044</anum></dnum><date>20180820</date></B861><B862>en</B862></B860><B870><B871><dnum><pnum>WO2019046024</pnum></dnum><date>20190307</date><bnum>201910</bnum></B871></B870></B800></SDOBI>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001">FIELD</heading>
<p id="p0001" num="0001">The present invention relates generally to autonomous vehicles. More particularly, the present invention relates to communicating intent of an autonomous vehicle.</p>
<heading id="h0002">BACKGROUND</heading>
<p id="p0002" num="0002">An autonomous vehicle is a vehicle that is capable of sensing its environment and navigating with minimal or no human input. In particular, an autonomous vehicle can observe its surrounding environment using a variety of sensors and identify an appropriate motion path through such surrounding environment.</p>
<p id="p0003" num="0003"><patcit id="pcit0001" dnum="EP3050771A1"><text>EP3050771A1</text></patcit> proposes an information provision method that includes obtaining information about at least one of a property and a state of an occupant of a vehicle (A) of which at least part of driving operation is automatically controlled, and notifying the occupant of the vehicle of the information about the automated control on the basis of the obtained information. <patcit id="pcit0002" dnum="WO2016109829A1"><text>WO2016109829A1</text></patcit> proposes systems and methods of operating an autonomous vehicle to perform an autonomous maneuver. One system includes a human machine interface and an electronic controller electrically coupled to the human machine interface. The electronic controller includes an electronic processor configured to detect at least one driving condition and determine the autonomous maneuver based on the at least one driving condition. The electronic processor is also configured to display, via the human machine interface, a maneuver notification before performing the autonomous maneuver. The electronic processor is further configured to control the autonomous vehicle to perform the autonomous maneuver.</p>
<p id="p0004" num="0004"><patcit id="pcit0003" dnum="US8676431B1"><text>US8676431B1</text></patcit> proposes a vehicle having a plurality of control apparatuses, a user input, a geographic position component, an object detection apparatus, memory, and a display. A processor is also included and is programmed to receive the destination information, identify a route, and determine the current geographic location of the vehicle. The processor is also programmed to identify an object and object type based on object information received from the object detection apparatus and to determine at least one warning characteristic of the identified object based on at least one of: the object type, a detected proximity of the detected object to the vehicle, the location of the detected object relative to predetermined peripheral areas of the vehicle, the current geographic location of the vehicle, and the route. The<!-- EPO <DP n="2"> --> processor is also configured to select and display on the display an object warning image based on the at least one warning characteristic.</p>
<p id="p0005" num="0005"><patcit id="pcit0004" dnum="US2014222277A"><text>US2014/222277</text></patcit> proposes methods and control systems for automatically controlling operation of a vehicle. In one example, the control system includes an exterior sensor for sensing the environment outside the vehicle. A processor is in communication with the exterior sensor and configured to calculate a driving plan of the vehicle based at least partially on the sensed environment outside the vehicle. The processor is also configured to calculate a confidence level of the driving plan of the vehicle based at least partially on the sensed environment around the vehicle. The control system also includes a display in communication with the processor and configured to receive data from the processor and display a representation of at least one of the driving plan and the confidence level.</p>
<heading id="h0003">SUMMARY</heading>
<p id="p0006" num="0006">Aspects and advantages of embodiments of the present invention will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.</p>
<p id="p0007" num="0007">According to a first aspect of the present invention, there is provided a computer implemented method as set out in claim 1.</p>
<p id="p0008" num="0008">According to a second aspect of the present invention, there is provided a computing system as set out in claim 9.<!-- EPO <DP n="3"> --></p>
<p id="p0009" num="0009">According to a third aspect of the present invention, there is provided one or more non-transitory computer-readable media as set out in claim 10.</p>
<p id="p0010" num="0010">Other aspects of the present invention are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.</p>
<p id="p0011" num="0011">These and other features, aspects, and advantages of various embodiments of the present invention will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present invention and, together with the description, serve to explain the related principles.</p>
<heading id="h0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p0012" num="0012">Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:
<ul id="ul0001" list-style="none" compact="compact">
<li><figref idref="f0001">FIG. 1</figref> depicts a block diagram of an example computing system according to example embodiments of the present invention;</li>
<li><figref idref="f0002">FIG. 2</figref> depicts example graphical interface elements for communicating intent of an autonomous vehicle according to example embodiments of the present invention;<!-- EPO <DP n="4"> --></li>
<li><figref idref="f0003 f0009">FIGs. 3A-G</figref> depict example graphical interfaces for communicating intent of an autonomous vehicle according to example embodiments of the present invention; and</li>
<li><figref idref="f0010">FIG. 4</figref> depicts a flow chart diagram of an example method to communicate intent of an autonomous vehicle according to example embodiments of the present invention.</li>
</ul></p>
<heading id="h0005">DETAILED DESCRIPTION</heading>
<p id="p0013" num="0013">Generally, the present invention is directed to methods and systems for communicating intent of an autonomous vehicle. For example, an autonomy computing system of an autonomous vehicle can determine a motion plan for the autonomous vehicle based on maps and data received from sensors of the autonomous vehicle that describes the environment in which the autonomous vehicle is operating. Such a motion plan can include data that indicates intents of the autonomous vehicle. For example, the autonomous vehicle can plan to stop at an approaching intersection (e.g., based on a stop sign or signal determined from map and/or environmental sensor data). A computing system can receive, from the autonomy computing system of the autonomous vehicle, data indicating the intent of the autonomous vehicle (e.g., that the autonomous vehicle intends to stop at the approaching intersection). Multiple intents of the autonomous vehicle can be determined based on the data indicating the intent. For example, the data can indicate that the autonomous vehicle plans to stop at the approaching intersection because there is a red signal light and because pedestrians are currently crossing the path of the autonomous vehicle (e.g., at the intersection). From amongst the determined intents, an intent of the autonomous vehicle can be selected. In some implementations, the intent can be selected based on a predetermined hierarchy. For example, an intent indicating that the autonomous vehicle plans to stop at the approaching intersection because there is a red signal light can be selected instead of an intent indicating that the autonomous vehicle plans to stop because pedestrians are currently crossing the path of the autonomous vehicle because a predetermined hierarchy can indicate that intents associated with traffic signals should be selected in lieu of intents associated with pedestrian crossings. A determination can be made that the intent of the autonomous vehicle (e.g., planned deceleration associated with the approaching intersection) should be communicated to a passenger of the autonomous vehicle (e.g., operator or non-operator passenger of the autonomous vehicle). For example, a determination can be made that the deceleration associated with the approaching intersection is of sufficient magnitude (e.g., exceeds a predetermined threshold) that a passenger of the autonomous vehicle should be notified.<!-- EPO <DP n="5"> --></p>
<p id="p0014" num="0014">Responsive to a determination that a passenger of the autonomous vehicle should be notified of the intent of the autonomous vehicle, a graphical interface indicating the intent of the autonomous vehicle can be generated and displayed for viewing by the passenger of the autonomous vehicle. For example, a graphical interface indicating that the autonomous vehicle is (or is planning to) decelerate due to the approaching intersection can be generated and displayed to a passenger of the autonomous vehicle.</p>
<p id="p0015" num="0015">In some implementations, the graphical interface can include a map of the area surrounding the current location of the autonomous vehicle and can depict the location of the autonomous vehicle (e.g., within the surrounding area). In such implementations, the graphical interface can include a graphic overlay on the map indicating the intent of the autonomous vehicle. In some implementations, generating the graphical interface can include distilling or summarizing complex vehicle intent information into an easily understandable graphical element that quickly conveys to the passenger the intent of the autonomous vehicle. The graphical interface can include (e.g., as part of the graphic overlay) one or more elements depicting an event associated with the intent of the autonomous vehicle. For example, the autonomous vehicle can be stopped for two cars, and the graphic overlay can indicate that the autonomous vehicle is stopped (e.g., via a graphic corresponding to a stop sign) and the graphical interface can indicate that the associated event is waiting for two cars (e.g., via one or more textual elements). Similarly, the autonomous vehicle can be approaching a traffic signal and the graphical overlay can indicate the intent of the autonomous vehicle (e.g., continuing, stopping, stopped, or the like) and the associated event, for example, the status of the traffic signal (e.g., red, yellow, green, or the like). In some implementations, the graphical interface can indicate one or more parameters associated with the event. For example, the autonomous vehicle can be approaching a turn and the graphical interface can indicate a time until completion or execution of the turn. Similarly, the autonomous vehicle can be yielding (e.g., decelerating) in response to another vehicle, a pedestrian, a cyclist, or the like, and the graphical interface can indicate a speed of the vehicle, cyclist, pedestrian, or the like and/or a distance between the autonomous vehicle and the vehicle, cyclist, pedestrian, or the like.</p>
<p id="p0016" num="0016">In some implementations, the computing system can determine that the intent of the autonomous vehicle should be communicated to the passenger based on a determination that a frequency in the change of the intent indicated by the data received from the autonomy computing system meets a predetermined threshold. The data received from the autonomy computing system can indicate multiple intents within a short period of time (e.g., the<!-- EPO <DP n="6"> --> autonomy computing system can frequently alter its planned motion path for the autonomous vehicle based on, for example, changes in data received from sensors of the autonomous vehicle). For example, the data received from the autonomy computing system can indicate that the autonomous vehicle intends to decelerate in order to avoid a pedestrian, however, the data received from the autonomy computing system could subsequently indicate that the autonomous vehicle intends to maintain its current speed (e.g., because data received from sensors of the autonomous vehicle indicates the pedestrian has already cleared the path of the autonomous vehicle). Accordingly, in order to prevent multiple intents (e.g., possibly contradictory intents) from being communicated to the passenger of the autonomous vehicle within a short time period, a determination can be made with regards to the frequency in the change of the intent indicated by the data received from the autonomy computing system, and the intent of the autonomous vehicle can be communicated in response to a determination that the frequency in the change of the intent meets a predetermined threshold (e.g., the intent of the autonomous vehicle has remained constant for a threshold period of time). For example, the intent of the autonomous vehicle can be communicated in response to a determination that the frequency in the change of the intent is less than a predetermined threshold.</p>
<p id="p0017" num="0017">Additionally or alternatively, the computing system can determine that the intent of the autonomous vehicle should be communicated to the passenger based on a determination that a degree of confidence of an event associated with the intent indicated by the data received from the autonomy computing system meets a predetermined threshold. The data received from the autonomy computing system can indicate an event associated with the intent of the autonomous vehicle. For example, the data can indicate that the autonomous vehicle intends to decelerate because a pedestrian has entered the path of the autonomous vehicle. In certain scenarios, the autonomous vehicle can determine its intent, but the nature of the associated event can be unclear. For example, the autonomous vehicle can detect an object in its path and determine to decelerate in order to avoid the object, but the autonomous vehicle can be uncertain regarding the nature of the object (e.g., whether the object is a pedestrian, cyclist, or the like). This uncertainty can be indicated by the data received from the autonomy computing system. For example, the data can indicate the nature of the object to a degree of confidence. Because communicating erroneous information about the nature of the event to the passenger could undermine the confidence of the passenger in the autonomous vehicle, a determination can be made about whether to communicate the intent of the autonomous vehicle based on a determination that the degree of confidence<!-- EPO <DP n="7"> --> meets a predetermined threshold (e.g., the intent can be conveyed when the confidence regarding the event exceeds the predetermined threshold).</p>
<p id="p0018" num="0018">The computing system can include a human machine interface device (e.g., a mobile device, tablet computing device, or the like) that can be viewable by a passenger of the autonomous vehicle. The device can include a display (e.g., for viewing by the passenger), one or more processors, and memory. The memory can include instructions that when executed by the processor(s) cause the device to perform one or more of the operations described herein.</p>
<p id="p0019" num="0019">The systems and methods described herein can provide a number of technical effects and benefits. For example, the systems and methods described herein can communicate the intent of an autonomous vehicle to its passenger thereby allowing the passenger to anticipate impending changes in force and increasing passenger comfort. Additionally or alternatively, by communicating the intent of an autonomous vehicle to its passenger, the systems and methods described herein can demonstrate environmental understanding and driving competence of the autonomous vehicle to its passenger thereby reassuring the passenger that the autonomous vehicle is correctly interpreting its environment and acting appropriately.</p>
<p id="p0020" num="0020">With reference now to the Figures, example embodiments of the present invention will be discussed in further detail.</p>
<p id="p0021" num="0021"><figref idref="f0001">FIG. 1</figref> depicts a block diagram of an example computing system according to example embodiments of the present invention. Referring to <figref idref="f0001">FIG. 1</figref>, the example computing system includes autonomous vehicle 10, central computing system 170, and user computing device 175 that are communicatively coupled over one or more communication networks 180. Autonomous vehicle 10 can include one or more sensors 101, autonomy computing system 102, one or more vehicle controls 107, and human machine interface device 150.</p>
<p id="p0022" num="0022">Human machine interface device 150 can enable communication, control, and/or other interface actions to occur between autonomous vehicle 10 and a human (e.g., a passenger located within autonomous vehicle 10). Human machine interface device 150 can be communicatively coupled to autonomy computing system 102 to enable exchange of data, instructions, and/or requests between system 102 and device 150.</p>
<p id="p0023" num="0023">Human machine interface device 150 can include or be implemented by one or more computing devices that are operatively connected. Human machine interface device 150 can be an embedded computing device or a stand-alone computing device. In one particular example, human machine interface device 150 can be a tablet computing device<!-- EPO <DP n="8"> --> that is positioned within autonomous vehicle 10 for viewing by a passenger (e.g., within a rear seat area of autonomous vehicle 10).</p>
<p id="p0024" num="0024">Human machine interface device 150 can include one or more processors 152, memory 154, event detector 156, display 158, one or more user input components 160, one or more sensors 162, and navigational system 164. Processor(s) 152 can be any suitable processing device (e.g., a processor core, a microprocessor, an application-specific integrated circuit (ASIC), a field-programmable gate array (FPGA), a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Memory 154 can include one or more non-transitory computer-readable storage media, such as random-access memory (RAM), read-only memory (ROM), electrically erasable programmable read-only memory (EEPROM), erasable programmable read-only memory (EPROM), one or more memory devices, flash memory devices, etc., and combinations thereof.</p>
<p id="p0025" num="0025">Memory 154 can store information that can be accessed by processor(s) 152. For instance, memory 154 (e.g., one or more non-transitory computer-readable storage mediums, memory devices) can store data that can be obtained, received, accessed, written, manipulated, created, and/or stored. Memory 154 can also store computer-readable instructions that can be executed by processor(s) 152. The instructions can be software written in any suitable programming language or can be implemented in hardware. Additionally, or alternatively, the instructions can be executed in logically and/or virtually separate threads on processor(s) 152. For example, memory 154 can store instructions that when executed by processor(s) 152 cause processor(s) 152 to perform any of the operations and/or functions described herein.</p>
<p id="p0026" num="0026">In some implementations, human machine interface device 150 can include display device 158 (e.g., a touch-sensitive display device) and/or other input/output components 160 that provide an interactive user interface. For example, display device 158 can be a rear-seat display device that is accessible by a passenger that is located in a rear seat of autonomous vehicle 10.</p>
<p id="p0027" num="0027">In some implementations, in addition or alternatively to human machine interface device 150, the systems and methods of the present invention can include or leverage user computing device 175 that is associated with the passenger. For example, in some implementations, in addition or alternatively to the display of the user interface by human machine interface device 150, the interactive user interface can be provided on or accessible via a display of user computing device 175. User computing device 175 can be<!-- EPO <DP n="9"> --> communicatively connected to human machine interface device 150 via a local area network such as a short range wireless connection (e.g., a Bluetooth, ZigBee, near-field communication (NFC), infrared, etc.) or other forms of connections (e.g., hardwiring). As examples, user computing device 175 can be a smartphone, tablet computing device, wearable computing device, portable gaming device, hand-held display screen, or other form of computing device.</p>
<p id="p0028" num="0028">In yet further implementations, certain operations described herein can be performed by central computing system 170 that is remotely located to autonomous vehicle 10 and in communication with autonomous vehicle 10 over network(s) 180 (e.g., cellular data networks, satellite communication networks, wide area networks, etc.). As an example, central computing system 170 can include one or more server computing devices. In the event that plural server computing devices are used, the server computing devices can be arranged according to a parallel computing architecture, a sequential computing architecture, or combinations thereof. In some implementations, central computing system 170 can provide control, monitoring, management, and/or other functionality for a fleet of autonomous vehicles including autonomous vehicle 10.</p>
<p id="p0029" num="0029">Network(s) 180 can be any type of network or combination of networks that allows for communication between devices. In some embodiments, network(s) 180 can include one or more of a local area network, wide area network, the Internet, secure network, cellular network, mesh network, peer-to-peer communication link, and/or some combination thereof, and can include any number of wired or wireless links. Communication over network(s) 180 can be accomplished, for instance, via a network interface using any type of protocol, protection scheme, encoding, format, packaging, etc.</p>
<p id="p0030" num="0030">In accordance with aspects of the invention, a computing system of autonomous vehicle 10 can be configured to communicate intent of autonomous vehicle 10 to a passenger of autonomous vehicle 10 (e.g., an operator and/or non-operator passenger). For example, human machine interface device 150 can receive, from autonomy computing system 102, data indicating an intent of autonomous vehicle 10 (e.g., intent to perform a driving maneuver) and can determine, based on the data indicating the intent, that the intent of autonomous vehicle 10 should be communicated to a passenger of autonomous vehicle 10. Responsive to determining that the intent of autonomous vehicle 10 should be communicated to the passenger, human machine interface device 150 can generate a graphical interface indicating the intent of autonomous vehicle 10 and can provide the graphical interface for display (e.g., via display 158) for viewing by the passenger. In some implementations,<!-- EPO <DP n="10"> --> human machine interface device 150 can generate or otherwise trigger an audible signal for the passenger (e.g., a signal indicating the intent, a signal alerting the passenger to view display 158 for information regarding the intent, or the like).</p>
<p id="p0031" num="0031">In some implementations, human machine interface device 150 can determine multiple intents of autonomous vehicle 10 based on the data received from autonomy computing system 102. For example, the data received from autonomy computing system 102 can indicate autonomous vehicle 10 plans to stop at an approaching intersection because there is a red signal light and because pedestrians are currently crossing the path of autonomous vehicle 10 (e.g., at the intersection). In such implementations, human machine interface device 150 can select, from amongst the intents, an intent to communicate to the passenger of autonomous vehicle 10. In some implementations, human machine interface device 150 can be configured to select the intent to be communicated to the passenger based on a predetermined hierarchy. For example, human machine interface device 150 can be configured to select an intent indicating that autonomous vehicle 10 plans to stop at the approaching intersection because there is a red signal light instead of an intent indicating that autonomous vehicle 10 plans to stop because pedestrians are currently crossing the path of autonomous vehicle 10 because a predetermined hierarchy can indicate that intents associated with traffic signals should be selected in lieu of intents associated with pedestrian crossings.</p>
<p id="p0032" num="0032">In some implementations, human machine interface device 150 can be configured to determine that the intent of autonomous vehicle 10 should be communicated to the passenger based on a determination that a frequency in the change of the intent indicated by the data received from autonomy computing system 102 meets a predetermined threshold. The data received from autonomy computing system 102 can indicate multiple intents within a short period of time (e.g., autonomy computing system 102 can frequently alter its planned motion path for autonomous vehicle 10 based on, for example, changes in data received from sensor(s) 101). For example, the data received from autonomy computing system 102 can indicate that autonomous vehicle 10 intends to decelerate in order to avoid a pedestrian, however, the data received from autonomy computing system 102 could subsequently indicate that autonomous vehicle 10 intends to maintain its current speed (e.g., because data received from sensor(s) 101 indicates the pedestrian has already cleared the path of autonomous vehicle 10). Accordingly, in order to prevent multiple intents (e.g., possibly contradictory intents) from being communicated to the passenger of autonomous vehicle 10 within a short time period, human machine interface device 150 can be configured to make a determination with regards to the frequency in the change of the intent indicated by the data<!-- EPO <DP n="11"> --> received from autonomy computing system 102, and the intent of autonomous vehicle 10 can be communicated in response to a determination that the frequency in the change of the intent meets a predetermined threshold (e.g., the intent of autonomous vehicle 10 has remained constant for a threshold period of time). For example, the intent of autonomous vehicle 10 can be communicated in response to a determination that the frequency in the change of the intent is less than a predetermined threshold.</p>
<p id="p0033" num="0033">Additionally or alternatively, human machine interface device 150 can be configured to determine that the intent of autonomous vehicle 10 should be communicated to the passenger based on a determination that a degree of confidence of an event associated with the intent indicated by the data received from autonomy computing system 102 meets a predetermined threshold. The data received from autonomy computing system 102 can indicate an event associated with the intent of autonomous vehicle 10. For example, the data can indicate that autonomous vehicle 10 intends to decelerate because a pedestrian has entered the path of autonomous vehicle 10. In certain scenarios, autonomous vehicle 10 can determine its intent, but the nature of the associated event can be unclear. For example, autonomous vehicle 10 can detect an object in its path and determine to decelerate in order to avoid the object, but autonomous vehicle 10 can be uncertain regarding the nature of the object (e.g., whether the object is a pedestrian, cyclist, or the like). This uncertainty can be indicated by the data received from autonomy computing system 102. For example, the data can indicate the nature of the object to a degree of confidence. Because communicating erroneous information about the nature of the event to the passenger could undermine the confidence of the passenger in autonomous vehicle 10, human machine interface device 150 can be configured to make a determination about whether to communicate the intent of autonomous vehicle 10 based on a determination that the degree of confidence meets a predetermined threshold (e.g., the intent can be conveyed when the confidence regarding the event exceeds the predetermined threshold).</p>
<p id="p0034" num="0034">The intent can be an intent to perform a driving maneuver. For example, the intent could be that autonomous vehicle 10 intends to change lanes, decelerate (e.g., to yield to and/or stop for a traffic sign or signal, approaching vehicle, pedestrian, cyclist, unclassified object or the like), merge into traffic, pull alongside a curb or roadside, perform a left- or right-hand turn (e.g., a protected or unprotected turn), deviate from a typical pathway within its lane and/or without substantially intruding an adjacent lane in order to avoid an object or obstruction, decelerate or alter its speed based on a context-based speed limit (e.g., a school zone), or other driving maneuver. In some implementations, human machine interface device<!-- EPO <DP n="12"> --> 150 can be configured to distill or summarize complex vehicle intent information such that it is easily understandable by a passenger. For example, as indicated above, human machine interface device 150 can be configured to generate a graphical interface indicating the intent of autonomous vehicle 10. The graphical interface can include one or more elements that distill or summarize complex vehicle intent information such that it is easily understandable by a passenger. For example, <figref idref="f0002">FIG. 2</figref> depicts example graphical interface elements for communicating intent of an autonomous vehicle according to example embodiments of the present invention.</p>
<p id="p0035" num="0035">Referring to <figref idref="f0002">FIG. 2</figref>, graphics 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, and 224 include elements that distill or summarize complex vehicle intent information such that it is easily understandable by a passenger. In some implementations, the one or more elements of a graphic indicating intent can include elements depicting an event associated with the intent. For example, graphic 202 can depict an intent of autonomous vehicle 10 to decelerate in response to a red traffic signal. Similarly, graphic 204 can depict an intent of autonomous vehicle 10 to remain stopped for a red traffic signal; graphic 204 can depict an intent of autonomous vehicle 10 to maintain its current velocity in response to a yellow traffic signal; graphic 208 can depict an intent of autonomous vehicle 10 to maintain its current velocity in response to a green traffic signal; graphics 210 and 212 can depict an intent of autonomous vehicle 10 to perform a left-hand turn; graphics 214 and 216 can depict an intent of autonomous vehicle 10 to perform a right-hand turn; graphic 218 can depict an intent of autonomous vehicle 10 to remain stopped for two other vehicles; graphic 220 can depict an intent of autonomous vehicle 10 to remain stopped or decelerate for another vehicle; graphic 222 can depict an intent of autonomous vehicle 10 to remain stopped or decelerate for a cyclist; and graphic 224 can depict an intent of autonomous vehicle 10 to remain stopped or decelerate for a pedestrian. In some implementations, the one or more elements of a graphic indicating intent can include elements that provide contextual information associated with the intent and/or associated event. For example, graphics 210, 212, and/or 214 can indicate a time until the turn is completed; graphic 212 can indicate a margin between autonomous vehicle 10 and an approaching vehicle; graphics 214 and/or 216 can indicate a velocity and/or acceleration status of an approaching vehicle; and graphics 220, 222, and/or 224 can indicate a distance to and/or velocity of another vehicle, a cyclist, and/or a pedestrian, respectively.</p>
<p id="p0036" num="0036">In some implementations, human machine interface device 150 can be configured to generate a graphical interface that includes a map depicting the current location of<!-- EPO <DP n="13"> --> autonomous vehicle 10. In such implementations, the graphical interface can include a graphic overlay on the map indicating an intent of autonomous vehicle 10. For example, <figref idref="f0003 f0009">FIGs. 3A-G</figref> depict example graphical interfaces for communicating intent of an autonomous vehicle according to example embodiments of the present invention.</p>
<p id="p0037" num="0037">Referring to <figref idref="f0003">FIG. 3A</figref>, graphical interface 302 can include portion 304 and map 306. Portion 304 can indicate a status of autonomous vehicle 10 on a planned route (e.g., a distance to the next turn on the planned route, or the like). Map 306 can include element 308, which can depict the current location of autonomous vehicle 10 (e.g., within its surrounding environment). As indicated above, human machine interface device 150 can be configured to generate a graphical interface that includes a graphic overlay on the map indicating an intent of autonomous vehicle 10. For example, referring to <figref idref="f0004">FIG. 3B</figref>, graphical interface 310 can include graphic overlay 312 indicating an intent of autonomous vehicle 10 to remain stopped and/or decelerate in response to a detected cyclist. Similarly, referring to <figref idref="f0005">FIG. 3C</figref>, graphical interface 314 can include graphic overlay 316 indicating an intent of autonomous vehicle 10 to remain stopped and/or decelerate in response to a red traffic signal; referring to <figref idref="f0006">FIG. 3D</figref>, graphical interface 318 can include graphic overlay 320 indicating an intent of autonomous vehicle 10 to execute a right-hand turn; referring to <figref idref="f0007">FIG. 3E</figref>, graphical interface 322 can include graphic overlay 324 indicating an intent of autonomous vehicle 10 to abort a planned lane change (e.g., in response to the presence of another vehicle in the lane); referring to <figref idref="f0008">FIG. 3F</figref>, graphical interface 326 can include graphic overlay 328 indicating an intent of autonomous vehicle 10 to execute a lane change (e.g., in order to pass an approaching vehicle in its current lane); and referring to <figref idref="f0009">FIG. 3G</figref>, graphical interface 330 can include graphic overlay 332 indicating an intent of autonomous vehicle 10 to execute a lane change.</p>
<p id="p0038" num="0038"><figref idref="f0010">FIG. 4</figref> depicts a flow chart diagram of an example method to communicate intent of an autonomous vehicle according to example embodiments of the present invention. Referring to <figref idref="f0010">FIG. 4</figref>, at (402), data indicating one or more intents of an autonomous vehicle can be received. For example, human machine interface device 150 can receive data indicating one or more intents of autonomous vehicle 10 from autonomy computing system 102. At (404), one or more intents of the autonomous vehicle can be determined. For example, human machine interface device 150 can determine, based on the data received from autonomy computing system 102, one or more intents of autonomous vehicle 10. At (406), an intent of the autonomous vehicle can be selected. For example, human machine interface device 150 can select, from amongst the determined intent(s) (e.g., based on a predetermined hierarchy), an intent to communicate to a passenger of autonomous vehicle 10.<!-- EPO <DP n="14"> --> At (408), a determination that the intent should be communicated to a passenger of the autonomous vehicle can be made. For example, human machine interface device 150 can determine to communicate the selected intent of autonomous vehicle 10 to a passenger of autonomous vehicle 10 (e.g., based on a determination that a frequency in the change of the intent meets a predetermined threshold, a determination that a degree of confidence of an event associated with the intent meets a predetermined threshold, or the like). At (410), a graphical interface indicating the intent of the autonomous vehicle can be generated. For example, human machine interface device 150 can generate a graphical interface similar to graphical interfaces 310, 314, 318, 322, 326, and/or 330. At (412), the graphical interface indicating the intent of the autonomous vehicle can be provided for display for viewing by the passenger of the autonomous vehicle. For example, human machine interface device 150 can provide a graphical interface similar to graphical interfaces 310, 314, 318, 322, 326, and/or 330 (e.g., via display 158) for viewing by a passenger of autonomous vehicle 10.</p>
<p id="p0039" num="0039">The technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For instance, processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.</p>
<p id="p0040" num="0040">While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the invention. Accordingly, the subject invention does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="15"> -->
<claim id="c-en-01-0001" num="0001">
<claim-text>A computer-implemented method to communicate intent of an autonomous vehicle, the method comprising:
<claim-text>receiving (402), by a computing system that comprises one or more computing devices and from an autonomy computing system of an autonomous vehicle, data indicating an intent of the autonomous vehicle to perform a driving maneuver;</claim-text>
<claim-text>determining (408), by the computing system and based at least in part on the data indicating the intent, that the intent of the autonomous vehicle should be communicated to a passenger of the autonomous vehicle;</claim-text>
<claim-text>wherein determining that the intent of the autonomous vehicle should be communicated to the passenger comprises determining that a degree of confidence of an event associated with the intent meets a predetermined threshold; and</claim-text>
<claim-text>responsive to determining that the intent of the autonomous vehicle should be communicated to the passenger:
<claim-text>generating (410), by the computing system, a graphical interface (302, 310, 314, 318, 322, 326, 330) indicating the intent of the autonomous vehicle; and</claim-text>
<claim-text>providing (412) for display, by the computing system and for viewing by the passenger, the graphical interface (302, 310, 314, 318, 322, 326, 330).</claim-text></claim-text></claim-text></claim>
<claim id="c-en-01-0002" num="0002">
<claim-text>The computer-implemented method of claim 1, wherein the intent comprises at least one of changing lanes, decelerating for a traffic signal, decelerating for an approaching vehicle, decelerating for a pedestrian, or decelerating for a cyclist.</claim-text></claim>
<claim id="c-en-01-0003" num="0003">
<claim-text>The computer-implemented method of claim 1, further comprising:
<claim-text>determining (404), by the computing system and based at least in part on the data indicating the intent, a plurality of intents of the autonomous vehicle; and</claim-text>
<claim-text>selecting (406), by the computing system, from amongst the plurality of intents, and based at least in part on a predetermined hierarchy, the intent of the autonomous vehicle.</claim-text></claim-text></claim>
<claim id="c-en-01-0004" num="0004">
<claim-text>The computer-implemented method of claim 3, wherein:
<claim-text>the plurality of intents indicate that the autonomous vehicle is stopped or stopping for at least a first event and a second event; and</claim-text>
<claim-text>selecting (406) the intent of the autonomous vehicle comprises determining a position of the first event relative to the second event in the predetermined hierarchy.</claim-text><!-- EPO <DP n="16"> --></claim-text></claim>
<claim id="c-en-01-0005" num="0005">
<claim-text>The computer-implemented method of claim 1, wherein determining (408) that the intent of the autonomous vehicle should be communicated to the passenger comprises determining that a frequency in change of the intent meets a predetermined threshold.</claim-text></claim>
<claim id="c-en-01-0006" num="0006">
<claim-text>The computer-implemented method of claim 1, wherein generating (410) the graphical interface (302, 310, 314, 318, 322, 326, 330) comprises generating an interface comprising:<br/>
a map (306) depicting a current location of the autonomous vehicle; and a graphic overlay (312, 316, 320, 324, 328, 332) on the map (306) indicating the intent of the autonomous vehicle.</claim-text></claim>
<claim id="c-en-01-0007" num="0007">
<claim-text>The computer-implemented method of claim 1, wherein (410) generating the graphical interface (302, 310, 314, 318, 322, 326, 330) comprises generating an interface comprising one or more elements depicting an event associated with the intent of the autonomous vehicle.</claim-text></claim>
<claim id="c-en-01-0008" num="0008">
<claim-text>The computer-implemented method of claim 1, further comprising, responsive to determining that the intent of the autonomous vehicle should be communicated to the passenger, generating an audible signal for the passenger.</claim-text></claim>
<claim id="c-en-01-0009" num="0009">
<claim-text>A computing system comprising:
<claim-text>one or more processors; and</claim-text>
<claim-text>one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing system to perform a method according to any one of the preceding claims.</claim-text></claim-text></claim>
<claim id="c-en-01-0010" num="0010">
<claim-text>One or more non-transitory computer-readable media that collectively store instructions that, when executed by one or more processors, cause a computing system to perform a method according to any one of claims 1 to 8.</claim-text></claim>
</claims>
<claims id="claims02" lang="de"><!-- EPO <DP n="17"> -->
<claim id="c-de-01-0001" num="0001">
<claim-text>Computerimplementiertes Verfahren, um Vorhaben eines autonomen Fahrzeugs mitzuteilen, wobei das Verfahren umfasst:
<claim-text>durch ein Computersystem, das eine oder mehrere Computervorrichtungen umfasst, und von einem autonomen Computersystem eines autonomen Fahrzeugs erfolgendes Empfangen (402) von Daten, die ein Vorhaben des autonomen Fahrzeugs, ein Fahrmanöver durchzuführen, angeben;</claim-text>
<claim-text>Bestimmen (408), durch das Computersystem und mindestens teilweise auf der Grundlage der das Vorhaben angebenden Daten, dass das Vorhaben des autonomen Fahrzeugs einem Fahrgast des autonomen Fahrzeugs mitgeteilt werden sollte;</claim-text>
<claim-text>worin das Bestimmen, dass das Vorhaben des autonomen Fahrzeugs dem Fahrgast mitgeteilt werden sollte, das Bestimmen umfasst, dass ein Vertrauensgrad eines Ereignisses, das mit dem Vorhaben assoziiert ist, einen vorbestimmten Schwellenwert erfüllt; und</claim-text>
<claim-text>als Reaktion auf das Bestimmen, dass das Vorhaben des autonomen Fahrzeugs dem Fahrgast mitgeteilt werden sollte:
<claim-text>Erzeugen (410), durch das Computersystem, einer grafischen Schnittstelle (302, 310, 314, 318, 322, 326, 330), die das Vorhaben des autonomen Fahrzeugs angibt,; und</claim-text>
<claim-text>zur Anzeige Bereitstellen (412) der grafischen Schnittstelle (302, 310, 314, 318, 322, 326, 330) durch das Computersystem und zum Betrachten durch den Fahrgast.</claim-text></claim-text></claim-text></claim>
<claim id="c-de-01-0002" num="0002">
<claim-text>Computerimplementiertes Verfahren nach Anspruch 1, worin das Vorhaben mindestens eines von Folgendem umfasst: Wechseln der Spur, Abbremsen für ein Verkehrssignal, Abbremsen für ein sich näherndes Fahrzeug, Abbremsen für einen Fußgänger oder Abbremsen für einen Radfahrer.</claim-text></claim>
<claim id="c-de-01-0003" num="0003">
<claim-text>Computerimplementiertes Verfahren nach Anspruch 1, ferner umfassend:
<claim-text>Bestimmen (404) einer Vielzahl von Vorhaben des autonomen Fahrzeugs durch das Computersystem und mindestens teilweise auf der Grundlage der das Vorhaben angebenden Daten; und</claim-text>
<claim-text>Auswählen (406) des Vorhabens des autonomen Fahrzeugs aus der Vielzahl von Vorhaben durch das Computersystem und mindestens teilweise auf der Grundlage einer vorbestimmten Hierarchie.</claim-text></claim-text></claim>
<claim id="c-de-01-0004" num="0004">
<claim-text>Computerimplementiertes Verfahren nach Anspruch 3, worin:
<claim-text>die Vielzahl von Vorhaben angibt, dass das autonome Fahrzeug für mindestens ein erstes Ereignis und ein zweites Ereignis angehalten hat oder anhält; und</claim-text>
<claim-text>das Auswählen (406) des Vorhabens des autonomen Fahrzeugsdas Bestimmen einer Position des ersten Ereignisses in Bezug auf das zweite Ereignis in der vorbestimmten Hierarchie umfasst.</claim-text><!-- EPO <DP n="18"> --></claim-text></claim>
<claim id="c-de-01-0005" num="0005">
<claim-text>Computerimplementiertes Verfahren nach Anspruch 1, worin das Bestimmen (408), dass das Vorhaben des autonomen Fahrzeugs dem Fahrgast mitgeteilt werden sollte, das Bestimmen umfasst, dass eine Änderungshäufigkeit des Vorhabens einen vorbestimmten Schwellenwert erfüllt.</claim-text></claim>
<claim id="c-de-01-0006" num="0006">
<claim-text>Computerimplementiertes Verfahren nach Anspruch 1, worin das Erzeugen (410) der grafischen Schnittstelle (302, 310, 314, 318, 322, 326, 330) das Erzeugen einer Schnittstelle umfasst, die Folgendes umfasst: eine Karte (306), die einen aktuellen Standort des autonomen Fahrzeugs darstellt; und ein grafisches Overlay (312, 316, 320, 324, 328, 332) auf der Karte (306), welches das Vorhaben des autonomen Fahrzeugs angibt.</claim-text></claim>
<claim id="c-de-01-0007" num="0007">
<claim-text>Computerimplementiertes Verfahren nach Anspruch 1, worin (410) das Erzeugen der grafischen Schnittstelle (302, 310, 314, 318, 322, 326, 330) das Erzeugen einer Schnittstelle umfasst, die ein oder mehrere Elemente umfasst, die ein Ereignis darstellen, das mit dem Vorhaben des autonomen Fahrzeugs assoziiert ist.</claim-text></claim>
<claim id="c-de-01-0008" num="0008">
<claim-text>Computerimplementiertes Verfahren nach Anspruch 1, ferner umfassend das Erzeugen eines hörbaren Signals für den Fahrgast als Reaktion auf das Bestimmen, dass das Vorhaben des autonomen Fahrzeugs dem Fahrgast mitgeteilt werden sollte.</claim-text></claim>
<claim id="c-de-01-0009" num="0009">
<claim-text>Computersystem, umfassend:
<claim-text>einen oder mehrere Prozessoren; und</claim-text>
<claim-text>ein oder mehrere nichtflüchtige computerlesbare Medien, die gemeinsam Anweisungen speichern, die, wenn sie durch den einen oder die mehreren Prozessoren ausgeführt werden, das Computersystem veranlassen, ein Verfahren nach einem der vorhergehenden Ansprüche durchzuführen.</claim-text></claim-text></claim>
<claim id="c-de-01-0010" num="0010">
<claim-text>Ein oder mehrere nichtflüchtige computerlesbare Medien, die gemeinsam Anweisungen speichern, die, wenn sie durch einen oder mehrere Prozessoren ausgeführt werden, ein Computersystem veranlassen, ein Verfahren nach einem der Ansprüche 1 bis 8 durchzuführen.</claim-text></claim>
</claims>
<claims id="claims03" lang="fr"><!-- EPO <DP n="19"> -->
<claim id="c-fr-01-0001" num="0001">
<claim-text>Procédé implémenté par ordinateur de communication des intentions d'un véhicule autonome, le procédé comprenant:
<claim-text>la réception (402), par un système informatique qui comprend un ou plusieurs dispositifs informatiques et à partir d'un système informatique autonome d'un véhicule autonome, de données indiquant une intention du véhicule autonome d'effectuer une manœuvre de conduite;</claim-text>
<claim-text>la détermination (408), par le système informatique et sur la base au moins en partie des données indiquant l'intention, que l'intention du véhicule autonome doit être communiquée à un passager du véhicule autonome;</claim-text>
<claim-text>dans lequel la détermination que l'intention du véhicule autonome doit être communiquée au passager comprend la détermination qu'un degré de confiance d'un événement associé à l'intention satisfait un seuil prédéterminé; et</claim-text>
<claim-text>en réponse à la détermination que l'intention du véhicule autonome doit être communiquée au passager:
<claim-text>la génération (410), par le système informatique, d'une interface graphique (302, 310, 314, 318, 322, 326, 330) indiquant l'intention du véhicule autonome; et</claim-text>
<claim-text>la fourniture (412), par le système informatique et pour le visionnement par le passager, de l'interface graphique (302, 310, 314, 318, 322, 326, 330) en vue de son affichage.</claim-text></claim-text></claim-text></claim>
<claim id="c-fr-01-0002" num="0002">
<claim-text>Procédé implémenté par ordinateur selon la revendication 1, dans lequel l'intention comprend au moins l'un parmi un changement de voies, un ralentissement pour un feu de circulation, un ralentissement pour un véhicule à l'approche, un ralentissement pour un piéton, ou un ralentissement pour un cycliste.</claim-text></claim>
<claim id="c-fr-01-0003" num="0003">
<claim-text>Procédé implémenté par ordinateur selon la revendication 1, comprenant en outre:
<claim-text>la détermination (404), par le système informatique et sur la base d'au moins en partie des données indiquant l'intention, d'une pluralité d'intentions du véhicule autonome; et</claim-text>
<claim-text>la sélection (406), par le système informatique, parmi la pluralité d'intentions, et sur la base d'au moins en partie d'une hiérarchie prédéterminée, de l'intention du véhicule autonome.</claim-text></claim-text></claim>
<claim id="c-fr-01-0004" num="0004">
<claim-text>Procédé implémenté par ordinateur selon la revendication 3, dans lequel:
<claim-text>la pluralité d'intentions indique que le véhicule autonome est à l'arrêt ou s'arrête pour au moins un premier événement et un second événement; et</claim-text>
<claim-text>la sélection (406) de l'intention du véhicule autonome comprend la détermination d'une position du premier événement par rapport au second événement dans la hiérarchie prédéterminée.</claim-text></claim-text></claim>
<claim id="c-fr-01-0005" num="0005">
<claim-text>Procédé implémenté par ordinateur selon la revendication 1, dans lequel la détermination (408) que l'intention du véhicule autonome doit être communiquée au<!-- EPO <DP n="20"> --> passager comprend la détermination qu'une fréquence dans un changement de l'intention satisfait un seuil prédéterminé.</claim-text></claim>
<claim id="c-fr-01-0006" num="0006">
<claim-text>Procédé implémenté par ordinateur selon la revendication 1, dans lequel la génération (410) de l'interface graphique (302, 310, 314, 318, 322, 326, 330) comprend la génération d'une interface comprenant: une carte (306) représentant un emplacement courant du véhicule autonome; et une incrustation graphique (312, 316, 320, 324, 328, 332) sur la carte (306) indiquant l'intention du véhicule autonome.</claim-text></claim>
<claim id="c-fr-01-0007" num="0007">
<claim-text>Procédé implémenté par ordinateur selon la revendication 1, dans lequel (410) la génération de l'interface graphique (302, 310, 314, 318, 322, 326, 330) comprend la génération d'une interface comprenant un ou plusieurs éléments représentant un événement associé à l'intention du véhicule autonome.</claim-text></claim>
<claim id="c-fr-01-0008" num="0008">
<claim-text>Procédé implémenté par ordinateur selon la revendication 1, comprenant en outre, en réponse à la détermination que l'intention du véhicule autonome doit être communiquée au passager, la génération d'un signal audible pour le passager.</claim-text></claim>
<claim id="c-fr-01-0009" num="0009">
<claim-text>Système informatique comprenant:
<claim-text>un ou plusieurs processeurs; et</claim-text>
<claim-text>un ou plusieurs supports lisibles par ordinateur non transitoires qui stockent collectivement des instructions qui, lorsqu'exécutées par les un ou plusieurs processeurs, amènent le système informatique à réaliser un procédé selon l'une quelconque des revendications précédentes.</claim-text></claim-text></claim>
<claim id="c-fr-01-0010" num="0010">
<claim-text>Un ou plusieurs supports lisibles par ordinateur non transitoires qui stockent collectivement des instructions qui, lorsqu'exécutées par un ou plusieurs processeurs, amènent un système informatique à réaliser un procédé selon l'une quelconque des revendications 1 à 8.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="21"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="162" he="223" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="22"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="165" he="199" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="23"> -->
<figure id="f0003" num="3A"><img id="if0003" file="imgf0003.tif" wi="157" he="218" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="24"> -->
<figure id="f0004" num="3B"><img id="if0004" file="imgf0004.tif" wi="153" he="213" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="25"> -->
<figure id="f0005" num="3C"><img id="if0005" file="imgf0005.tif" wi="165" he="226" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="26"> -->
<figure id="f0006" num="3D"><img id="if0006" file="imgf0006.tif" wi="158" he="218" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="27"> -->
<figure id="f0007" num="3E"><img id="if0007" file="imgf0007.tif" wi="161" he="221" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="28"> -->
<figure id="f0008" num="3F"><img id="if0008" file="imgf0008.tif" wi="159" he="220" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="29"> -->
<figure id="f0009" num="3G"><img id="if0009" file="imgf0009.tif" wi="165" he="223" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="30"> -->
<figure id="f0010" num="4"><img id="if0010" file="imgf0010.tif" wi="64" he="213" img-content="drawing" img-format="tif"/></figure>
</drawings>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="EP3050771A1"><document-id><country>EP</country><doc-number>3050771</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0001">[0003]</crossref></li>
<li><patcit id="ref-pcit0002" dnum="WO2016109829A1"><document-id><country>WO</country><doc-number>2016109829</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0002">[0003]</crossref></li>
<li><patcit id="ref-pcit0003" dnum="US8676431B1"><document-id><country>US</country><doc-number>8676431</doc-number><kind>B1</kind></document-id></patcit><crossref idref="pcit0003">[0004]</crossref></li>
<li><patcit id="ref-pcit0004" dnum="US2014222277A"><document-id><country>US</country><doc-number>2014222277</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0004">[0005]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
