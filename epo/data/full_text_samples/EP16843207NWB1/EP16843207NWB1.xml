<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP16843207B1" file="EP16843207NWB1.xml" lang="en" country="EP" doc-number="3336662" kind="B1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSK..HRIS..MTNORS..SM..................</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  2100000/0</B007EP></eptags></B000><B100><B110>3336662</B110><B120><B121>EUROPEAN PATENT SPECIFICATION</B121></B120><B130>B1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>16843207.8</B210><B220><date>20160812</date></B220><B240><B241><date>20170307</date></B241><B242><date>20201007</date></B242></B240><B250>ja</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>2015181580</B310><B320><date>20150915</date></B320><B330><ctry>JP</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20180620</date><bnum>201825</bnum></B430><B450><date>20211006</date><bnum>202140</bnum></B450><B452EP><date>20210430</date></B452EP></B400><B500><B510EP><classification-ipcr sequence="1"><text>G06F   3/01        20060101AFI20210331BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>G06F   3/023       20060101ALI20210331BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>G06F   3/042       20060101ALI20210331BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G06F   3/0236      20130101 LI20171219BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>G06F   3/042       20130101 LI20171219BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>G06F  40/274       20200101 LA20200103RHEP        </text></classification-cpc><classification-cpc sequence="4"><text>G06F   3/0488      20130101 LI20170418BHEP        </text></classification-cpc><classification-cpc sequence="5"><text>G06F   3/017       20130101 FI20171005BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>ZEICHENEINGABEVERFAHREN UND ZEICHENEINGABEPROGRAMM, AUFZEICHNUNGSMEDIUM UND INFORMATIONSVERARBEITUNGSVORRICHTUNG</B542><B541>en</B541><B542>CHARACTER INPUT METHOD, AND CHARACTER INPUT PROGRAM, RECORDING MEDIUM, AND INFORMATION PROCESSING DEVICE</B542><B541>fr</B541><B542>PROCÉDÉ DE SAISIE DE CARACTÈRES, PROGRAMME DE SAISIE DE CARACTÈRES, SUPPORT D'ENREGISTREMENT, ET DISPOSITIF DE TRAITEMENT D'INFORMATIONS</B542></B540><B560><B561><text>JP-A- 2002 203 208</text></B561><B561><text>JP-A- 2005 196 530</text></B561><B561><text>JP-A- 2010 097 401</text></B561><B561><text>JP-A- 2012 084 190</text></B561><B561><text>JP-A- 2014 060 640</text></B561><B561><text>JP-A- 2014 082 605</text></B561><B561><text>JP-A- 2014 082 605</text></B561><B561><text>JP-A- 2014 107 848</text></B561><B561><text>JP-A- 2014 191 782</text></B561><B561><text>JP-A- 2014 191 782</text></B561><B561><text>JP-A- 2015 146 082</text></B561><B561><text>KR-A- 20090 116 591</text></B561><B561><text>US-A1- 2007 216 659</text></B561><B561><text>US-A1- 2008 180 403</text></B561><B561><text>US-A1- 2011 302 518</text></B561><B561><text>US-A1- 2012 120 066</text></B561><B561><text>US-A1- 2014 157 180</text></B561><B561><text>US-A1- 2015 130 728</text></B561><B565EP><date>20190502</date></B565EP></B560></B500><B700><B720><B721><snm>YAGI, Hideki</snm><adr><str>c/o OMRON SOFTWARE Co. Ltd.
801 Minamifudodo-cho
Horikawahigashiiru
Shiokoji-dori
Shimogyo-ku</str><city>Kyoto-shi
Kyoto 600-8530</city><ctry>JP</ctry></adr></B721></B720><B730><B731><snm>Omron Corporation</snm><iid>101217144</iid><irf>O 1649EU - ds</irf><adr><str>801, Minamifudodo-cho 
Horikawahigashiiru 
Shiokoji-dori 
Shimogyo-ku</str><city>Kyoto-shi, Kyoto 600-8530</city><ctry>JP</ctry></adr></B731></B730><B740><B741><snm>Müller-Boré &amp; Partner 
Patentanwälte PartG mbB</snm><iid>100060440</iid><adr><str>Friedenheimer Brücke 21</str><city>80639 München</city><ctry>DE</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B860><B861><dnum><anum>JP2016073713</anum></dnum><date>20160812</date></B861><B862>ja</B862></B860><B870><B871><dnum><pnum>WO2017047304</pnum></dnum><date>20170323</date><bnum>201712</bnum></B871></B870></B800></SDOBI>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001">FIELD OF THE INVENTION</heading>
<p id="p0001" num="0001">The present invention relates to a character input method for detecting a gesture as an operation for character input in response to a gesture of predetermined form performed in space, and determining a character to be input on the basis of the detection results; an information-processing device to which the method is applied; a program; and a recording medium.</p>
<heading id="h0002">BACKGROUND OF THE INVENTION</heading>
<p id="p0002" num="0002">In recent years, a technique for detecting the movement in space of a user's finger or hand by means of a compact range image sensor, and recognizing patterns of the movement as operations for performing prescribed information processing, has been put to practical use (for example, see Non-patent Literature 1).</p>
<heading id="h0003">DESCRIPTION OF RELATED ART</heading>
<heading id="h0004">NON-PATENT LITERATURE</heading>
<p id="p0003" num="0003">Non-patent Literature 1: <nplcit id="ncit0001" npl-type="b"><text>New Mobile Era Realized by Gesture Interface - Space is stereoscopically captured with a compact, low-cost range image sensor, TERA, vol. 59, 2014, Public Relations Department, NTT Comware Corporation (https://www.nttcom.co.jp/tera/tera59</text></nplcit><u>/)</u></p>
<p id="p0004" num="0004"><patcit id="pcit0001" dnum="JP2014191782A"><text>JP 2014 191782 A</text></patcit> (NTT DOCOMO INC) describes an information processing apparatus according configured to be capable of receiving a touch input caused by direct contact of an input instruction portion with a touch surface and a hover input by positioning an input instruction portion in a hover input space spaced a predetermined distance from a touch surface.</p>
<p id="p0005" num="0005"><patcit id="pcit0002" dnum="US2015130728A1"><text>US 2015/130728 A1</text></patcit> (TAKENAKA HIDETOSHI [JP]) describes an information processing device including a display input part accepting an operation of an operation object; an operation<!-- EPO <DP n="2"> --> detecting means for accepting an input operation of the operation object by detecting an operation position of the operation object on a face of the display input part; and a controlling means for executing a preset process in accordance with the input operation.</p>
<p id="p0006" num="0006"><patcit id="pcit0003" dnum="JP2005196530A"><text>JP 2005 196530 A</text></patcit> (ALPINE ELECTRONICS INC) describes a space input device provided with a solid virtual image display part for displaying a solid virtual image, a space position detecting part for detecting contact with the displayed solid virtual image at a plurality of operating points, a control part for analyzing inputted instructions from the operating points where contact has been detected and the detection order of the operating points, and for performing the instructed processing.</p>
<p id="p0007" num="0007"><patcit id="pcit0004" dnum="KR20090116591A"><text>KR 2009 0116591 A</text></patcit> (LG ELECTRONICS INC [KR]) describes a user interface control method using a gesture at adjacent space to control an electronic system by not touching the electronic system, but using the gesture at adjacent space to the electronic system.</p>
<p id="p0008" num="0008"><patcit id="pcit0005" dnum="JP2014082605A"><text>JP 2014 082605 A</text></patcit> (CANON MARKETING JAPAN INC) describes an information processing apparatus in which virtual keys are stored in association with a plurality of hierarchies, a position of a pointing medium is specified, whether the pointing medium is positioned in a region corresponding to any hierarchy is determined, and if the pointing medium is positioned in a region corresponding to any hierarchy, the virtual keys stored in association with the hierarchy are displayed on a display screen.</p>
<p id="p0009" num="0009"><patcit id="pcit0006" dnum="US2012120066A1"><text>US 2012/120066 A1</text></patcit> (HIROTA TAKASHI [JP]) specifies that when an instruction is accepted from a user using an instruction acceptance image, which is a stereoscopic image, a plurality of instruction acceptance images are displayed transparently one on top of the other.</p>
<p id="p0010" num="0010"><patcit id="pcit0007" dnum="JP2014060640A"><text>JP 2014 060640 A</text></patcit> (SHARP KK) describes a kana character input device including: a column selection section for selecting a kana column on the basis of the operation position of a down<!-- EPO <DP n="3"> --> operation; an input character selection section for selecting a kana character belonging to the kana column as an input character on the basis of a slide operation after the kana column selection.</p>
<p id="p0011" num="0011"><patcit id="pcit0008" dnum="JP2015146082A"><text>JP 2015 146082 A</text></patcit> (KONICA MINOLTA INC) describes an input device that includes: a display operation section for displaying selection items on a display screen, and receiving a touch input from a user; a development section which develops input candidate groups corresponding to each of two selection items, on a display screen when the user touches two of the selection items.</p>
<heading id="h0005">SUMMARY OF THE INVENTION</heading>
<heading id="h0006">PROBLEM TO BE SOLVED BY THE INVENTION</heading>
<p id="p0012" num="0012">The above-stated recognition technique is expected to be widely used in the near future, particularly in compact information-processing devices called "wearable terminals." Specifically, given the fact that it is difficult to apply, as is, user interfaces having the same operations as those currently used in smartphones to miniaturized touch panels, it<!-- EPO <DP n="4"> --> is necessary to practically achieve techniques for character input by the performance of gestures in space.</p>
<p id="p0013" num="0013">It is more desirable for general users that a gesture interface employing character input operations that users are accustomed to performing on a touch panel is provided. However, if gestures corresponding to conventional tap operations and flick operations are accepted while the entire image of a virtual keyboard is displayed on a miniaturized screen, input errors can easily occur, or the character input efficiency may be reduced.</p>
<p id="p0014" num="0014">Further, when using a virtual keyboard having a configuration in which a plurality of characters is allocated to a single character key, such as a 12-key keyboard for Japanese character input, confusion is likely to arise when determining whether a gesture is for selecting a character key, or for selecting a character allocated to the character key selected. For example, when a user performs a gesture simulating a tap operation by continuing operation on the same key with the intention of changing characters, if the user's finger is positionally displaced in a longitudinal or horizontal direction while the finger is moving upward and downward, the positional displacement may be erroneously determined to be a switching operation for selection of adjoining character keys. Additionally, when a character is selected by a gesture simulating a flick operation from a character key, there can be, due to a change in finger height of a finger while the user moves the finger, an erroneous determination as to whether a tap operation or a switching operation for selection of a character key is performed.</p>
<p id="p0015" num="0015">The present invention has been made in view of the above-described problems, and an object thereof is to make it possible to accurately recognize a character input operation by a gesture performed in space, thereby ensuring input operation for the correct characters.<!-- EPO <DP n="5"> --></p>
<heading id="h0007">MEANS FOR SOLVING THE PROBLEMS</heading>
<p id="p0016" num="0016">The invention relates to a method, a device, a computer program, and a computer-readable recording medium as set out in the appended claims. Any embodiments not falling within the scope of the claims may be regarded as examples useful for understanding the invention. The present invention is applied to a character input method as defined in the appended claim 1 for detecting a gesture of predetermined form as an operation for character input, by means of a gesture recognition device that recognizes a gesture performed in space; and determining a character to be input on the basis of the detection results in a computer that performs a prescribed process in response to character input. The gesture recognition device is composed of a sensor (range image sensor etc.) for measuring a three-dimensional shape of a part of a human body; a computer having software incorporated therein that processes the measurement results measured by the sensor to thereby recognize the pattern, the movement direction, and the movement distance of the gesture (a computer for implementing the character input method according to the present invention or a computer independent from the computer); etc.</p>
<p id="p0017" num="0017">In the character input method according to the present invention, the computer is configured to register first definition information for layout of a plurality of character keys on a virtual plane, each having a plurality of characters allocated thereto; and second definition information for layout of each character allocated to each of the arranged character keys outside the virtual plane at individually different positions relative to a virtual axis passing through the character key, after which the following steps 1 through 3 are performed.</p>
<p id="p0018" num="0018">In step 1, the plurality of character keys is arranged by applying the first definition information to a virtual plane including a detection position of a gesture, in response to detection of a gesture that satisfies predetermined<!-- EPO <DP n="6"> --> conditions. In step 2, a selection position with respect to the layout of the character keys is moved in accordance with the gesture movement, under the condition that the gesture moving along the virtual plane on which the layout of the character keys is established is detected. In step 3, a character<!-- EPO <DP n="7"> --> disposed at a moving destination of the gesture is selected as a character to be input in response to detection of a gesture moving toward any of the plurality of characters arranged on the basis of the second definition information with respect to the character key being selected after the movement direction of the gesture detected is switched from a direction along the virtual plane on which the character keys are established to a direction away from the virtual plane, under the condition that the prescribed character key in the layout is selected.</p>
<p id="p0019" num="0019">For example, given that character input is carried out by the movement of a hand with fingers formed in a specific shape, steps 1 and 2 are performed when a user starts to move the hand formed in the specific shape (for example, along a direction parallel to the plane on which a guidance screen for operation is displayed). While the user continues moving the hand along the virtual plane, the process of step 2 continues; therefore, selection with respect to the layout of character keys is switched in accordance with the movement direction and the movement distance per unit of time. When the hand that selected a target character key is moved in a direction away from the virtual plane on which the character keys are arranged (for example, downward from the virtual plane), the program proceeds from step 2 to step 3, in which the hand is moved in a direction toward any of characters arranged on the basis of the second definition information with respect to the selected character key, to thereby select the character as a character to be input.</p>
<p id="p0020" num="0020">As described above, according to the present invention, by switching the movement direction of a gesture from a direction along the virtual plane on which the character keys are arranged on a direction in which the gesture moves away from the virtual plane, the user can switch from a character key selection mode to a character selection mode, in which a character is selected from among characters allocated to the selected character key. Thereby, an operation for selection of<!-- EPO <DP n="8"> --> a character is prevented from being erroneously determined to be an operation for switching selection to another character key, thereby allowing a user to precisely input an intended character.</p>
<p id="p0021" num="0021">In a first embodiment which is not according to the present invention, second definition information is registered, implying that a plurality of characters allocated to the same character key is arranged on a second virtual plane that is along the virtual plane on which character keys are arranged. According to this second definition information, a virtual keyboard having a two-layered structure is established in space, in which the second virtual plane for each character key is linked to the virtual plane on which the character keys are arranged; therefore, character input can be accepted.</p>
<p id="p0022" num="0022">In a second embodiment which is according to the present invention, second definition information is registered, implying that with respect to layout of character keys, assuming a maximum of five characters allocated to each character key, such as in a 12-key keyboard for input of Japanese <i>kana</i> characters, each character of a plurality of characters allocated to the same character key, except for one character, is distributed in a range not including a position intersecting with the virtual axis on the second virtual plane along the virtual plane, while the one remaining character is disposed at the position intersecting with the virtual axis on a third virtual plane opposite the character keys, with the second virtual plane interposed between the virtual plane and the third virtual plane. According to this definition information, a virtual keyboard having a three-layered structure is established in space, in which the second and the third virtual planes for each character key are linked to the virtual plane on which the character keys are arranged; therefore, character input can be accepted. Further, five or fewer characters allocated to a character key can be arranged, employing the key layout in the virtual keyboard for a touch panel; therefore, it is possible to provide<!-- EPO <DP n="9"> --> a gesture interface that is easily accepted by a user accustomed to touch-panel operation.</p>
<p id="p0023" num="0023">In a third embodiment which is according to the present invention, step 2 further includes a process of displaying an image of a region that is a portion of an image showing the layout of the character keys, with a character key selected along a movement direction of the gesture as a center on the display connected to the computer. Further, step 3 includes a process of showing an image on the display, said image representing the positional relationship of each character allocated to the selected character key in place of the display in step 2, on the basis of the second definition information. These displays allow the user to correctly select input characters, while recognizing the character keys and characters selected by the gesture.</p>
<p id="p0024" num="0024">Further, in the third embodiment described above, in response to the selection of an input character in step 3, the computer can perform step 4 of extracting candidates for a character string to be input adapting to the input character and displaying a list of the extracted candidates on the display in response to selection of an input character in step 3, while performing step 5 of updating a display indicating a selection position in the list under a state in which the list is displayed, in accordance with a gesture movement in response to detection of a gesture having a form different from the above-described gesture, moving along the virtual plane on which the character keys are arranged. According to these steps, a character string composed of a plurality of characters can be efficiently input.</p>
<p id="p0025" num="0025">In a fourth embodiment according to the present invention, the gesture recognition device further is configured to recognize a gesture including hand movement, and step 1 is performed in response to detection of a one-finger hand gesture on the basis of the recognition results by the gesture recognition device. Thereafter, step 2 is started with a position of any character key as an initial selection position<!-- EPO <DP n="10"> --> in the layout of character keys established in step 1. Further, when it is detected that a one-finger hand starts to move in a direction along the virtual axis passing through a character key selected, the program proceeds from step 2 to step 3. According to this embodiment, a character key is selected by moving the one-finger hand gesture along the layout of character keys; thereafter, the direction of hand movement is switched to a direction along the axis passing through the selected character key, which allows a shift to a gesture for selection of characters.</p>
<p id="p0026" num="0026">In a fifth embodiment according to the present invention, the first definition information and the second definition information further are registered in the computer for each of a plurality of types of virtual keyboards having different types of characters allocated to each character key. The virtual keyboard to be activated is switched in response to detection of a gesture having a prescribed form different from the gesture in relation to each of steps 1 through 3; accordingly, the computer performs each of steps 1 through 3 using the first definition information and the second definition information corresponding to the activated virtual keyboard. According to this embodiment, a plurality of types of characters can be input by gestures performed in space.</p>
<p id="p0027" num="0027">In a sixth embodiment according to the present invention, further the display connected to the computer provides a display region, for input character string, equipped with a cursor indicating an input position. When an input character is selected in step 2, the input character selected is inserted ahead of the cursor in the display region for input character string. According to this embodiment, a plurality of input characters is selected in order by continuing gestures for selection of character keys and characters; therefore, input character strings formed by a series of these characters can be displayed in a display region for input character strings.<!-- EPO <DP n="11"> --></p>
<p id="p0028" num="0028">Further, in the sixth embodiment, to improve convenience, it is also possible to move a position of the cursor in response to detection of a first gesture having a form different from the gesture relating to each of steps 1 through 3; and to delete a character ahead of the cursor in response to detection of a second gesture having a form different from the gesture relating to each of steps 1 through 3 and the first gesture under a state in which an input character string having a prescribed length is displayed in the display region for input character string.</p>
<p id="p0029" num="0029">The present invention further provides a computer program that causes a computer in an information-processing device to function, and a non-temporary computer-readable recording medium that records the program, in accordance to appended independent claims 12 and 13. A character input device operated by this program includes: a definition information storage unit that stores the aforementioned first and second definition information; a character key layout establishing unit that establishes the layout of a plurality of character keys by application of the first definition information to a virtual plane, including a detection position of a gesture in response to detection of a gesture satisfying predetermined conditions; a character key selection unit that moves a selection position with respect to the layout of character keys in accordance with the movement of the gesture, under the condition that the gesture moving along the virtual plane on which the layout of character keys is established is detected; and an input character selection unit that selects a character disposed at a movement destination of the gesture as a character to be input in response to detection of a gesture moving toward any of the plurality of characters arranged on the basis of the second definition information with respect to the character key being selected after the movement direction of the gesture detected is switched from a direction along the virtual plane on which the character keys are established to a direction<!-- EPO <DP n="12"> --> away from the virtual plane, under the condition that a prescribed character key in the layout is selected.</p>
<p id="p0030" num="0030">The above-described character input device is provided with a display control unit that displays an image showing character keys or characters that can be selected by gestures being detected on the display provided in the information-processing device, in conjunction with the operation of the character key selection unit and the input character selection unit.</p>
<p id="p0031" num="0031">Additionally, the above-described character input device may be provided with a candidate extraction unit that extracts candidates for input character string adapting to an input character in response to selection of the input character by the input character selection unit. In this case, the display control unit is provided with a function of displaying a list of candidates extracted by the candidate extraction unit on the display while updating the selection position of the candidate in the list under a state in which the list is displayed, in accordance with a gesture movement in response to detection of a gesture that has a form different from the gesture in relation to the processing by the character key selection unit, and moves along the virtual plane on which the character keys are arranged.</p>
<heading id="h0008">ADVANTAGEOUS EFFECT OF THE INVENTION</heading>
<p id="p0032" num="0032">According to the present invention, it is possible to correctly determine a gesture for selecting one of a plurality of character keys arranged on a virtual plane, and a gesture for selecting one of a plurality of characters allocated to the selected character key. Thereby, a gesture performed in space makes it possible to input a character efficiently and correctly.</p>
<heading id="h0009">BRIEF DESCRIPTION OF THE DRAWINGS</heading><!-- EPO <DP n="13"> -->
<p id="p0033" num="0033">
<ul id="ul0001" list-style="none" compact="compact">
<li><figref idref="f0001">Fig. 1</figref> is a view illustrating an example in which a gesture for character input is performed with respect to an information-processing device.</li>
<li><figref idref="f0002">Fig. 2</figref> is a block diagram illustrating functions of a character input device.</li>
<li><figref idref="f0003">Fig. 3</figref> is a view illustrating a configuration example of a virtual keyboard for input of Japanese characters.</li>
<li><figref idref="f0003">Fig. 4</figref> is a view illustrating an example of a gesture for selection with respect to a candidate list.</li>
<li><figref idref="f0004">Fig. 5</figref> is a view illustrating an example of a gesture for directing movement of a cursor and deletion of an input character in an input area.</li>
<li><figref idref="f0005">Fig. 6</figref> is a view illustrating an example of a gesture for switching a type of virtual keyboard.</li>
<li><figref idref="f0006">Fig. 7</figref> is a view illustrating a configuration example of a virtual keyboard for input of English characters.</li>
<li><figref idref="f0006">Fig. 8</figref> is a view illustrating a configuration example of a virtual keyboard for input of alphanumeric characters.</li>
<li><figref idref="f0007">Fig. 9-1</figref> is a flowchart illustrating steps of a main routine in the character input device.</li>
<li><figref idref="f0008">Fig. 9-2</figref> is a flowchart illustrating steps of a main routine in the character input device.</li>
<li><figref idref="f0009">Fig. 10-1</figref> is a flowchart illustrating the detailed steps of a character input acceptance process in a main routine.</li>
<li><figref idref="f0010">Fig. 10-2</figref> is a flowchart illustrating the detailed steps of a character input acceptance process in a main routine.</li>
<li><figref idref="f0011">Fig. 11</figref> is a flowchart illustrating the detailed steps of a candidate selection process in a main routine.</li>
<li><figref idref="f0012">Fig. 12-1</figref> is a view illustrating an example of a transition for a display screen created with the progression of processing following a user's gestures.</li>
<li><figref idref="f0013">Fig. 12-2</figref> is a view illustrating an example of a transition for a display screen created with the progression of processing following a user's gestures.</li>
</ul><!-- EPO <DP n="14"> --></p>
<heading id="h0010">DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p0034" num="0034"><figref idref="f0001">Fig. 1</figref> is a view illustrating an example in which a gesture for input of a character is performed with respect to an information-processing device S (<figref idref="f0001">Fig. 1</figref> shows only the main body thereof). The information-processing device S according to this embodiment is a wristwatch-type wearable terminal incorporating therein a circuit substrate on which a range image sensor, a microcomputer, etc., are mounted (not shown), having a display 4 constituted by a touch panel or the like provided on the front surface.</p>
<p id="p0035" num="0035">The range image sensor creates range image data reflecting the three-dimensional shape of a nearby object using infrared rays or lasers. The microcomputer includes a program for detecting a human finger from the range image data to thereby recognize the position and the shape thereof in space near the information-processing device. The function established in the information-processing device S on the basis of the program and the range image sensor is referred to as a "gesture recognition device," which is represented by reference numeral 2 in <figref idref="f0002">Fig. 2</figref>.</p>
<p id="p0036" num="0036">The microcomputer also includes a program for character input that causes the microcomputer to function as a character input device 1 that identifies several types of predetermined gestures from among gestures recognized by a gesture recognition device 2, and determines an input character on the basis of the gesture; and a program for an application 3 that performs a prescribed process using the determined input character. The above-described program for character input may be provided for the microcomputer through a non-temporary computer-readable recording medium such as a CD-ROM that records the program, or may be provided for the microcomputer through a communication line such as the internet etc.</p>
<p id="p0037" num="0037"><figref idref="f0002">Fig. 2</figref> illustrates functions included in the character input device 1 along with the relationship with the gesture recognition device 2, the application 3 to be input, and<!-- EPO <DP n="15"> --> the display 4. The input device 1 according to this embodiment is provided with processing units such as a gesture determination unit 10, a virtual keyboard control unit 11, an input character string assembling unit 12, a candidate retrieving unit 13, an input character string determination unit 14, a display control unit 15, etc.; a key layout definition table 101; and a dictionary database 102 in which a plurality of types of words is registered.</p>
<p id="p0038" num="0038">The gesture determination unit 10 determines gestures for character input from among the recognition results created by the gesture recognition device 2, and performs processing in response to the gestures with the other processing units performing operations when receiving the recognition results.</p>
<p id="p0039" num="0039">When the gesture determination unit 10 detects a first gesture for character input, the virtual keyboard control unit 11 establishes a virtual keyboard having a three-layered structure (detailed later) at the position of the fingers for the gesture on the basis of the information registered in the key layout definition table 101. Further, the virtual keyboard control unit 11 switches the selection position with respect to the layout of character keys in response to gestures subsequently determined by the gesture determination unit 10, and selects any character allocated to the character key ultimately selected.</p>
<p id="p0040" num="0040">The input character string assembling unit 12 sequentially connects input characters selected by the virtual keyboard control unit 11 in series to thereby assemble an input character string. The input character string determination unit 14 determines an input character string when a gesture representing determination is performed, and outputs the determined character string to the application 3. The candidate retrieving unit 13 performs a search of the dictionary database 102 with the input character string assembled by the input character string assembling unit 12 to thereby extract a candidate for an ultimately formed input character string<!-- EPO <DP n="16"> --> (hereinafter referred to as "estimated conversion candidate"), and performs a character-string extraction process for extracting a character string that can be input subsequent to the determined character string (hereinafter referred to as "connection candidate") by searching the dictionary database 102 with the character string determined by the input character string determination unit 14.</p>
<p id="p0041" num="0041">The display control unit 15 includes display processing units 16, 17, and 18 to display a navigation window 41, an input area 42, and a candidate window 43 on the display 4 (see <figref idref="f0001">Fig. 1</figref>). The navigation window 41 displays images for supporting character input operations. The input area 42 displays an input character string assembled by the input character string assembling unit 12 or an input character string determined by the input character string determination unit 14. The candidate window 43 displays lists of estimated conversion candidates and connection candidates extracted by the candidate retrieving unit 13.</p>
<p id="p0042" num="0042"><figref idref="f0003">Fig. 3</figref> is a view illustrating a configuration example of a virtual keyboard 11J for Japanese <i>kana</i> character input registered in the key layout definition table 101. The virtual keyboard 11J has a three-layered structure employing the 12-key keyboard that has been adopted in the user interface of conventional smartphones. Specifically, 12 character keys 100 are arranged on the top layer (hereinafter referred to as "first layer L1"); <i>kana</i> characters are allocated to 10 character keys 100 of these character keys; and symbols or the like are allocated to the two remaining character keys 100. In the allocation of <i>kana</i> characters, 5 or 3 <i>kana</i> characters having a common consonant are allocated to the same character key 100 on the basis of the 50-sound table. Each character key 100 bears <i>kana</i> characters "a" "ka" "sa" "ta" "na" ... "wa" (hereinafter referred to as "representative characters") having a vowel "a" contained in the <i>kana</i> characters allocated.<!-- EPO <DP n="17"> --></p>
<p id="p0043" num="0043">A second layer (hereinafter referred to as "second layer L2") and a third layer (hereinafter referred to as "third layer L3") are independently provided for each character key 100, and arranged parallel to the first layer L1 with a virtual axis C passing through a corresponding character key 100 as the center. A plurality of characters (four or fewer) excluding the representative character of the characters allocated to the corresponding character key is arranged on the second layer L2, and only the representative character is arranged on the third layer L3. Although the virtual keyboard 11J for <i>kana</i> input according to this embodiment is intended to input <i>hiragana</i> characters, a virtual keyboard having the same three-layered structure can be applied to <i>katakana</i> input.</p>
<p id="p0044" num="0044">According to this embodiment, a character input operation is performed by a gesture made by moving one finger held in space over the display 4 of the information-processing device S (see <figref idref="f0001">Fig. 1</figref>), when each layer L1, L2, and L3 is arranged along a virtual plane parallel to the surface of the display 4. Hereinafter, a direction along the horizontal axis of the layout of the character keys 100 (a direction in which character keys 100 of each column are aligned in the following order: "a" "ka" "sa") is defined as a horizontal direction; a direction along the vertical axis (a direction in which character keys 100 of each column are aligned in the following order: "a," "ta," "ma") is defined as a longitudinal direction; and the four directions of the character keys 100 are expressed as "forward," "backward," "left," and "right."</p>
<p id="p0045" num="0045">In the second layer L2, characters containing a vowel "i" are disposed at positions offset to the left of the corresponding character key 100 of the first layer L1;<br/>
characters containing a vowel "u" are disposed at positions offset to the rear side; characters containing a vowel "e" are disposed at positions offset to the right side; and characters containing a vowel "o" are disposed at positions offset to the front side. In the third layer L3, representative characters<!-- EPO <DP n="18"> --> containing a vowel "a" are disposed at positions opposite the character keys 100 in the first layer L1 (intersecting positions with the virtual axis C), with the second layer L2 interposed between the third layer L3 and the first layer L1. Therefore, when the second layer L2 and the third layer L3 are seen through the first layer L1, each character is disposed having the same positional relationship as the character layout for a flick operation in the virtual keyboard for a touch panel.</p>
<p id="p0046" num="0046">The actual surface of the display 4 is likely to be inclined in an arbitrary direction; therefore, the direction of each layer L1, L2, and L3 changes depending on the degree of the inclination. Further, the finger for performing a gesture is not necessarily opposite the display 4, and may be placed outside the range opposite the display 4 as long as the finger is in a range the range image sensor can detect. Similarly, a gesture can be performed at any height.</p>
<p id="p0047" num="0047">According to this embodiment, the operation with respect to the above-described virtual keyboard 11J includes a one-finger hand gesture movement in a direction substantially parallel to the surface of the display 4 (hereinafter this gesture is referred to as "parallel movement"), and a one-finger hand gesture movement toward the display 4 and a gesture moving away from the display 4 (hereinafter, the former gesture is referred to as a "downward-movement action" and the latter gesture is referred to as an "upward-movement action"). The gesture determination unit 10 tracks the position of the feature point group at the tip of the one finger on the basis of the recognition results created by the gesture recognition device 2 and calculates the movement direction and the movement distance, thereby determining, from among parallel movement, downward-movement action, and upward-movement action, the action that is currently performed.</p>
<p id="p0048" num="0048">When starting detection of a one-finger hand, the virtual keyboard control unit 11 establishes the first layer L1 at the position of the finger of the hand, and sets up a state<!-- EPO <DP n="19"> --> in which a prescribed character key 100 in the first layer L1 (for example, a character key 100 of the column "a") is selected. Thereafter, when parallel movement is detected, the virtual keyboard control unit 11 regards this parallel movement as a movement in the first layer L1, and switches the selection position with respect to the layout of the character keys 100 by following this movement.</p>
<p id="p0049" num="0049">When a downward-movement, one-finger hand action is detected under a state in which any character key 100 (for example, a character key 100 of the column "ta" in <figref idref="f0003">Fig. 3</figref>) is selected, the virtual keyboard control unit 11 activates the second layer L2 and the third layer L3 corresponding to the selected character key 100. Thereafter, when a parallel movement is again detected, the parallel movement is determined to be a movement in the second layer L2. Then, a character corresponding to the movement direction is selected as an input character from among characters (for example, "chi," "tsu," "te," "to") arranged on the second layer L2, corresponding to a movement directed toward any one of "forward," "backward," "left," and "right." Meanwhile, when a movement distance due to the downward-movement action exceeds a prescribed threshold without detection of a parallel movement, it is determined by the downward-movement action that the finger moves to the third layer L3, and a representative character disposed on the third layer L3 (for example, "ta") is selected as an input character.</p>
<p id="p0050" num="0050">When an upward-movement, one-finger hand action is detected under a state in which the movement distance of the downward-movement action remains under a threshold without detection of parallel movement, the upward-movement action is determined to be a return action to the first layer L1. Thereafter, if a parallel one-finger hand movement is detected again after the upward-movement action is finished, the process of switching selection of the character key 100 in the first layer L1 is resumed by following the parallel movement. Further, in the case where the one-finger hand is held after a character<!-- EPO <DP n="20"> --> in the second layer L2 or in the third layer L3 is selected, the process responding to the gesture for character input is subsequently carried out by returning to the state in which a character key 100 in the first layer L1 is selected.</p>
<p id="p0051" num="0051">As described above, according to this embodiment, when the one-finger hand action is switched from a parallel movement to a downward-movement action, a mode for accepting selection with respect to the first layer L1 on which the character keys 100 are arranged is switched to a mode for accepting selection of any character allocated to the selected character key 100. Thereby, a gesture for selection of a character after selection of a character key 100 is prevented from being determined as an operation for switching selection to another character key, thereby allowing a user to determine an intended character as an input character.</p>
<p id="p0052" num="0052">Further, after performing an action of moving a finger to a target character key 100 and pushing the character key 100 lightly (downward-movement action), the user can select an input character by making the same finger movement as that of a smartphone flick operation. Therefore, the user can carry out character input by performing a gesture already familiar to a user accustomed to smartphone operation.</p>
<p id="p0053" num="0053">A navigation window display processing unit 16 displays the navigation window 41 on the display 4, the navigation window 41 including images showing patterns of the layout of selected character keys 100 and characters allocated to the selected character key 100 on the basis of gestures per unit time while the one-finger hand gesture that is performed is detected (see <figref idref="f0012 f0013">Fig. 12</figref> detailed later). By following this display, the user can perform the above-described gestures while recognizing the selected character keys 100 and characters.</p>
<p id="p0054" num="0054">Operations except for the operation with respect to the virtual keyboard 11J may be performed by gestures performed in space based on a prescribed rule. <figref idref="f0003">Fig. 4</figref> shows a state in which a gesture for selection of a candidate is being performed<!-- EPO <DP n="21"> --> with respect to a display in the candidate window 43 including a list of estimated conversion candidates or connection candidates. In this embodiment, a two-finger hand gesture moving over the candidate window 43 in a longitudinal direction or a horizontal direction is established as an operation for switching selection of a candidate, and a candidate being selected is switched in accordance with the movement direction and movement distance of the fingers.</p>
<p id="p0055" num="0055"><figref idref="f0004">Fig. 5</figref> shows gestures with respect to the input area 42, and specific examples of the process performed by such gestures. In this embodiment, the position of a cursor 40 is moved in the input area 42 by holding a five-finger hand gesture made over the input area 42 wherein an input character string having a prescribed length is displayed, and moving the hand along the input character string (see <figref idref="f0004">Fig. 5(1)</figref>, (2), (3)). Further, as shown in <figref idref="f0004">Fig. 5(4)</figref>, closing the five-finger hand gesture into a fist is established as an operation that directs deletion of an input character, and a character ahead of the cursor 40 in the input area 42 is deleted in response to the action. Further, if the hand closed into a fist is held for a prescribed period of time, characters ahead of the cursor 40 are deleted sequentially, one by one.</p>
<p id="p0056" num="0056">The key layout definition table 101 shown in <figref idref="f0002">Fig. 2</figref> stores the definition information of a virtual keyboard 11E for English character input and a virtual keyboard 11N for numeral input, as shown in <figref idref="f0005">Fig. 6(1)</figref>, in addition to the definition information exhibiting the configuration of the virtual keyboard 11J for <i>kana</i> character input shown in <figref idref="f0003">Fig. 3</figref>. Further, in relation to the virtual keyboard 11E for English character input, the key layout definition table 101 stores the definition information for two types of virtual keyboards 11Ea and 11Eb, as shown in <figref idref="f0005">Fig. 6(2)</figref>.</p>
<p id="p0057" num="0057">In this embodiment, a three-finger hand gesture waving in a horizontal direction (swiping) is established as an operation for directing switchover between three types of<!-- EPO <DP n="22"> --> virtual keyboards 11J, 11N, and 11E (see <figref idref="f0005">Fig. 6(1)</figref>). Further, in relation to the virtual keyboard 11E for English character input, a three-finger hand gesture waving in a longitudinal direction is established as an operation for directing switchover between a virtual keyboard 11Ea for uppercase character input and a virtual keyboard 11Eb for lowercase character input (see <figref idref="f0005">Fig. 6(2)</figref>).</p>
<p id="p0058" num="0058">Furthermore, in relation to the virtual keyboards 11J, 11E, and 11N for Japanese character input, English character input, and numeral input, respectively, an input mode for full-width characters and an input mode for half-width characters can be switched between by a gesture of prescribed form.</p>
<p id="p0059" num="0059">Further, the virtual keyboard 11E for English character input has only four characters or fewer allocated to each character key 100, and is thus configured to have a two-layered structure made up of a first layer L1 on which the character keys 100 are arranged and a second layer L2 for each character key. In the second layer L2 corresponding to each character key 100, characters are arranged at positions displaced longitudinally and horizontally with respect to the corresponding character key 100.</p>
<p id="p0060" num="0060">The virtual keyboard 11N for numeral input has one character allocated to each character key 100, and is thus configured to have a single-layered structure made up of only the first layer L1.</p>
<p id="p0061" num="0061">The configuration of the virtual keyboard for input of English characters or numeric characters is not limited to those described above, and it is also possible to provide a virtual keyboard 11T having a three-layered structure as shown in <figref idref="f0006">Fig. 8</figref>, wherein a plurality of English characters and one numeric character are allocated each of 10 character keys 100; one of these characters is disposed on a third layer L3 as a representative character; and the remaining characters are arranged on the second layer L2 displaced longitudinally and horizontally with respect to the character key 100. Similarly,<!-- EPO <DP n="23"> --> in a character key 100 having four or fewer allocated characters, a representative character (for example, "T") is disposed on the third layer L3; and the other keys (for example, "U," "V," "8") are respectively arranged in a left-side region, a front-side region, and a rear-side region on the second layer L2. According to this virtual keyboard 11T, a user can operate character input with a feeling close to that of the virtual keyboard for input of alphanumeric characters of conventional smartphones.</p>
<p id="p0062" num="0062"><figref idref="f0007 f0008">Fig. 9</figref> (<figref idref="f0007">Fig. 9-1</figref> and <figref idref="f0008">Fig. 9-2</figref>) shows a flow of process (hereinafter referred to as "main routine") performed by the character input device 1 in response to various types of gestures, as described above. According to this main routine, the gesture determination unit 10 acquires recognition results from the gesture recognition device 2; therefore, processes (steps S1, S2) for determining intention of the gesture are repeated on the basis of the determination results, and procedures responsive to the determination results per unit of time are carried out.</p>
<p id="p0063" num="0063">When a one-finger hand gesture movement is detected, the program goes to the character input acceptance process (step S3) performed by the virtual keyboard control unit 11. When an input character is determined by the process, the candidate retrieving unit 13 retrieves an estimated conversion candidate (step S4) and a candidate window display processing unit 18 receiving the retrieving results displays a candidate list of estimated conversion candidates (step S5).</p>
<p id="p0064" num="0064">When a two-finger hand gesture movement is detected, the program goes to the candidate selection process in step 7 under the condition that the candidate window 43 is displayed ("YES" in step 6). When an input character string is determined on the basis of the candidate selected by the candidate selection process, a connection candidate with respect to the determined character string is retrieved by the candidate retrieving unit 13 (step S8), and the candidate window display<!-- EPO <DP n="24"> --> processing unit 18 receiving the retrieving results displays a candidate list of connection candidates (step S9) to thereby update the display on the candidate window 43.</p>
<p id="p0065" num="0065">When a three-finger swipe is detected, the virtual keyboard control unit 11 performs switchover of the virtual keyboard. However, when this swipe is carried out under the condition that an input character string is displayed in the input area 42, the program goes to the switchover of the virtual keyboard after the input character string in the input area 42 is determined (steps S10, S11).</p>
<p id="p0066" num="0066">If the swipe direction is either a leftward direction or a rightward direction, switchover is carried out between three types of virtual keyboards 11J, 11E, and 11N, as shown in <figref idref="f0005">Fig. 6(1)</figref> (step S13). Further, when either one of two types of virtual keyboards 11Ea and 11Eb, which are shown in <figref idref="f0005">Fig. 6(2)</figref> as the virtual keyboard 11E for English characters, is selected, if the above-described swipe is carried out in either the longitudinal or horizontal direction, a switchover to the other virtual keyboard for English characters is carried out (steps S14, S15).</p>
<p id="p0067" num="0067">When a five-finger hand gesture is detected, the process determines whether the hand moves to the right or left, or changes from an open state to a fist state under the condition that an input character string is displayed in the input area 42 (steps 16, 17). Here, if a movement in the right or left direction is detected, an input area display processing unit 17 performs the process of moving the cursor 40 in the input area 42 along the direction corresponding to the movement direction of the hand (step 18). Further, when the hand gesture made with five fingers is changed from an open state to a fist state, the input area display processing unit 17 performs the process of deleting a character ahead of the cursor 40 in the input area 42 (step S19).</p>
<p id="p0068" num="0068">The program returns to step S1 after the processes corresponding to each gesture are completed; regarding<!-- EPO <DP n="25"> --> subsequent gestures, processes are performed in the same manner as described above. If a gesture made with a form different from the gestures described above is detected, such a gesture is disregarded as a gesture unrelated to character input, and the program returns to step S1.</p>
<p id="p0069" num="0069"><figref idref="f0009 f0010">Fig. 10</figref> (<figref idref="f0009">Figs. 10-1</figref> and <figref idref="f0010">10-2</figref>) shows detailed steps when accepting character input using the three-structured virtual keyboard 11J in step S3 of a main routine. The character input acceptance process is started when a one-finger hand gesture is detected. Thereafter, while the gesture determination unit 10 performs gesture determination processes when necessary, the virtual keyboard control unit 11 progresses in the process in cooperation with the navigation window display processing unit 16 and the input character string assembling unit 12.</p>
<p id="p0070" num="0070">When the character input acceptance process is started, the virtual keyboard control unit 11 establishes a first layer L1 at a position corresponding to one finger making a gesture, and initializes the first layer L1 as a layer on which the finger is placed (hereinafter referred to as a "stay layer") (step S301). Upon this initialization, the navigation window display processing unit 16 starts display in the navigation window 41 (step S302). The display control subsequent to step S302 with respect to the navigation window 41 is supplementarily discussed in an example described later, with reference to <figref idref="f0012 f0013">Fig. 12</figref>.</p>
<p id="p0071" num="0071">Thereafter, the gesture determination unit 10 determines continuation of the one-finger hand gesture and movement direction of the hand (steps S303, S304, S305, and S310), and performs different processes depending on the determination results. Hereinafter, each process is discussed in correspondence to the gestures having a higher chance of being performed by a user.</p>
<p id="p0072" num="0072">When a user starts a parallel movement immediately after making a one-finger hand gesture, the program goes to step<!-- EPO <DP n="26"> --> S307 upon determination of stay area as the first layer L1 after "NO" and "YES" are determined in steps S304 and S305, respectively, where a character key being selected is switched in response to the movement direction and the movement distance of a feature point group of a finger per unit time (step S307). Following this, a program loop including steps S303 through S307 is repeated while the one-finger parallel movement of the hand continues, and the selection position in the layout of the character keys 100 is changed in accordance with the parallel movement per unit time.</p>
<p id="p0073" num="0073">When the user switches the gesture from the parallel movement to a downward-movement action after completing selection of a character key 100, the movement direction is determined as "down" in step S310 after "NO" is determined in both steps S304 and S305. Further, after the stay layer is determined as the second layer L2 in step S311, the program goes to step S312, in which the stay layer is changed from the first layer L1 to the second layer L2.</p>
<p id="p0074" num="0074">The program returns to step S303 after change of the stay layer as described above. When the user gesture is again switched from the downward-movement action to a parallel movement, the program goes to step S308 upon determination of the stay layer as the second layer L2 in step S306 after "NO" and "YES" are determined in steps S304 and S305, respectively. In step S308, the virtual keyboard control unit 11 selects a character corresponding to the movement direction of the hand as an input character from among the characters arranged on the second layer L2. Further, upon this selection, the input area display processing unit 17 displays the selected character in the input area 42 (step S309). Thereafter, the program turns to the main routine, and proceeds to retrieval of an estimated conversion candidate (step S4).</p>
<p id="p0075" num="0075">In the case in which the user continues downward-movement action with the intention of selecting a representative character on the third layer, the stay layer is changed from the<!-- EPO <DP n="27"> --> first layer L1 to the second layer L2 in response to the one-finger pushdown of the hand (step S312), and the downward-movement action is still detected in step S303 to which the program returned. Therefore, in this case, after determination in S304, S305, and S310, it is determined in step S311 that the stay layer is the second layer L2; upon this determination, the virtual keyboard control unit 11 performs steps S313 and S314, and the input area display processing unit 17 performs step S315.</p>
<p id="p0076" num="0076">In step S313, the stay layer is changed from the second layer L2 to the third layer L3; in step S314, a representative character disposed on the third layer L3 is selected as an input character. In step 315, the character selected in step S314 is displayed in the input area 42 (step S315). Thereafter, the program returns to the main routine, and proceeds to retrieval of the estimated conversion candidate (step S4).</p>
<p id="p0077" num="0077">After the one-finger downward-movement action by the hand is started, if the hand is raised without progressing toward the layout of characters, the stay layer is changed from the first layer L1 to the second layer L2 (step S312); thereafter, an upward-movement action is detected in step S303. Upon this detection, "NO" is determined in steps S304 and S305, after which it is determined in step S310 that the movement direction is upward and the program goes to step S316, after which it is determined in step S316 that the stay area is the second layer L2. The program goes to step S317 in response to these determinations, and the first layer L1 is established as the stay layer again, without selection of characters.</p>
<p id="p0078" num="0078">After a character is selected in step S308 and step S314, if the one-finger hand gesture is still continuously performed, the program goes through steps S1 and S2 in the main routine after steps S4 and S5 are performed, after which the character input acceptance process is started again. At this time as well, the stay layer is established as the first layer L1 (step S301), and the navigation window 41 showing the<!-- EPO <DP n="28"> --> selection position on the first layer L1 is displayed; therefore, a character key 100 corresponding to a next input character can be selected by subsequently performing parallel movement of the hand with which the previous selection of an input character has been completed. Although the user may raise the finger with which selection has been made after completion of the previous selection of an input character, upon detection of the upward-movement action, the program goes through steps S302 through S304, S305, S310, and S316, and then returns to step S303 after the first layer L1 is initialized as the stay layer in step S301 immediately after resuming the character input acceptance process; therefore, the upward-movement action is disregarded, and a state in which the first layer L1 is established as the stay layer is maintained.</p>
<p id="p0079" num="0079">In the case in which the one-finger hand gesture is released while it stays on the first layer L1, or is released after the downward-movement action, without creating subsequent parallel movement or falling down to the third layer L3, "YES" is determined in step S304, after which the program returns to step S1 in the main routine.</p>
<p id="p0080" num="0080">Additionally, in the character input acceptance process using the keyboard 11E (11Ea, 11Eb) shown in <figref idref="f0005">Figs. 6(1) and 6(2)</figref> for English-character input, the program goes through the same process as shown in <figref idref="f0009 f0010">Fig. 10</figref>, except that the processes relating to the third layer L3 are not performed. Meanwhile, in the character input acceptance process using the keyboard 11N composed of only the first layer L1 for numeric character input, the program goes through the same processes as steps S303 through S307 shown in <figref idref="f0009 f0010">Fig. 10</figref>; upon switchover of the gesture from parallel movement to downward-movement action, a character allocated to the character key 100 being selected is determined as an input character.</p>
<p id="p0081" num="0081"><figref idref="f0011">Fig. 11</figref> shows detailed procedures in step 7 (candidate selection process) in the main routine. This process is performed when the two-finger hand gesture is started with<!-- EPO <DP n="29"> --> respect to the display of a list of estimated conversion candidates in step S5, or the display of a list of connection candidates in step S9.</p>
<p id="p0082" num="0082">In first step S71, the candidate window display processing unit 18 initializes the display position of a leading candidate in the candidate window 43 as the selection position. Then, the gesture determination unit 10 determines the continuation of the two-finger hand gesture and the movement direction of a feature point group of the finger (steps S73, S74), and different procedures are carried out according to the determination results.</p>
<p id="p0083" num="0083">When the user continues parallel movement using the two-finger hand gesture, "NO" and "YES" are determined in steps S73 and S74, respectively, the program returns to step S75, and the candidate window display processing unit 18 changes the selection position according to the movement direction of the feature point group and the movement distance of the finger per unit time (step S75).</p>
<p id="p0084" num="0084">When the user completing selection of a candidate switches the two-finger hand gesture to a downward-movement action, "NO" is determined in steps S73 and S74, and "YES" is determined in step S76; afterward, the program goes to step S77, in which the candidate window display processing unit 18 determines the candidate being selected as an input character string.</p>
<p id="p0085" num="0085">The input area display processing unit 17, following the above determination, updates the display in the input area 42 with the determined character string (step S78).<br/>
Specifically, if an input character string is determined from the list of estimated conversion candidates displayed in step S5, a <i>kana</i> character string (unsettled display) displayed in the input area 42 is replaced by the determined character string. On the other hand, if an input character string is determined from the list of connection candidates displayed in step S9, the<!-- EPO <DP n="30"> --> determined character string is connected to the input character string displayed in the input area 42.</p>
<p id="p0086" num="0086">Upon completion of step S78, the program returns to the main routine, and the retrieval of a connection candidate with respect to the determined input character string is carried out (step S8).</p>
<p id="p0087" num="0087">Additionally, gestures aside from parallel movements and downward-movement actions are disregarded while the two-finger hand gesture continues. Further, the two-finger hand gesture is released prior to determination of selection of a candidate, "YES" is determined in step S73, and the program returns to step S1 in the main routine.</p>
<p id="p0088" num="0088"><figref idref="f0012 f0013">Fig. 12</figref> (<figref idref="f0012">Figs. 12-1</figref> and <figref idref="f0013">12-2</figref>) shows examples of displays developed in the display 4 in accordance with the one-finger hand gesture and the two-finger hand gesture. Hereinafter, display control is supplementarily discussed with respect to the navigation window 41 and the candidate window 43 with reference to this drawing.</p>
<p id="p0089" num="0089"><figref idref="f0012">Fig. 12(a)</figref> shows a display state of the display 4 immediately after one finger is detected in space, that is, when the character input acceptance process shown in <figref idref="f0009 f0010">Fig. 10</figref> is started, wherein the navigation window 41 and the blank input area 42 are shown.</p>
<p id="p0090" num="0090">In the navigation window 41 shown in <figref idref="f0012">Fig. 12(a)</figref>, the image of a character key 100 of the column "a" selected on the basis of initial data is disposed in the center. A user in this example moves the one-finger hand gesture down under a state in which the character key 100 of the column "a" is selected, and the downward-movement of the hand allows the stay layer to change from the first layer L1 to the second layer L2 (step S312 in <figref idref="f0009 f0010">Fig. 10</figref>). Upon this change, the display in the navigation window 41 is switched to an image showing a state in which each character arranged in the second layer L2 and the third layer L3 is viewed from overhead (<figref idref="f0012">Fig. 12(b)</figref>).<!-- EPO <DP n="31"> --></p>
<p id="p0091" num="0091">The user going through the navigation window 41 in <figref idref="f0012">Fig. 12(b)</figref> moves the one-finger hand gesture forward; following this movement, the display in the navigation window 41 is changed to a state in which a character "o" disposed in the movement direction is positioned in the center (<figref idref="f0012">Fig. 12(c)</figref>). Then, when the movement distance of the feature point group of a finger exceeds a threshold, steps S308 and S309 in <figref idref="f0009 f0010">Fig. 10</figref> are performed, and "o" is displayed in the input area 42. Further, the program returns to the main routine in <figref idref="f0007 f0008">Fig. 9</figref> and steps S4 and S5 are performed, to thereby display the candidate window 43 including a list of estimated conversion candidates with respect to "o."</p>
<p id="p0092" num="0092">According to the example shown in <figref idref="f0012 f0013">Fig. 12</figref>, the user still keeps the one-finger hand gesture after selection of "o," and the character input acceptance process is thus started again with the program going through steps S1 and S2 after performing steps S4 and S5. <figref idref="f0012">Fig. 12 (d)</figref> is an example of a display when the program returned to the character input acceptance process, wherein the navigation window 41 is displayed along with the input area 42 including the determined "o" and the candidate window 43 including a list of estimated conversion candidates.</p>
<p id="p0093" num="0093">Thereafter, the selection position on the first layer L1 is changed in accordance with the parallel movement of the one-finger hand gesture (step S307 in <figref idref="f0009 f0010">Fig. 10</figref>), and an image within a prescribed range is displayed in the navigation window 41, including the character key being selected, which is displayed in the center of the image, along with other character keys near the center key (<figref idref="f0012">Fig. 12(e), (f)</figref>).</p>
<p id="p0094" num="0094">According to the example shown in <figref idref="f0012 f0013">Fig. 12</figref>, the user selecting a character key 100 of the column "ha" carries out a downward-movement action again. In accordance with this downward-movement action, when the stay layer is changed from the first layer L1 to the second layer L2 (step S32 in <figref idref="f0009 f0010">Fig. 10</figref>), the display in the navigation window 41 is updated to a display showing a state in which characters arranged on the second layer<!-- EPO <DP n="32"> --> L2 and the third layer L3 are viewed from overhead (<figref idref="f0013">Fig. 12(g)</figref>). The user continues the downward-movement action after the update of display, and upon this downward-movement action, the stay layer is switched to the third layer L3 (step S313 in <figref idref="f0009 f0010">Fig. 10</figref>), and the navigation window 41 is changed to a state in which only the character "ha" disposed on the third layer is displayed (<figref idref="f0013">Fig. 12(h)</figref>).</p>
<p id="p0095" num="0095">Thereafter, the character "ha" is selected in step S314 in <figref idref="f0009 f0010">Fig. 10</figref>, and this character is added into the input area 42 subsequently in step S315; accordingly, the input character string is updated to "oha." Further, the program returns to the main routine, the retrieval of estimated conversion candidates based on the updated input character string is carried out (step S4 in <figref idref="f0007 f0008">Fig. 9</figref>), and the display in the candidate window 43 is updated subsequently in step S5 (<figref idref="f0013">Fig. 12(i)</figref>).</p>
<p id="p0096" num="0096">According to the example in <figref idref="f0012 f0013">Fig. 12</figref>, since the hand form is switched from a one-finger form to a two-finger form, the character input acceptance process is not resumed, and the navigation window 41 disappears from the screen shown in <figref idref="f0013">Fig. 12(i)</figref>. Further, the candidate selection process (step S7, <figref idref="f0011">Fig. 11</figref>) is started with the program going through step S6 in the main routine, and a display field for a leading candidate in the candidate window 43 is displayed with a specific color in accordance with initialization of the selection position (step S71 in <figref idref="f0011">Fig. 11</figref>). The colored position moves in accordance with update of the selection position (step S75 in <figref idref="f0011">Fig. 11</figref>) associated with subsequent parallel movement (<figref idref="f0013">Fig. 12(k)</figref>).</p>
<p id="p0097" num="0097">According to the example in <figref idref="f0012 f0013">Fig. 12</figref>, the hand gesture made with two fingers is switched over to a downward-movement action under a state in which a second candidate <i>"ohayou"</i> ("good morning" in Japanese) is selected. Upon this switchover, steps S77 and S78 in <figref idref="f0011">Fig. 11</figref> are performed to thereby update the display in the input area 41 to "<i>ohayou.</i>" Further, steps S8 and S9 are performed in the main routine to which the program returns after determination, and thus the display in the<!-- EPO <DP n="33"> --> candidate window is updated to display a list of connection candidates with respect to <i>"ohayou"</i> (<figref idref="f0013">Fig. 12(l)</figref>).</p>
<p id="p0098" num="0098">As discussed above, there are detailed descriptions concerning embodiments for accepting gestures for character input using the virtual keyboard 11J having a three-layered structure made up of the virtual plane (first layer L1) on which a plurality of character keys 100 is arranged, with each character key 100 having a plurality of characters allocated thereto, and the second layer L2 and the third layer L3 provided for each character key 100. In these descriptions, although there are no descriptions concerning gestures performed to directly determine <i>kana</i> characters and <i>kana</i> character strings selected by the character input acceptance process, a certain gesture having a form not included in the above descriptions, for example, a one-finger hand gesture in which a circle is drawn following selection of an input character, may be applied to this determination process.</p>
<p id="p0099" num="0099">In the embodiment discussed above, upon detection of a one-finger hand gesture, a virtual plane (first layer) including the position of the finger and the layout of character keys 100 is established along the surface of the display 4; however, the embodiment is not limited to this configuration, and the character keys 100 may be arranged upon start of the parallel movement of the one-finger hand gesture substantially parallel to the surface of the display 4 with the virtual plane along the movement direction as the first layer L1.</p>
<p id="p0100" num="0100">Additionally, the configuration of the virtual keyboard is not limited to the embodiment described above, and numerous different variations may be considered. For example, a virtual keyboard having a two-layered structure may be established by disposing a representative character of each character key 100 for <i>kana</i> character input at the center of the second layer L2 (position intersecting with the virtual axis C). If this virtual keyboard is used, after a parallel movement for selecting a character key 100 is switched to a downward-movement<!-- EPO <DP n="34"> --> action, upon switchover to a parallel movement again, a character disposed in the movement direction (excluding the representative character) is selected, while the representative character can be selected when action is suspended for a prescribed period of time without any parallel movement following the downward-movement action. Alternatively, characters excluding the central representative character may be selected as an input character by performing a gesture that moves the finger on the first layer L1 directly in a direction (obliquely downward) in which an intended character is disposed.</p>
<p id="p0101" num="0101">A layer on which characters are arranged is established not under the first layer L1, but over the first layer L1, and a mode for selecting a character key 100 may be shifted to a mode for selecting a character when a hand gesture is switched from a parallel movement to an upward-movement action. Further, a layer on which characters having a prescribed character type (for example, <i>hiragana</i> characters) are arranged is established under the first layer L1, and a layer on which characters having another character type (for example, <i>katakana</i> characters) are arranged is established over the first layer L1, wherein when the movement direction of the hand subjected to a parallel movement for selection of a character key 100 is switched to either an upward or downward direction, the character type of a character to be input may be switched.</p>
<p id="p0102" num="0102">The character layout is not limited to the layout parallel to the first layer L1; it is possible to establish, for example, a virtual keyboard having a structure in which a plurality of characters allocated to the same character key are disposed at individually different height positions along the virtual axis C passing through the character keys 100. When this virtual keyboard is used, the selection position on the first layer L1 is changed according to a parallel movement; when the gesture is switched from the parallel movement to a downward-movement action, a shift to a character selection operation is determined, and it is possible to switch character<!-- EPO <DP n="35"> --> selection in accordance with a change of movement distance thereafter while a navigation display showing a character being selected is carried out on the display 4. Additionally, it is possible to ensure the accurate selection of input characters by switching selection of a character in reverse order upon switchover from a downward-movement action to an upward-movement action, and by determining a character corresponding to a stop position of movement as an input character.</p>
<p id="p0103" num="0103">Additionally, except for the selection of character keys 100 and selection of characters, it is possible to, when necessary, set gesture forms easily accepted by a user as gesture forms relating to the processes (selection of a candidate, switchover of a virtual keyboard, cursor movement, deletion of an input character, etc.); the gestures are not limited to the above-mentioned embodiments. It is also possible to carry out the same character input acceptance process as the above-mentioned embodiments with a gesture not limited to a hand movement, but, for example, by performing a head- or line-of-sight-moving gesture.</p>
<p id="p0104" num="0104">The character input method according to the present invention including various types of embodiments is not limited to a wearable terminal, and may be introduced into other information-processing devices, such as smartphones. Further, it is also possible to accept character input in accordance with gestures performed in space at the front of the display based on the method for character input according to the present invention, with display areas such as the navigation window 41, the input area 42, and the candidate window 43 provided on a portion of a large-sized display.</p>
<heading id="h0011">DESCRIPTION OF THE REFERENCE SYMBOLS</heading>
<p id="p0105" num="0105">
<dl id="dl0001" compact="compact">
<dt>S</dt><dd>Information-processing device</dd>
<dt>1</dt><dd>Character input device</dd>
<dt>2</dt><dd>Gesture recognition device<!-- EPO <DP n="36"> --></dd>
<dt>3</dt><dd>Application</dd>
<dt>4</dt><dd>Display</dd>
<dt>10</dt><dd>Gesture determination unit</dd>
<dt>11</dt><dd>Virtual keyboard control unit</dd>
<dt>12</dt><dd>Input character string assembling unit</dd>
<dt>13</dt><dd>Candidate retrieving unit</dd>
<dt>14</dt><dd>Input character string determination unit</dd>
<dt>15</dt><dd>Display control unit</dd>
<dt>16</dt><dd>Navigation window display processing unit</dd>
<dt>17</dt><dd>Input area display processing unit</dd>
<dt>18</dt><dd>Candidate window display processing unit</dd>
<dt>40</dt><dd>Cursor</dd>
<dt>41</dt><dd>Navigation window</dd>
<dt>42</dt><dd>Input area</dd>
<dt>43</dt><dd>Candidate window</dd>
<dt>11J, 11E, 11E1, 11E2, 11T</dt><dd>Multilayered virtual keyboard</dd>
<dt>L1</dt><dd>First layer</dd>
<dt>L2</dt><dd>Second layer</dd>
<dt>L3</dt><dd>Third layer</dd>
<dt>C</dt><dd>Virtual axis</dd>
<dt>100</dt><dd>Character key</dd>
<dt>101</dt><dd>Key layout definition table</dd>
<dt>102</dt><dd>Dictionary database</dd>
</dl></p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="37"> -->
<claim id="c-en-01-0001" num="0001">
<claim-text>A character input method for detecting a gesture having a predetermined form as an operation for character input by means of a gesture recognition device (2) that recognizes a gesture performed in space, and determining a character to be input on the basis of the detection results in a computer configured to perform a prescribed process in response to input of characters, wherein
<claim-text>the computer is configured to register first definition information for layout of a plurality of character keys (100) on a virtual plane (L1), each having a plurality of characters allocated thereto, and second definition information for layout of each character allocated to each of the arranged character keys (100) outside the virtual plane (L1) at individually different positions relative to a virtual axis (C) passing through the character key (100),</claim-text>
<claim-text>wherein a maximum of five keys is allocated to each character key (100) arranged on the virtual plane (L1),</claim-text>
<claim-text>wherein the second definition information implies distributing each character except for one character among the plurality of characters allocated to the same character key (100) in a range not including a position intersecting with the virtual axis (C) on a second virtual plane (L2) along the virtual plane (L1), while disposing the one remaining character at the position intersecting with the virtual axis (C) on a third virtual plane (L3) opposite the character keys (100) with the second virtual plane (L2) interposed between the virtual plane (L1) and the third virtual plane (L3), and</claim-text>
<claim-text>wherein the computer is configured to perform:
<claim-text>step 1 of establishing a layout of the plurality of character keys (100) by applying the first definition information to the virtual plane (L1) including a detection position of a gesture in response to detection of a gesture that satisfies predetermined conditions;</claim-text>
<claim-text>step 2 of moving a selection position with respect to the layout of character keys in accordance with a gesture movement under the condition that the gesture<!-- EPO <DP n="38"> --> moving along the virtual plane (L1) on which the layout of character keys is established is detected; and</claim-text>
<claim-text>step 3 of selecting a character disposed at a moving destination of the gesture as a character to be input in response to detection of a gesture moving toward any of the plurality of characters on the second virtual plane (L2) and the third virtual plane (L3) arranged on the basis of the second definition information with respect to the character key (100) being selected after the movement direction of the gesture detected is switched from a direction along the virtual plane (L1) on which the character keys (100) are established to a direction moving downward from the virtual plane (L1) under the condition that a prescribed character key (100) in the layout is selected,</claim-text></claim-text>
<claim-text>in step 3, when a parallel movement is again detected after the movement direction of the gesture detected is switched to the direction moving downward, a character corresponding to the movement direction is selected as an input character from among characters arranged on the second virtual plane (L2), and</claim-text>
<claim-text>when a movement distance due to the downward-movement action exceeds a prescribed threshold without detection of a parallel movement after the movement direction of the gesture detected is switched to the direction moving downward, a character disposed on the third virtual plane (L3) is selected as an input character, and wherein a display control unit displays an image showing a character key or a character that can be selected by the gesture being detected in conjunction with operations of the character key selection unit and the input character selection unit.</claim-text></claim-text></claim>
<claim id="c-en-01-0002" num="0002">
<claim-text>The character input method according to claim 1, wherein step 2 includes a process of displaying an image of a region that is a part of an image showing the layout of the character keys (100) with a character key (100) selected along a movement direction of the gesture as a center, and step 3 includes a process of showing an image representing the positional relationship of each character allocated to the selected character key (100) in place of the display (4) in step 2 on the basis of the second definition information.<!-- EPO <DP n="39"> --></claim-text></claim>
<claim id="c-en-01-0003" num="0003">
<claim-text>The character input method according to claim 2, wherein<br/>
<!-- EPO <DP n="40"> -->the computer performs step 4 of extracting candidates for character string to be input adapting to the input character and displaying a list of the extracted candidates on the display (4) in response to selection of an input character in step 3, while performing step 5 of updating a display (4) indicating a selection position in the list under a state in which the list is displayed, in accordance with a gesture movement in response to detection of a gesture having a form different from the above-described gesture, moving along the virtual plane (L1) on which the character keys (100) are arranged.</claim-text></claim>
<claim id="c-en-01-0004" num="0004">
<claim-text>The character input method according to claim 1, wherein the gesture recognition device (2) is provided to recognize a gesture that moves a hand; the computer starts to perform step 2 with a position of any character key (100) in the layout of character keys established in step 1 as an initial selection position, after performing step 1 in response to detection of a one-finger hand gesture on the basis of the recognition results of the gesture recognition device (2); and step 2 is shifted to step 3 when it is detected that the one-finger hand gesture begins to move in a direction along the virtual axis (C) that passes through the character key (100) being selected.</claim-text></claim>
<claim id="c-en-01-0005" num="0005">
<claim-text>The character input method according to claim 1, wherein
<claim-text>the first definition information and the second definition information are registered in the computer for a plurality of types of virtual keyboards having different types of characters allocated to each character key (100);</claim-text>
<claim-text>a virtual keyboard (11J, 11E, 11E1, 11E2, 11T) to be activated is switched in response to detection of a gesture having a prescribed form different from the gesture relating to each of steps 1 through 3; and</claim-text>
<claim-text>the computer performs each of steps 1 through 3 using the first definition information and the second definition information corresponding to the activated virtual keyboard (11J, 11E, 11E1, 11E2, 11T).</claim-text></claim-text></claim>
<claim id="c-en-01-0006" num="0006">
<claim-text>The character input method according to claim 1, wherein a display region for input character string equipped with a cursor indicating an input position is provided on the display (4) connected to the computer, and when an input character is selected in step 2, the selected input character is inserted ahead of<!-- EPO <DP n="41"> --> the cursor in the display region for input character string.</claim-text></claim>
<claim id="c-en-01-0007" num="0007">
<claim-text>The character input method according to claim 6, wherein<br/>
a position of the cursor is moved in response to detection of a first gesture having a form different from the gesture relating to each of steps 1 through 3, and a character ahead of the cursor is deleted in response to detection of a second gesture having a form different from the gesture relating to each of steps 1 through 3 and the first gesture under a state in which an input character string having a prescribed length is displayed in the display region for input character string.</claim-text></claim>
<claim id="c-en-01-0008" num="0008">
<claim-text>An information-processing device (S) comprising: a gesture recognition device (2) for recognizing a gesture performed in space; a character input device (1) for detecting a gesture of predetermined form as an operation for character input by means of the gesture recognition device (2), and determining a character to be input on the basis of the detection results; and a display (4), wherein<br/>
the character input device (1) includes:
<claim-text>a definition information storage unit configured to register first definition information for layout of a plurality of character keys (100) on a virtual plane (L1), each having a plurality of characters allocated thereto; and second definition information for layout of each character allocated to each of the arranged character keys (100) outside the virtual plane (L1) at individually different positions relative to a virtual axis (C) passing through the character key (100);
<claim-text>wherein a maximum of five keys is allocated to each character key (100) arranged on the virtual plane (L1),</claim-text>
<claim-text>wherein the second definition information implies distributing each character except for one character among the plurality of characters allocated to the same character key (100) in a range not including a position intersecting with the virtual axis (C) on a second virtual plane (L2) along the virtual plane (L1), while disposing the one remaining character at the position intersecting with the virtual axis (C) on a third virtual plane (L3) opposite the character keys (100) with the second virtual plane (L2) interposed between the virtual plane (L1) and the third virtual plane (L3),</claim-text></claim-text>
<claim-text>a character key layout establishing unit configured to establish a layout of the plurality of character keys (100) by applying the first definition information to the virtual plane (L1) including a detection position of a gesture in response to detection<!-- EPO <DP n="42"> --> of a gesture that satisfies predetermined conditions;</claim-text>
<claim-text>a character key selection unit configured to move a selection position with respect to the layout of character keys in accordance with a gesture movement under the condition that the gesture moving along the virtual plane (L1) on which the layout of character keys is established is detected;</claim-text>
<claim-text>an input character selection unit configured to select a character disposed at a moving destination of the gesture as a character to be input in response to detection of a gesture moving toward any of the plurality of characters on the second virtual plane (L2) and the third virtual plane (L3) arranged on the basis of the second definition information with respect to the character key (100) being selected after the movement direction of the gesture detected is switched from a direction along the virtual plane (L1) on which the character keys (100) are established to a direction moving downward from the virtual plane (L1) under the condition that a prescribed character key (100) in the layout is selected,</claim-text>
<claim-text>wherein when a parallel movement is again detected after the movement direction of the gesture detected is switched to the direction moving downward, a character corresponding to the movement direction is selected as an input character from among characters arranged on the second virtual plane (L2), and</claim-text>
<claim-text>when a movement distance due to the downward-movement action exceeds a prescribed threshold without detection of a parallel movement after the movement direction of the gesture detected is switched to the direction moving downward, a character disposed on the third virtual plane (L3) is selected as an input character; and</claim-text>
<claim-text>a display control unit (15) configured to display an image showing a character key (100) or a character that can be selected by the gesture being detected in conjunction with operations of the character key selection unit and the input character selection unit.</claim-text></claim-text></claim>
<claim id="c-en-01-0009" num="0009">
<claim-text>The information-processing device (S) according to claim 8, wherein the display control unit (15) displays an image of a region that is a part of an image showing the layout of the character keys with a character key (100) selected along a movement direction of the gesture as a center, and shows an image representing the positional relationship of each character allocated to the selected character key (100) on the display (4) in response to start of the process of the input character selection<!-- EPO <DP n="43"> --> unit, in place of the display of character keys (100), on the basis of the second definition information.</claim-text></claim>
<claim id="c-en-01-0010" num="0010">
<claim-text>The information-processing device (S) according to claim 8, wherein the character input device (1) further comprises a candidate extraction unit (13) configured to extract candidates for input character string adapting to an input character in response to selection of the input character using the input character selection unit, wherein<br/>
the display control unit (15) displays a list of the candidates extracted by the candidate extraction unit (13) on the display (4) while updating a selection position of candidates in the list under a state in which the list is displayed, in accordance with a gesture movement in response to detection of a gesture having a form different from a gesture relating to the process of the character key selection unit, moving along the virtual plane (L1) on which the character keys (100) are arranged.</claim-text></claim>
<claim id="c-en-01-0011" num="0011">
<claim-text>The information-processing device (S) according to claim 9, wherein the character input device (1) further comprises a candidate extraction unit (13) configured to extract candidates for input character string adapting to an input character in response to selection of the input character using the input character selection unit, wherein<br/>
the display control unit (15) displays a list of the candidates extracted by the candidate extraction unit (13) on the display (4) while updating a selection position of candidates in the list under a state in which the list is displayed, in accordance with a gesture movement in response to detection of a gesture having a form different from a gesture relating to the process of the character key selection unit, moving along the virtual plane (L1) on which the character keys (100) are arranged.</claim-text></claim>
<claim id="c-en-01-0012" num="0012">
<claim-text>A computer program for character input configured to cause a computer in an information-processing device (S) equipped with a gesture recognition device (2) for recognizing a gesture performed in space to execute the method of claim 1.<!-- EPO <DP n="44"> --></claim-text></claim>
<claim id="c-en-01-0013" num="0013">
<claim-text>A non-temporary computer-readable recording medium that records a computer program for character input configured to cause a computer in an information-processing device (S) equipped with a gesture recognition device (2) for recognizing a gesture performed in space to execute the method of claim 1.</claim-text></claim>
</claims>
<claims id="claims02" lang="de"><!-- EPO <DP n="45"> -->
<claim id="c-de-01-0001" num="0001">
<claim-text>Ein Zeicheneingabeverfahren zum Erfassen einer Geste mit einer vorbestimmten Form als eine Operation zur Zeicheneingabe mittels einer Gestenerkennungsvorrichtung (2), die eine im Raum ausgeführte Geste erkennt, und zum Bestimmen eines auf der Grundlage der Erfassungsergebnisse in einen Computer einzugebenden Zeichens, der so konfiguriert ist, dass er als Reaktion auf die Eingabe von Zeichen einen vorgeschriebenen Prozess durchführt, wobei
<claim-text>der Computer konfiguriert ist, um Folgendes zu registrieren (<i>register</i>): erste Definitionsinformationen für die Anordnung (<i>layout</i>) einer Vielzahl von Zeichentasten (100) auf einer virtuellen Ebene (L1), denen jeweils eine Vielzahl von Zeichen zugeordnet ist, und zweite Definitionsinformationen für die Anordnung jedes Zeichens, das jeder der angeordneten Zeichentasten (100) zugeordnet ist, außerhalb der virtuellen Ebene (L1) an individuell unterschiedlichen Positionen relativ zu einer virtuellen Achse (C), die durch die Zeichentaste (100) verläuft,</claim-text>
<claim-text>wobei jeder auf der virtuellen Ebene (L1) angeordneten Zeichentaste (100) maximal fünf Tasten zugeordnet sind,</claim-text>
<claim-text>wobei die zweiten Definitionsinformationen das Verteilen jedes Zeichens mit Ausnahme eines Zeichens unter der Vielzahl von Zeichen, die derselben Zeichentaste (100) zugeordnet sind, in einem Bereich implizieren, der eine Position, die sich mit der virtuellen Achse (C) auf einer zweiten virtuellen Ebene (L2) schneidet, entlang der virtuellen Ebene (L1) nicht einschließt, während das eine verbleibende Zeichen an der Position, die sich mit der virtuellen Achse (C) schneidet, auf einer dritten virtuellen Ebene (L3) gegenüber den Zeichentasten (100) angeordnet wird, wobei die zweite virtuelle Ebene (L2) zwischen der virtuellen Ebene (L1) und der dritten virtuellen Ebene (L3) angeordnet ist, und</claim-text>
<claim-text>wobei der Computer dazu konfiguriert ist, Folgendes auszuführen:
<claim-text>Schritt 1: Einrichten einer Anordnung der Vielzahl von Zeichentasten (100) durch Anwenden der ersten Definitionsinformationen auf die virtuelle Ebene (L1), die eine Erfassungsposition einer Geste beinhaltet, als Reaktion auf die Erfassung einer Geste, die vorbestimmte Bedingungen erfüllt;</claim-text>
<claim-text>Schritt 2: Bewegen einer Auswahlposition in Bezug auf die Anordnung von Zeichentasten in Übereinstimmung mit einer Gestenbewegung unter der Bedingung, dass die Geste, die sich entlang der virtuellen Ebene (L1) bewegt, auf der die Anordnung von Zeichentasten eingerichtet ist, erfasst wird; und</claim-text>
<claim-text>Schritt 3: Auswählen eines Zeichens, das an einem Bewegungsziel der Geste angeordnet ist, als ein einzugebendes Zeichen in Reaktion auf die Erfassung einer Geste, die sich in Richtung eines der Vielzahl von Zeichen auf der zweiten virtuellen Ebene (L2) und der dritten virtuellen Ebene (L3) bewegt, die auf der Grundlage der zweiten Definitionsinformationen relativ zur ausgewählten Zeichentaste (100) angeordnet sind, die ausgewählt wird, nachdem die Bewegungsrichtung der erfassten Geste von einer Richtung entlang der virtuellen Ebene (L1), auf der die Zeichentasten (100) eingerichtet sind, in eine Richtung umgeschaltet wird, die sich von der virtuellen Ebene (L1) nach unten bewegt, unter der Bedingung, dass eine vorgeschriebene Zeichentaste (100) in der Anordnung ausgewählt ist,</claim-text><!-- EPO <DP n="46"> --></claim-text>
<claim-text>wobei in Schritt 3, wenn eine parallele Bewegung erneut erfasst wird, nachdem die Bewegungsrichtung der erfassten Geste auf die sich nach unten bewegende Richtung umgeschaltet wurde, ein Zeichen ausgewählt wird, das der Bewegungsrichtung entspricht, als ein Eingabezeichen aus den Zeichen, die auf der zweiten virtuellen Ebene (L2) angeordnet sind, und wobei</claim-text>
<claim-text>wenn eine Bewegungsdistanz aufgrund der Abwärtsbewegungs-Aktion einen vorgeschriebenen Schwellenwert überschreitet, ohne dass eine parallele Bewegung erfasst wird, nachdem die Bewegungsrichtung der erfassten Geste in die Abwärtsbewegungsrichtung umgeschaltet wird, ein Zeichen ausgewählt wird, das auf der dritten virtuellen Ebene (L3) angeordnet ist, als ein Eingabezeichen, und</claim-text>
<claim-text>wobei eine Anzeigesteuereinheit ein Bild anzeigt, das eine Zeichentaste oder ein Zeichen zeigt, das durch die erfasste Geste, in Verbindung mit Operationen der Zeichentasten-Auswahleinheit und der Eingabezeichen-Auswahleinheit ausgewählt werden kann.</claim-text></claim-text></claim>
<claim id="c-de-01-0002" num="0002">
<claim-text>Das Zeicheneingabeverfahren nach Anspruch 1, wobei<br/>
Schritt 2 einen Prozess des Anzeigens eines Bildes eines Bereichs beinhaltet, der ein Teil eines Bildes ist, das die Anordnung der Zeichentasten (100) bei einer Zeichentaste (100) zeigt, die entlang einer Bewegungsrichtung der Geste als Zentrum ausgewählt ist, und wobei Schritt 3 einen Prozess des Zeigens eines Bildes beinhaltet, das die Positionsbeziehung jedes Zeichens darstellt, das der ausgewählten Zeichentaste (100) zugeordnet ist, anstelle der Anzeige (4) in Schritt 2 auf der Grundlage der zweiten Definitionsinformationen.</claim-text></claim>
<claim id="c-de-01-0003" num="0003">
<claim-text>Das Zeicheneingabeverfahren nach Anspruch 2, wobei<br/>
der Computer den Schritt 4 des Extrahierens von Kandidaten als eine einzugebende Zeichenkette, die sich an das Eingabezeichen anpassen, und des Anzeigens einer Liste der extrahierten Kandidaten auf der Anzeige (4) als Reaktion auf die Auswahl eines Eingabezeichens in Schritt 3 durchführt, während der Computer Schritt 5 des Aktualisierens einer Anzeige (4), die eine Auswahlposition in der Liste unter einem Zustand anzeigt, in dem die Liste angezeigt wird, in Übereinstimmung mit einer Gestenbewegung als Reaktion auf die Erfassung einer Geste mit einer Form, die sich von der oben beschriebenen Geste unterscheidet, durchführt, die sich entlang der virtuellen Ebene (L1) bewegt, auf der die Zeichentasten (100) angeordnet sind.</claim-text></claim>
<claim id="c-de-01-0004" num="0004">
<claim-text>Das Zeicheneingabeverfahren nach Anspruch 1, wobei<br/>
die Gestenerkennungsvorrichtung (2) bereitgestellt wird, um eine Geste zu erkennen, die eine Hand bewegt; wobei der Computer beginnt, Schritt 2 auszuführen mit einer Position einer beliebigen Zeichentaste (100) in der in Schritt 1 festgelegten Anordnung von Zeichentasten als eine anfängliche Auswahlposition, nachdem er Schritt 1 als Reaktion auf die Erfassung einer Ein-Finger-Handgeste auf der Grundlage der Erkennungsergebnisse der Gestenerkennungsvorrichtung (2) ausgeführt hat; und wobei Schritt 2 zu Schritt 3 umgestellt wird, wenn erfasst wird, dass die Ein-Finger-Handgeste beginnt, sich in eine Richtung entlang der virtuellen Achse (C) zu bewegen, die durch die ausgewählte Zeichentaste (100) verläuft.<!-- EPO <DP n="47"> --></claim-text></claim>
<claim id="c-de-01-0005" num="0005">
<claim-text>Das Zeicheneingabeverfahren nach Anspruch 1, wobei
<claim-text>die ersten Definitionsinformationen und die zweiten Definitionsinformationen im Computer für eine Vielzahl von Typen von virtuellen Tastaturen registriert sind, die unterschiedliche Arten von Zeichen haben, die jeder Zeichentaste (100) zugeordnet sind;</claim-text>
<claim-text>eine zu aktivierende virtuelle Tastatur (11J, 11E, 11E1, 11E2, 11T) als Reaktion auf die Erfassung einer Geste mit einer vorgeschriebenen Form geschaltet wird, die sich von der Geste unterscheidet, die mit jeden der Schritte von 1 bis 3 verknüpft ist; und wobei</claim-text>
<claim-text>der Computer jeden der Schritte 1 bis 3 ausführt unter Verwendung der ersten Definitionsinformationen und der zweiten Definitionsinformationen entsprechend der aktivierten virtuellen Tastatur (11J, 11E, 11E1, 11E2, 11T).</claim-text></claim-text></claim>
<claim id="c-de-01-0006" num="0006">
<claim-text>Das Zeicheneingabeverfahren nach Anspruch 1, wobei<br/>
ein Anzeigebereich für eine Eingabezeichenfolge, der mit einem eine Eingabeposition anzeigenden Cursor ausgestattet ist, auf der mit dem Computer verbundenen Anzeige (4) vorgesehen ist, und wobei, wenn ein Eingabezeichen in Schritt 2 ausgewählt wird, das ausgewählte Eingabezeichen vor dem Cursor in den Anzeigebereich für die Eingabezeichenfolge eingefügt wird.</claim-text></claim>
<claim id="c-de-01-0007" num="0007">
<claim-text>Das Zeicheneingabeverfahren nach Anspruch 6, wobei<br/>
eine Position des Cursors bewegt wird als Reaktion auf die Erfassung einer ersten Geste mit einer Form, die sich von der Geste bezüglich jedes der Schritte 1 bis 3 unterscheidet, und wobei ein Zeichen vor dem Cursor gelöscht wird als Reaktion auf die Erfassung einer zweiten Geste mit einer Form, die sich von der Geste bezüglich jedes der Schritte 1 bis 3 und der ersten Geste unterscheidet, in einem Zustand, in dem eine Eingabezeichenfolge mit einer vorgeschriebenen Länge in dem Anzeigebereich für die Eingabezeichenfolge angezeigt wird.</claim-text></claim>
<claim id="c-de-01-0008" num="0008">
<claim-text>Eine Informationsverarbeitungsvorrichtung (S), die Folgendes umfasst: eine Gestenerkennungsvorrichtung (2) zum Erkennen einer im Raum ausgeführten Geste; eine Zeicheneingabevorrichtung (1) zum Erfassen einer Geste von vorbestimmter Form als eine Operation zur Zeicheneingabe mittels der Gestenerkennungsvorrichtung (2) und zum Bestimmen eines einzugebenden Zeichens auf der Grundlage der Erfassungsergebnisse; und eine Anzeige (4), wobei<br/>
die Zeicheneingabevorrichtung (1) Folgendes beinhaltet:
<claim-text>eine Definitionsinformations-Speichereinheit, die konfiguriert ist, um erste Definitionsinformationen für die Anordnung einer Vielzahl von Zeichentasten (100) auf einer virtuellen Ebene (L1) zu registrieren (<i>register</i>), wobei jeder eine Vielzahl von Zeichen zugeordnet ist; und zweite Definitionsinformationen für die Anordnung jedes Zeichens, das jeder der angeordneten Zeichentasten (100) außerhalb der virtuellen Ebene (L1) an individuell unterschiedlichen Positionen relativ zu einer virtuellen Achse (C) zugeordnet ist, die durch die Zeichentaste (100) verläuft;<!-- EPO <DP n="48"> --></claim-text>
<claim-text>wobei jeder Zeichentaste (100), die auf der virtuellen Ebene (L1) angeordnet ist, maximal fünf Tasten zugeordnet sind,</claim-text>
<claim-text>wobei die zweiten Definitionsinformationen das Verteilen jedes Zeichens mit Ausnahme eines Zeichens unter der Vielzahl von Zeichen, die derselben Zeichentaste (100) zugeordnet sind, in einem Bereich implizieren, der eine Position, die sich mit der virtuellen Achse (C) auf einer zweiten virtuellen Ebene (L2) schneidet, entlang der virtuellen Ebene (L1) nicht einschließt, während das eine verbleibende Zeichen an der Position, die sich mit der virtuellen Achse (C) schneidet, auf einer dritten virtuellen Ebene (L3) gegenüber den Zeichentasten (100) angeordnet wird, wobei die zweite virtuelle Ebene (L2) zwischen der virtuellen Ebene (L1) und der dritten virtuellen Ebene (L3) angeordnet ist,</claim-text>
<claim-text>eine Zeichentasten-Anordnungs-Erstellungseinheit, die so konfiguriert ist, dass sie eine Anordnung der Vielzahl von Zeichentasten (100) erstellt, indem sie die ersten Definitionsinformationen auf die virtuelle Ebene (L1) anwendet, die eine Erfassungsposition einer Geste in Reaktion auf die Erfassung einer Geste beinhaltet, die vorbestimmte Bedingungen erfüllt;</claim-text>
<claim-text>eine Zeichentasten-Auswahleinheit, die so konfiguriert ist, dass sie eine Auswahlposition relativ zur Anordnung von Zeichentasten in Übereinstimmung mit einer Gestenbewegung unter der Bedingung bewegt, dass die Geste, die sich entlang der virtuellen Ebene (L1) bewegt, auf der das Layout von Zeichentasten eingerichtet ist, erfasst wird;</claim-text>
<claim-text>eine Eingabezeichen-Auswahleinheit, die so konfiguriert ist, dass sie ein Zeichen, das an einem Bewegungsziel der Geste angeordnet ist, als ein einzugebendes Zeichen in Reaktion auf die Erfassung einer Geste auswählt, die sich in Richtung eines der Vielzahl von Zeichen auf der zweiten virtuellen Ebene (L2) und der dritten virtuellen Ebene (L3) bewegt, die auf der Grundlage der zweiten Definitionsinformationen relativ zur ausgewählten Zeichentaste (100) angeordnet sind, die ausgewählt wird, nachdem die Bewegungsrichtung der erfassten Geste von einer Richtung entlang der virtuellen Ebene (L1), auf der die Zeichentasten (100) eingerichtet sind, in eine Richtung umgeschaltet wird, die sich von der virtuellen Ebene (L1) nach unten bewegt, unter der Bedingung, dass eine vorgeschriebene Zeichentaste (100) in der Anordnung ausgewählt ist,</claim-text>
<claim-text>wobei, wenn eine parallele Bewegung erneut erfasst wird, nachdem die Bewegungsrichtung der erfassten Geste auf die sich nach unten bewegende Richtung umgeschaltet wurde, ein Zeichen ausgewählt wird, das der Bewegungsrichtung entspricht, als ein Eingabezeichen aus den Zeichen, die auf der zweiten virtuellen Ebene (L2) angeordnet sind, und wobei</claim-text>
<claim-text>wenn eine Bewegungsdistanz aufgrund der Abwärtsbewegungs-Aktion einen vorgeschriebenen Schwellenwert überschreitet, ohne dass eine parallele Bewegung erfasst wird, nachdem die Bewegungsrichtung der erfassten Geste in die Abwärtsbewegungsrichtung umgeschaltet wird, ein Zeichen ausgewählt wird, das auf der dritten virtuellen Ebene (L3) angeordnet ist, als ein Eingabezeichen; und</claim-text>
<claim-text>eine Anzeigesteuereinheit (15), die so konfiguriert ist, dass sie ein Bild anzeigt, das eine Zeichentaste (100) oder ein Zeichen zeigt, das durch die erfasste Geste, in Verbindung mit Operationen der Zeichentasten-Auswahleinheit und der Eingabezeichen-Auswahleinheit ausgewählt werden kann.</claim-text><!-- EPO <DP n="49"> --></claim-text></claim>
<claim id="c-de-01-0009" num="0009">
<claim-text>Die Informationsverarbeitungsvorrichtung (S) nach Anspruch 8, wobei die Anzeigesteuereinheit (15) ein Bild eines Bereichs anzeigt, der ein Teil eines Bildes ist, das die Anordnung der Zeichentasten mit einer Zeichentaste (100) zeigt, die entlang einer Bewegungsrichtung der Geste als Zentrum ausgewählt ist, und ein Bild anzeigt, das die Positionsbeziehung jedes Zeichens, das der ausgewählten Zeichentaste (100) zugeordnet ist, auf der Anzeige (4) darstellt, als Reaktion auf den Start des Prozesses der Eingabezeichen-Auswahleinheit anstelle der Anzeige von Zeichentasten (100) auf der Grundlage der zweiten Definitionsinformationen.</claim-text></claim>
<claim id="c-de-01-0010" num="0010">
<claim-text>Die Informationsverarbeitungsvorrichtung (S) nach Anspruch 8, wobei die Zeicheneingabevorrichtung (1) ferner eine Kandidatenextraktionseinheit (13) umfasst, die so konfiguriert ist, dass sie als Reaktion auf die Auswahl des Eingabezeichens unter Verwendung der Eingabezeichenauswahleinheit Kandidaten für eine an ein Eingabezeichen angepasste Eingabezeichenfolge extrahiert, wobei<br/>
die Anzeigesteuereinheit (15) eine Liste der von der Kandidatenextraktionseinheit (13) extrahierten Kandidaten auf der Anzeige (4) anzeigt, während sie eine Auswahlposition von Kandidaten in der Liste in einem Zustand aktualisiert, in dem die Liste angezeigt wird, in Übereinstimmung mit einer Gestenbewegung in Reaktion auf die Erfassung einer Geste, die eine Form hat, die sich von einer Geste unterscheidet, die sich auf den Prozess der Zeichentastenauswahleinheit bezieht, die sich entlang der virtuellen Ebene (L1) bewegt, auf der die Zeichentasten (100) angeordnet sind.</claim-text></claim>
<claim id="c-de-01-0011" num="0011">
<claim-text>Die Informationsverarbeitungsvorrichtung (S) nach Anspruch 9, wobei die Zeicheneingabevorrichtung (1) ferner eine Kandidatenextraktionseinheit (13) umfasst, die so konfiguriert ist, dass sie als Reaktion auf die Auswahl des Eingabezeichens unter Verwendung der Eingabezeichenauswahleinheit Kandidaten für eine an ein Eingabezeichen angepasste Eingabezeichenfolge extrahiert, wobei<br/>
die Anzeigesteuereinheit (15) eine Liste der von der Kandidatenextraktionseinheit (13) extrahierten Kandidaten auf der Anzeige (4) anzeigt, während sie eine Auswahlposition von Kandidaten in der Liste in einem Zustand aktualisiert, in dem die Liste angezeigt wird, in Übereinstimmung mit einer Gestenbewegung in Reaktion auf die Erfassung einer Geste, die eine Form hat, die sich von einer Geste unterscheidet, die sich auf den Prozess der Zeichentastenauswahleinheit bezieht, die sich entlang der virtuellen Ebene (L1) bewegt, auf der die Zeichentasten (100) angeordnet sind.</claim-text></claim>
<claim id="c-de-01-0012" num="0012">
<claim-text>Ein Computerprogramm zur Zeicheneingabe, das so konfiguriert ist, dass es einen Computer in einer informationsverarbeitenden Vorrichtung (S), die mit einer Gestenerkennungsvorrichtung (2) zur Erkennung einer im Raum ausgeführten Geste ausgestattet ist, veranlasst, das Verfahren nach Anspruch 1 auszuführen.</claim-text></claim>
<claim id="c-de-01-0013" num="0013">
<claim-text>Nicht temporäres computerlesbares Aufzeichnungsmedium, das ein Computerprogramm zur Zeicheneingabe aufzeichnet, das so konfiguriert ist, dass es einen Computer in einer informationsverarbeitenden Vorrichtung (S), die mit einer Gestenerkennungsvorrichtung (2) zur Erkennung einer im Raum ausgeführten Geste ausgestattet ist, veranlasst, das Verfahren nach Anspruch 1 auszuführen.</claim-text></claim>
</claims>
<claims id="claims03" lang="fr"><!-- EPO <DP n="50"> -->
<claim id="c-fr-01-0001" num="0001">
<claim-text>Un procédé d'entrée de caractères pour détecter un geste présentant une forme prédéterminée en tant qu'opération d'entrée de caractères au moyen d'un dispositif de reconnaissance de gestes (2) qui reconnaît un geste effectué dans l'espace, et pour déterminer un caractère à entrer, sur la base des résultats de détection, dans un ordinateur configuré pour exécuter un processus prescrit en réponse à l'entrée de caractères, sachant que
<claim-text>l'ordinateur est configuré pour enregistrer des premières informations de définition pour la disposition d'une pluralité de touches de caractères (100) sur un plan virtuel (L1), chacune présentant une pluralité de caractères qui lui sont attribués, et des deuxièmes informations de définition pour la disposition de chaque caractère attribué à chacune des touches de caractères (100) disposées en dehors du plan virtuel (L1) à des positions individuellement différentes par rapport à un axe virtuel (C) passant par la touche de caractère (100),</claim-text>
<claim-text>sachant qu'un maximum de cinq touches est attribué à chaque touche de caractère (100) disposée sur le plan virtuel (L1),</claim-text>
<claim-text>sachant que les deuxièmes informations de définition impliquent la distribution de chaque caractère, à l'exception d'un caractère, parmi la pluralité de caractères attribués à la même touche de caractère (100), dans une plage n'incluant pas une position coupant l'axe virtuel (C) sur un deuxième plan virtuel (L2) le long du plan virtuel (L1), tout en disposant le caractère restant à la position coupant l'axe virtuel (C) sur un troisième plan virtuel (L3) opposé aux touches de caractère (100), le deuxième plan virtuel (L2) étant interposé entre le plan virtuel (L1) et le troisième plan virtuel (L3), et</claim-text>
<claim-text>sachant que l'ordinateur est configuré pour exécuter :
<claim-text>l'étape 1 consistant à établir une disposition de la pluralité de touches de caractères (100) en appliquant les premières informations de définition au plan virtuel (L1) incluant une position de détection d'un geste en réponse à la détection d'un geste qui satisfait à des conditions prédéterminées ;</claim-text>
<claim-text>l'étape 2 consistant à déplacer une position de sélection par rapport à la disposition des touches de caractères conformément à un mouvement de geste, à la condition que le geste qui se déplace le long du plan virtuel (L1) sur lequel la disposition des touches de caractères est établie, soit détecté ; et</claim-text>
<claim-text>l'étape 3 de sélection d'un caractère disposé au niveau d'une destination de déplacement du geste en tant que caractère à entrer en réponse à la détection d'un geste qui se déplace vers l'un quelconque de la pluralité de caractères sur le deuxième plan virtuel (L2) et le troisième plan virtuel (L3) agencés sur la base des deuxièmes informations de définition par rapport à la touche de caractère (100) sélectionnée après que la direction de mouvement du geste détecté est commutée d'une direction le long du plan virtuel (L1) sur lequel les touches de caractère (100) sont établies à une direction qui se déplace vers le bas à partir du plan virtuel (L1), à la condition qu'une touche de caractère prescrite (100) dans la disposition soit sélectionnée,</claim-text><!-- EPO <DP n="51"> --></claim-text>
<claim-text>à l'étape 3, lorsqu'un mouvement parallèle est à nouveau détecté après que la direction du mouvement du geste détecté est commutée à la direction qui se déplace vers le bas, un caractère correspondant à la direction du mouvement est sélectionné comme caractère d'entrée parmi les caractères disposés sur le deuxième plan virtuel (L2), et</claim-text>
<claim-text>lorsqu'une distance de mouvement due à l'action du mouvement vers le bas dépasse un seuil prescrit sans détection d'un mouvement parallèle après que la direction de mouvement du geste détecté est commutée à la direction de mouvement vers le bas, un caractère disposé sur le troisième plan virtuel (L3) est sélectionné comme caractère d'entrée, et</claim-text>
<claim-text>sachant qu'une unité de commande d'affichage affiche une image montrant une touche de caractère ou un caractère qui peut être sélectionné par le geste détecté en conjonction avec des opérations de l'unité de sélection de touche de caractère et de l'unité de sélection de caractère d'entrée.</claim-text></claim-text></claim>
<claim id="c-fr-01-0002" num="0002">
<claim-text>Le procédé d'entrée de caractères d'après la revendication 1, sachant que<br/>
l'étape 2 inclut un processus consistant à afficher une image d'une région qui est une partie d'une image montrant la disposition des touches de caractères (100) avec une touche de caractère (100) sélectionnée le long d'une direction de mouvement du geste comme centre, et que l'étape 3 inclut un processus consistant à montrer une image représentant la relation positionnelle de chaque caractère attribué à la touche de caractère sélectionnée (100) à la place de l'affichage (4) à l'étape 2 sur la base des deuxièmes informations de définition.</claim-text></claim>
<claim id="c-fr-01-0003" num="0003">
<claim-text>Le procédé d'entrée de caractères d'après la revendication 2, sachant que<br/>
l'ordinateur exécute l'étape 4 consistant à extraire des candidats pour une chaîne de caractères à entrer s'adaptant au caractère d'entrée, et à afficher une liste des candidats extraits sur l'affichage (4) en réponse à la sélection d'un caractère d'entrée à l'étape 3, tout en exécutant l'étape 5 consistant à mettre à jour un affichage (4) indiquant une position de sélection dans la liste dans un état dans lequel la liste est affichée, conformément à un mouvement de geste en réponse à la détection d'un geste présentant une forme différente du geste décrit ci-dessus, se déplaçant le long du plan virtuel (L1) sur lequel les touches de caractères (100) sont disposées.</claim-text></claim>
<claim id="c-fr-01-0004" num="0004">
<claim-text>Le procédé d'entrée de caractères d'après la revendication 1, sachant que<br/>
le dispositif de reconnaissance de gestes (2) est prévu pour reconnaître un geste qui déplace une main ; l'ordinateur commence à exécuter l'étape 2 avec une position de n'importe quelle touche de caractère (100), dans la disposition des touches de caractère établie à l'étape 1, comme position de sélection initiale, après avoir exécuté l'étape 1 en réponse à la détection d'un geste de la main réalisé avec un seul doigt sur la base des résultats de reconnaissance du dispositif de reconnaissance de gestes (2) ; et l'étape 2 est déplacée vers l'étape 3 lorsqu'il est<!-- EPO <DP n="52"> --> détecté que le geste de la main réalisé avec un seul doigt commence à se déplacer dans une direction le long de l'axe virtuel (C) qui passe par la touche de caractère (100) en cours de sélection.</claim-text></claim>
<claim id="c-fr-01-0005" num="0005">
<claim-text>Le procédé d'entrée de caractères d'après la revendication 1, sachant que
<claim-text>les premières informations de définition et les deuxièmes informations de définition sont enregistrées dans l'ordinateur pour une pluralité de types de claviers virtuels présentant différents types de caractères attribués à chaque touche de caractère (100) ;</claim-text>
<claim-text>un clavier virtuel (11J, 11E, 11E1, 11E2, 11T) à activer est commuté en réponse à la détection d'un geste présentant une forme prescrite différente du geste relatif à chacune des étapes de 1 à 3 ; et que</claim-text>
<claim-text>l'ordinateur exécute chacune des étapes de 1 à 3 en utilisant les premières informations de définition et les deuxièmes informations de définition correspondant au clavier virtuel activé (11J, 11E, 11E1, 11E2, 11T).</claim-text></claim-text></claim>
<claim id="c-fr-01-0006" num="0006">
<claim-text>Le procédé d'entrée de caractères d'après la revendication 1, sachant que<br/>
une région d'affichage pour une chaîne de caractères d'entrée équipée d'un curseur indiquant une position d'entrée est prévue sur l'affichage (4) connecté à l'ordinateur, et que lorsqu'un caractère d'entrée est sélectionné à l'étape 2, le caractère d'entrée sélectionné est inséré devant le curseur dans la région d'affichage pour la chaîne de caractères d'entrée.</claim-text></claim>
<claim id="c-fr-01-0007" num="0007">
<claim-text>Le procédé d'entrée de caractères d'après la revendication 6, sachant que<br/>
une position du curseur est déplacée en réponse à la détection d'un premier geste présentant une forme différente du geste relatif à chacune des étapes de 1 à 3, et qu'un caractère en avant du curseur est supprimé en réponse à la détection d'un deuxième geste présentant une forme différente du geste relatif à chacune des étapes de 1 à 3 et du premier geste dans un état dans lequel une chaîne de caractères d'entrée présentant une longueur prescrite est affichée dans la région d'affichage pour chaîne de caractères d'entrée.</claim-text></claim>
<claim id="c-fr-01-0008" num="0008">
<claim-text>Un dispositif de traitement d'informations (S) comprenant : un dispositif de reconnaissance de gestes (2) pour reconnaître un geste effectué dans l'espace ; un dispositif d'entrée de caractères (1) pour détecter un geste de forme prédéterminée en tant qu'opération d'entrée de caractères au moyen du dispositif de reconnaissance de gestes (2), et pour déterminer un caractère à entrer sur la base des résultats de détection ; et un affichage (4), sachant que<br/>
le dispositif d'entrée de caractères (1) inclut :
<claim-text>une unité de stockage d'informations de définition configurée pour enregistrer des premières informations de définition pour la disposition d'une pluralité de touches de caractères (100) sur un plan virtuel (L1), chacune présentant une pluralité de<!-- EPO <DP n="53"> --> caractères qui lui sont attribués ; et des deuxièmes informations de définition pour la disposition de chaque caractère attribué à chacune des touches de caractères (100) disposées en dehors du plan virtuel (L1) à des positions individuellement différentes par rapport à un axe virtuel (C) passant par la touche de caractère (100) ;</claim-text>
<claim-text>sachant qu'un maximum de cinq touches est attribué à chaque touche de caractère (100) disposée sur le plan virtuel (L1),</claim-text>
<claim-text>sachant que les deuxièmes informations de définition impliquent la distribution de chaque caractère, à l'exception d'un caractère, parmi la pluralité de caractères attribués à la même touche de caractère (100), dans une plage n'incluant pas une position coupant l'axe virtuel (C) sur un deuxième plan virtuel (L2) le long du plan virtuel (L1), tout en disposant le caractère restant à la position coupant l'axe virtuel (C) sur un troisième plan virtuel (L3) opposé aux touches de caractère (100), le deuxième plan virtuel (L2) étant interposé entre le plan virtuel (L1) et le troisième plan virtuel (L3),</claim-text>
<claim-text>une unité d'établissement de disposition de touches de caractères configurée pour établir une disposition de la pluralité de touches de caractères (100) en appliquant les premières informations de définition au plan virtuel (L1) incluant une position de détection d'un geste en réponse à la détection d'un geste qui satisfait à des conditions prédéterminées ;</claim-text>
<claim-text>une unité de sélection de touches de caractères configurée pour déplacer une position de sélection par rapport à la disposition des touches de caractères conformément à un mouvement de geste, à la condition que le geste qui se déplace le long du plan virtuel (L1) sur lequel la disposition des touches de caractères est établie, soit détecté ;</claim-text>
<claim-text>une unité de sélection de caractères d'entrée configurée pour sélectionner un caractère disposé au niveau d'une destination de déplacement du geste en tant que caractère à entrer en réponse à la détection d'un geste qui se déplace vers l'un quelconque de la pluralité de caractères sur le deuxième plan virtuel (L2) et le troisième plan virtuel (L3) agencés sur la base des deuxièmes informations de définition par rapport à la touche de caractère (100) sélectionnée après que la direction de mouvement du geste détecté est commutée d'une direction le long du plan virtuel (L1) sur lequel les touches de caractère (100) sont établies à une direction qui se déplace vers le bas à partir du plan virtuel (L1), à la condition qu'une touche de caractère prescrite (100) dans la disposition soit sélectionnée,</claim-text>
<claim-text>sachant que, lorsqu'un mouvement parallèle est à nouveau détecté après que la direction du mouvement du geste détecté est commutée à la direction qui se déplace vers le bas, un caractère correspondant à la direction du mouvement est sélectionné comme caractère d'entrée parmi les caractères disposés sur le deuxième plan virtuel (L2), et que</claim-text>
<claim-text>lorsqu'une distance de mouvement due à l'action du mouvement vers le bas dépasse un seuil prescrit sans détection d'un mouvement parallèle après que la direction de mouvement du geste détecté est commutée à la direction de mouvement vers le bas, un caractère disposé sur le troisième plan virtuel (L3) est sélectionné comme caractère d'entrée ; et</claim-text>
<claim-text>une unité de commande d'affichage (15) configurée pour afficher une image montrant une touche de caractère (100) ou un caractère qui peut être sélectionné par le geste détecté en conjonction avec des opérations de l'unité de sélection de touche de caractère et de l'unité de sélection de caractère d'entrée.</claim-text><!-- EPO <DP n="54"> --></claim-text></claim>
<claim id="c-fr-01-0009" num="0009">
<claim-text>Le dispositif de traitement d'informations (S) d'après la revendication 8, sachant que l'unité de commande d'affichage (15) affiche une image d'une région qui est une partie d'une image montrant la disposition des touches de caractères avec une touche de caractère (100) sélectionnée le long d'une direction de mouvement du geste comme centre, et montre une image représentant la relation positionnelle de chaque caractère attribué à la touche de caractère sélectionnée (100) sur l'affichage (4) en réponse au démarrage du processus de l'unité de sélection de caractères d'entrée, à la place de l'affichage des touches de caractères (100), sur la base des deuxièmes informations de définition.</claim-text></claim>
<claim id="c-fr-01-0010" num="0010">
<claim-text>Le dispositif de traitement d'informations (S) d'après la revendication 8, sachant que le dispositif d'entrée de caractères (1) comprend en outre une unité d'extraction de candidats (13) configurée pour extraire des candidats pour une chaîne de caractères d'entrée s'adaptant à un caractère d'entrée en réponse à la sélection du caractère d'entrée en utilisant l'unité de sélection de caractères d'entrée, sachant que<br/>
l'unité de commande d'affichage (15) affiche une liste des candidats extraits par l'unité d'extraction de candidats (13) sur l'affichage (4) tout en mettant à jour une position de sélection des candidats dans la liste dans un état dans lequel la liste est affichée, conformément à un mouvement de geste en réponse à la détection d'un geste présentant une forme différente d'un geste relatif au processus de l'unité de sélection de touches de caractères, se déplaçant le long du plan virtuel (L1) sur lequel les touches de caractères (100) sont disposées.</claim-text></claim>
<claim id="c-fr-01-0011" num="0011">
<claim-text>Le dispositif de traitement d'informations (S) d'après la revendication 9, sachant que le dispositif d'entrée de caractères (1) comprend en outre une unité d'extraction de candidats (13) configurée pour extraire des candidats pour une chaîne de caractères d'entrée s'adaptant à un caractère d'entrée en réponse à la sélection du caractère d'entrée en utilisant l'unité de sélection de caractères d'entrée, sachant que<br/>
l'unité de commande d'affichage (15) affiche une liste des candidats extraits par l'unité d'extraction de candidats (13) sur l'affichage (4) tout en mettant à jour une position de sélection des candidats dans la liste dans un état dans lequel la liste est affichée, conformément à un mouvement de geste en réponse à la détection d'un geste présentant une forme différente d'un geste relatif au processus de l'unité de sélection de touches de caractères, se déplaçant le long du plan virtuel (L1) sur lequel les touches de caractères (100) sont disposées.</claim-text></claim>
<claim id="c-fr-01-0012" num="0012">
<claim-text>Un programme informatique pour l'entrée de caractères configuré pour amener un ordinateur, dans un dispositif de traitement d'informations (S) équipé d'un dispositif de reconnaissance de gestes (2) pour reconnaître un geste effectué dans l'espace, à exécuter le procédé d'après la revendication 1.</claim-text></claim>
<claim id="c-fr-01-0013" num="0013">
<claim-text>Un support d'enregistrement lisible par ordinateur non temporaire qui enregistre un programme informatique pour l'entrée de caractères configuré pour<!-- EPO <DP n="55"> --> amener un ordinateur, dans un dispositif de traitement d'informations (S) équipé d'un dispositif de reconnaissance de gestes (2) pour reconnaître un geste effectué dans l'espace, à exécuter le procédé d'après la revendication 1.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="56"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="135" he="83" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="57"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="158" he="232" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="58"> -->
<figure id="f0003" num="3,4"><img id="if0003" file="imgf0003.tif" wi="163" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="59"> -->
<figure id="f0004" num="5(1),5(2),5(3),5(4)"><img id="if0004" file="imgf0004.tif" wi="144" he="170" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="60"> -->
<figure id="f0005" num="6(1),6(2)"><img id="if0005" file="imgf0005.tif" wi="165" he="220" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="61"> -->
<figure id="f0006" num="7,8"><img id="if0006" file="imgf0006.tif" wi="147" he="220" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="62"> -->
<figure id="f0007" num="9-1"><img id="if0007" file="imgf0007.tif" wi="154" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="63"> -->
<figure id="f0008" num="9-2"><img id="if0008" file="imgf0008.tif" wi="165" he="226" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="64"> -->
<figure id="f0009" num="10-1"><img id="if0009" file="imgf0009.tif" wi="165" he="200" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="65"> -->
<figure id="f0010" num="10-2"><img id="if0010" file="imgf0010.tif" wi="165" he="136" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="66"> -->
<figure id="f0011" num="11"><img id="if0011" file="imgf0011.tif" wi="165" he="173" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="67"> -->
<figure id="f0012" num="12-1,12-1(a),12-1(b),12-1(c),12-1(d),12-1(e),12-1(f)"><img id="if0012" file="imgf0012.tif" wi="165" he="214" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="68"> -->
<figure id="f0013" num="12-2,12-2(g),12-2(h),12-2(i),12-2(j),12-2(k),12-2(l)"><img id="if0013" file="imgf0013.tif" wi="165" he="221" img-content="drawing" img-format="tif"/></figure>
</drawings>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="JP2014191782A"><document-id><country>JP</country><doc-number>2014191782</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0001">[0004]</crossref></li>
<li><patcit id="ref-pcit0002" dnum="US2015130728A1"><document-id><country>US</country><doc-number>2015130728</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0002">[0005]</crossref></li>
<li><patcit id="ref-pcit0003" dnum="JP2005196530A"><document-id><country>JP</country><doc-number>2005196530</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0003">[0006]</crossref></li>
<li><patcit id="ref-pcit0004" dnum="KR20090116591A"><document-id><country>KR</country><doc-number>20090116591</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0004">[0007]</crossref></li>
<li><patcit id="ref-pcit0005" dnum="JP2014082605A"><document-id><country>JP</country><doc-number>2014082605</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0005">[0008]</crossref></li>
<li><patcit id="ref-pcit0006" dnum="US2012120066A1"><document-id><country>US</country><doc-number>2012120066</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0006">[0009]</crossref></li>
<li><patcit id="ref-pcit0007" dnum="JP2014060640A"><document-id><country>JP</country><doc-number>2014060640</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0007">[0010]</crossref></li>
<li><patcit id="ref-pcit0008" dnum="JP2015146082A"><document-id><country>JP</country><doc-number>2015146082</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0008">[0011]</crossref></li>
</ul></p>
<heading id="ref-h0003"><b>Non-patent literature cited in the description</b></heading>
<p id="ref-p0003" num="">
<ul id="ref-ul0002" list-style="bullet">
<li><nplcit id="ref-ncit0001" npl-type="b"><article><atl/><book><book-title>New Mobile Era Realized by Gesture Interface - Space is stereoscopically captured with a compact, low-cost range image sensor, TERA</book-title><imprint><name>Public Relations Department, NTT Comware Corporation</name><pubdate>20140000</pubdate></imprint><vid>59</vid></book></article></nplcit><crossref idref="ncit0001">[0003]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
