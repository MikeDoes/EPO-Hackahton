<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP14730078B1" file="EP14730078NWB1.xml" lang="en" country="EP" doc-number="3004924" kind="B1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSK..HRIS..MTNORS..SM..................</B001EP><B003EP>*</B003EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  2100000/0</B007EP></eptags></B000><B100><B110>3004924</B110><B120><B121>EUROPEAN PATENT SPECIFICATION</B121></B120><B130>B1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>14730078.4</B210><B220><date>20140605</date></B220><B240><B241><date>20151124</date></B241><B242><date>20190409</date></B242></B240><B250>en</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>201361831647 P</B310><B320><date>20130606</date></B320><B330><ctry>US</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20160413</date><bnum>201615</bnum></B430><B450><date>20211006</date><bnum>202140</bnum></B450><B452EP><date>20210416</date></B452EP></B400><B500><B510EP><classification-ipcr sequence="1"><text>G01S  17/894       20200101AFI20210319BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>G01S   7/484       20060101ALI20210319BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>G01S   7/4911      20200101ALI20210319BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>G01S   7/4915      20200101ALI20210319BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G01S   7/4915      20130101 LI20140813BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>G01S  17/894       20200101 LI20200110BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>G01S   7/4911      20130101 LI20140813BHEP        </text></classification-cpc><classification-cpc sequence="4"><text>G01S   7/484       20130101 FI20140812BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>SENSORSYSTEM MIT AKTIVER BELEUCHTUNG</B542><B541>en</B541><B542>SENSOR SYSTEM WITH ACTIVE ILLUMINATION</B542><B541>fr</B541><B542>SYSTÈME DE CAPTEUR À ÉCLAIRAGE ACTIF</B542></B540><B560><B561><text>EP-A2- 2 017 651</text></B561><B561><text>US-A1- 2004 263 510</text></B561><B561><text>US-A1- 2005 162 638</text></B561><B561><text>US-A1- 2005 190 206</text></B561><B561><text>US-A1- 2013 002 823</text></B561></B560></B500><B700><B720><B721><snm>OGGIER, Thierry</snm><adr><str>7602 Hollanderry P1</str><city>Cupertino, CA 95014</city><ctry>US</ctry></adr></B721><B721><snm>DESCHLER, Mathias</snm><adr><str>Route Saint-Nicolas-de-Flüe 4a</str><city>1700 Fribourg</city><ctry>CH</ctry></adr></B721><B721><snm>KALOUSTIAN, Stéphane</snm><adr><str>Niederhäusernstrasse 14</str><city>3086 Zimmerwald</city><ctry>CH</ctry></adr></B721></B720><B730><B731><snm>Heptagon Micro Optics Pte. Ltd.</snm><iid>101401855</iid><irf>PC930270EP</irf><adr><str>26 Woodlands Loop</str><city>Singapore 738317</city><ctry>SG</ctry></adr></B731></B730><B740><B741><snm>Iqbal, Md Mash-Hud</snm><iid>101419109</iid><adr><str>Marks &amp; Clerk LLP 
62-68 Hills Road</str><city>Cambridge CB2 1LA</city><ctry>GB</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B860><B861><dnum><anum>EP2014001526</anum></dnum><date>20140605</date></B861><B862>en</B862></B860><B870><B871><dnum><pnum>WO2014195020</pnum></dnum><date>20141211</date><bnum>201450</bnum></B871></B870></B800></SDOBI>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001"><b>FIELD OF THE INVENTION</b></heading>
<p id="p0001" num="0001">The present invention relates to vision sensors based on an active illumination. The invention improves image quality of an active imaging system, improves the dynamic range and its background light stability. Such sensors typically are used to detect and measure their surroundings, objects or people.</p>
<heading id="h0002"><b>BACKGROUND ART</b></heading>
<p id="p0002" num="0002">Active illuminations are implemented in many camera designs to improve the camera measurement results. Camera is understood as an electro-optical device containing an image sensor with an array of pixels.</p>
<p id="p0003" num="0003">In many machine vision applications, active illuminations are used to guarantee for certain light intensity level on the camera system. By doing so. the performance of an active system becomes less prone to lighting changes in its surrounding. The stability and reliability is improved. In addition, by adding illumination power to the system, shorter exposures can be set and higher frame rates achieved.</p>
<p id="p0004" num="0004">Other sensors with an active illumination camera system use specific properties of the object of interest in the scene, e.g. an application might use reflectors in the scene that can easily be recognized and possibly tracked.<!-- EPO <DP n="2"> --></p>
<p id="p0005" num="0005">Other applications are using specific reflection properties of the object of interest such as the reflection properties of eyes. The typical back-reflection of the eyes captured by a system with active illumination can be used to detect and track eyes and to count the eye blinking to e.g. check a driver's drowsiness and build a fatigue measurement sensor.</p>
<p id="p0006" num="0006">In interferometers, an active system illuminates the object of interest and a reference target. Based on the interference of the reflections, depths can be measured and analyzed.</p>
<p id="p0007" num="0007">Other active systems modulate the illumination to get more information about its surroundings. Such illuminations are typically modulated in time (temporal modulation) or in space and can be applied to e.g. measure distances to map the environment in 3D. Temporal modulation is implemented in so-called time-of-flight (TOF) cameras (indirect and direct); spatial modulation is used in triangulation-based depth measurement system, also called structured light technology.</p>
<p id="p0008" num="0008">Time of flight image sensor pixels are dedicated pixels designs to guarantee an extremely fast transfer of the photo-generated charges to their storage nodes. Higher modulation frequencies result in better depth noise performance. Therefore, TOF demodulation pixels typically work in the range of a few tens of MHz up to a several hundreds of MHz. Further, TOF pixels often include background light suppression on pixel-level, in order to increase the dynamic range of the TOF imaging system. In most implementations, TOF pixels include two storage nodes to store and integrate two sampled signals per pixel.<!-- EPO <DP n="3"> --></p>
<p id="p0009" num="0009"><patcit id="pcit0001" dnum="US2013002823A1"><text>US 2013/002823 A1</text></patcit> describes an image generating apparatus may include a first reflector and a second reflector.</p>
<p id="p0010" num="0010"><patcit id="pcit0002" dnum="US2005190206A1"><text>US 2005/190206 A1</text></patcit> describes a method for determining a pixel gray scale value image, particularly for a multi-dimensional image system.</p>
<p id="p0011" num="0011"><patcit id="pcit0003" dnum="US2005162638A1"><text>US 2005/162638 A1</text></patcit> describes apparatus, method, and program for generating range-image-data.</p>
<p id="p0012" num="0012"><patcit id="pcit0004" dnum="EP2017651A2"><text>EP 2 017 651 A2</text></patcit> describes reference pixel array with varying sensitivities for TOF sensor.</p>
<p id="p0013" num="0013"><patcit id="pcit0005" dnum="US2004263510A1"><text>US 2004/263510 A1</text></patcit> describes methods and systems for animating facial features and methods and systems for expression transformation.<!-- EPO <DP n="4"> --></p>
<heading id="h0003"><b>DISCLOSURE OF THE INVENTION</b></heading>
<p id="p0014" num="0014">The robustness of all actively illuminated imaging systems benefit if background light signal such as sun light can be removed. Most of the time, background light signal cancellation is achieved by the acquisition of two consecutive images, one with illumination turned on, one with the illumination turned off. The subtraction of the two images results in an image including the intensity information of the active illumination only. The drawback of such an approach is first that the system needs to acquire two separate images. The scene or object in the scene might change different from image to image and the background subtraction in that case not ideal. Further, the two acquired images need to handle the full dynamic signal range from the background light and the active light. Even though the background light signal is not required, it eats up the dynamic range of the system.</p>
<p id="p0015" num="0015">It is an object of this invention to provide an imaging system with improved dynamic range. It is a further objective of this invention to provide an imaging system for three-dimensional measurement with improved resolution and less optical power consumption. It is further an object of this invention to provide a two-dimensional intensity and a three-dimensional imaging system.</p>
<p id="p0016" num="0016">According to the present invention, these objectives are achieved particularly through the features of the independent claims. In addition, further advantageous embodiments follow from the dependent claims and the description.<!-- EPO <DP n="5"> --></p>
<p id="p0017" num="0017">According to the present invention an imaging system includes an imaging sensor. The imaging system is adapted to process an image of a scene being illuminated by at least two different illumination sources each having a wavelength in the near infrared range. Preferably, the wavelength is between 800 and 1000nm. The illumination sources may be placed next to the imaging sensor and may be synchronized with the image sensor. The at least two illumination sources may be placed to minimize occlusion.</p>
<p id="p0018" num="0018">In some embodiments, the wavelengths of the at least two different illumination sources are different, e.g. at least one illumination source is at around 850 nm and at least one illumination source is at around 940nm.</p>
<p id="p0019" num="0019">The at least two different illumination sources include at least one structured illumination source. For example, in case of the image sensor being a time-of-flight image sensor, this provides the benefit of the advantages provided by the time of flight measurement and the structured illumination source measurement.</p>
<p id="p0020" num="0020">The at least two different illumination sources include at least one uniform illumination source.</p>
<p id="p0021" num="0021">The at least two different illumination sources are each in the near infrared range but include at least one uniform illumination source and at least one structured illumination source. The acquisition of an image based on the structured illumination source may be interleaved with the acquisitions based on the uniform illumination source. The actual raw images based on the structured illumination<!-- EPO <DP n="6"> --> source do not represent its acquired environment in a conventional colour or intensity representation. For this reason, state-of-the art systems based on structured illumination sources add a second image sensing device, typically an RGB sensor. By implementing two different illumination sources as proposed in the invention, for example, one structured illumination source and one uniform illumination source, the imaging system may derive the depth information and generate a representative intensity (or black and white) image by the same image sensor. This further allows an easy and one to one mapping of the 3D map onto the 2D intensity image. In case the image sensor is a TOF image sensor, the TOF image sensor and the two different illumination sources can be further temporally modulated and synchronized.</p>
<p id="p0022" num="0022">In some embodiments, the structured illumination source and the uniform illumination source, each have similar central wavelengths, both preferably between 800 and 1000nm. In such an embodiment, the image from the structured illumination source and the image from the uniform illumination source can each be imaged throughout the same optical path and a narrow optical bandpass filter can be implemented in the optical path. The implementation of a narrow bandpass filter allows to optically blocking out as much of the background light signal as possible.</p>
<p id="p0023" num="0023">The image sensor is a time of flight sensor.</p>
<p id="p0024" num="0024">In some embodiments, the at least two different illumination sources include at least one illumination source which is temporally modulated during exposure.. Appropriate temporal modulation scheme enable to reduce artefacts caused by<!-- EPO <DP n="7"> --> changing scene or moving objects during the acquisition and further enable to avoid interferences from other imaging system in the area.</p>
<p id="p0025" num="0025">In some embodiments, the at least two different illumination sources are on the same emitting die. This is of special interest, in case the at least two illuminations sources are structured illumination sources and are both temporally modulated . A system set up with the at least two structured illumination sources on the same emitting die and modulate them in sync with the image sensor allows higher information density level on the structured image.</p>
<p id="p0026" num="0026">Moreover, the invention is directed to an imaging method using an imaging sensor. Images of a scene being illuminated by at least two different illumination sources each having a wavelength in the near infrared range is processed. In a variant, the wavelengths of the at least two different illumination sources are different. The at least two different illumination sources include at least one structured illumination source. The at least two different illumination sources include at least one uniform illumination source. The image sensor used is a time of flight sensor. In a variant, the at least two different illumination sources include at least one uniform illumination source modulated with a modulation frequency below the modulation frequency used in time of flight measurement. In a variant, the at least one structured illumination source is temporally modulated during exposure.. In a variant, the at least two different illumination sources are on the same light emitting die.</p>
<p id="p0027" num="0027">According to the present invention, an imaging system including a time of flight sensor is adapted to use an illumination source having a modulation frequency<!-- EPO <DP n="8"> --> below the modulation frequency used to perform a 3 dimensional time of flight measurement.</p>
<p id="p0028" num="0028">In preferred specific embodiments, the present invention proposes to implement in the imaging system a time-of-flight sensors or architectures with typically two storage nodes per pixel, preferably even containing some sort of in-pixel background suppressing capability. Further, in the actively illuminated imaging system, the time of flight image sensor and the illumination source are controlled with such low modulation frequencies that the actual time-of-flight of the signal has a negligible effect on the sampling signals. Further, the invention proposes to implement an acquisition timing on the imaging system that results in a number of samples and acquisitions that are not sufficient to perform actual time-of-flight measurements and base the image evaluation on those acquisitions only. Time-of-flight pixels include in most practical implementation two storage nodes and capture at least two but most commonly four consequent but phase delayed images to derive depth information. In this embodiment, one storage node of the pixel is preferably used to integrate background light only, which then is subtracted from the other storage node, in which the background light signal together with the actively emitted and reflected light is integrated. The acquisition and integration of the signals, and their transfer to the two time-of-flight pixel storage nodes is preferably repeated and interleaved many times during an acquisition. The resulting differential pixel output will then be a representation of the actively emitted signal only, which makes the system more robust with respect to changing lighting conditions in the surroundings.<!-- EPO <DP n="9"> --></p>
<p id="p0029" num="0029">In some embodiments, the modulation frequency is between 100Hz and 1MHz. The slower modulation reduces power consumption of the imaging system compared to toggling at a few tens of MHz in state-of-the-art time-of-flight measurement systems. Further, it reduces the speed requirement of the illumination sources and driver and increases the modulation and demodulation efficiency. Highly efficient high power light sources such as high power LEDs that are typically too slow for time-of-flight measurements can be implemented.</p>
<p id="p0030" num="0030">In some embodiments, the imaging system is adapted to perform a direct subtraction on the level of a pixel of the time-of-flight sensor. The on-pixel background light cancellation of time-of-flight pixels helps in other systems with active illumination sources to avoid saturation due to background light. In addition, the requirements to the analogue to digital conversion are relaxed since the background level is already subtracted on pixel-level and does not need to be converted anymore. Further, light modulation within a system with active illumination, interleaving the integration of signal light and background light during the same frame acquisition, and integrating over several cycles reduces challenges of motion in the scene, and with appropriate temporal modulation schemes even enables parallel operation of several systems with reduced interferences.</p>
<p id="p0031" num="0031">The imaging system is adapted to use a structured illumination source. The structured illumination source is synchronized with the time-of-flight image sensor and is used to capture the image based on the structured illumination source and to derive depth information.<!-- EPO <DP n="10"> --></p>
<p id="p0032" num="0032">The imaging system includes a time-of-flight image sensor and at least one illumination source. In some embodiments, the time-of-flight image sensor is used to sample the back-reflected light. The evaluation of the signal is based on a number of acquisitions and acquired samples per frame, which are not sufficient to derive depth information based on the time-of-flight principle.</p>
<p id="p0033" num="0033">In some embodiments, the imaging system is adapted to use a pseudo-random modulated illumination source, so that different interferences between different acquisition systems are minimized.</p>
<p id="p0034" num="0034">In some embodiments, the imaging system is adapted to use at least two illumination sources having at least two different wavelengths. By doing a differential readout or on-pixel subtraction, the difference image of the two illumination sources can directly be measured. This enables, for example, to build a highly robust eye tracking system.</p>
<p id="p0035" num="0035">In some embodiments, the present invention provides an imaging system which uses a time-of-flight sensor, wherein an illumination source has a modulation frequency below the modulation frequency used to perform a three dimensional time of flight measurement.</p>
<p id="p0036" num="0036">In some embodiments, the imaging system is adapted to acquire a reduced number of samples per frame than required to perform time of flight measurements.</p>
<p id="p0037" num="0037">In some embodiments, the timing of an imaging system including a time-of-flight sensor and at least one illumination source is adapted to acquire a reduced number of samples per frame than required to derive time of flight measurements.<!-- EPO <DP n="11"> --></p>
<heading id="h0004"><b>BRIEF DESCRIPTION OF THE DRAWINGS</b></heading>
<p id="p0038" num="0038">The herein described invention will be more fully understood from the detailed description given herein below and the accompanying drawings which should not be considered limiting to the invention described in the appended claims. The drawings are showing:
<dl id="dl0001">
<dt>Fig. 1</dt><dd>a) shows a diagram of the state-of-the-art TOF pixel with the different building blocks. b) to d) plot prior art of timing diagrams and samplings to derive depth measurement based on the time-of-flight principle;</dd>
<dt>Fig. 2</dt><dd>shows the most common used prior art acquisition timing of the four samples required to derive depth measurement on time-of-flight 3 dimensional imaging systems;</dd>
<dt>Fig. 3</dt><dd>a) illustrates an imaging system 100 according to an example with the time-of-flight image sensor 110 and the active illumination source 120, both controlled and synchronized by the controller 140, b) a rough timing diagram according to the example and c) a more detailed timing diagram according to the example;</dd>
<dt>Fig. 4</dt><dd>shows a imaging system 100 according to an example based on a structured illumination source 121;</dd>
<dt>Fig. 5</dt><dd>illustrates an imaging system 100 according to the invention for eye / pupil detection applications based on two illumination sources 122 and<!-- EPO <DP n="12"> --> 123 with different wavelengths in the near infrared range and a time-of-flight image sensor (110);</dd>
<dt>Fig. 6</dt><dd>shows a imaging system 105 according to an example, that includes a first structured illumination source 121 and a second differently structured illumination source 125;</dd>
<dt>Fig. 7</dt><dd>shows an embodiment of an imaging system according to the invention, which enables to gather three-dimensional and colour or grey-scale information from the scene / object through the same optical path and image sensor. a) illustrates the operation of the acquisition to acquire depth information. A sample greyscale image captured with a structured illumination source is shown in c). b) plots the imaging system and its operation during the acquisition of the greyscale image and the resulting greyscale image is plotted in d); and</dd>
<dt>Fig. 8</dt><dd>shows an actual design according to the invention.</dd>
</dl></p>
<heading id="h0005"><b>MODE(S) FOR CARRYING OUT THE INVENTION</b></heading>
<p id="p0039" num="0039">Time-of-flight imaging systems area capable to determine the time it takes to the emitted light to travel from the measurement system to the object and back. The emitted light signal is reflected by an object at a certain distance. This distance corresponds to a time delay from the emitted signal to the received signal caused by the travel time of the light from the measurement system to the object and back. In indirect time-of-flight measurements, the distance-dependant time delay<!-- EPO <DP n="13"> --> corresponds to a phase delay of the emitted to the received signals. Further, the received signal not only includes the back-reflected emitted signal, but may also have background light signal from e.g. the sun or other light sources. A state-of-the-art time-of-flight pixel is illustrated in <figref idref="f0001">Fig. 1a</figref>. The time-of-flight pixel includes a photo-sensitive area P, connected with a first switch SW1 to storage node C1 and connected with a second switch SW2 to storage node C2. The sampling of the photo-generated electrons is done by closing switch SW1 and opening SW2 or vice versa. The switches SW1 and SW2 are synchronized to an illumination source by a controller. In order to enable reasonable time of flight measurements, the switches SW1 and SW2 and illumination sources need to operate in the range of around 10 MHz to above 200 MHz and the photo-generated charges have to be transferred from the photo-sensitive area P to either storage node C1 or C2 within a few ns. Time-of-flight pixels are specifically designed to reach such high-speed samplings. Possible implementations of such high-speed time-of-flight pixel architectures are described in patents <patcit id="pcit0006" dnum="US5856667A"><text>US5'856'667</text></patcit>, <patcit id="pcit0007" dnum="EP1009984B1"><text>EP1009984B1</text></patcit>, <patcit id="pcit0008" dnum="EP1513202B1"><text>EP1513202B1</text></patcit> or <patcit id="pcit0009" dnum="US7884310B2"><text>US7'884'310B2</text></patcit>. The output circuitry C of the time-of-flight pixel typically includes a readout amplifier and a reset node. In many time-of-flight pixel implementations, the in-pixel output circuitry C further includes a common level removal or background subtraction circuitry of the two samples in the storage nodes C1 and C2. Such in-pixel common signal level removal drastically increases the dynamic range of time-of-flight pixels. Possible implementations that perform a common level removal of the samples while integrating are presented in <patcit id="pcit0010" dnum="WO2009135952A2"><text>PCT publication WO2009135952A2</text></patcit> and in patent <patcit id="pcit0011" dnum="US7574190B2"><text>US7574190B2</text></patcit>. An approach of another common signal level subtraction of the samples done after the exposure during the readout cycle of the data is described in US patent <patcit id="pcit0012" dnum="US7897928B2"><text>US7897928B2</text></patcit>.<!-- EPO <DP n="14"> --> Commercially available 3 dimensional time-of-flight measurement systems are either based on sine wave or pseudo-noise modulation. Both require at least three samples to derive phase or depth information respectively, offset, and amplitude information. For design simplification and signal robustness reasons, time-of-flight systems commonly sample four times the impinging light signal. However, the most sensitive and therefore most widely used time-of-flight pixel architecture includes only two storage nodes, as sketched in <figref idref="f0001">Fig. 1a</figref>. In order to get the four samples, the system then requires the acquisition of at least two subsequent images. The timing diagram of the sine wave sampling is sketched in <figref idref="f0001">Fig. 1b</figref>. This received light signal generates on the pixel from <figref idref="f0001">Fig. 1a</figref> a photocurrent, which is then sampled and integrated during a first exposure E1, as represented in <figref idref="f0001">Fig. 1b</figref>. The first exposure E1 is followed by a readout RO1. A second exposure E2 acquires the samples delayed by 90° compared to E1. After the exposure E2, the newly acquired samples are readout RO2. At this point in time D, all data is ready to enable to determine phase, offset and amplitude information of the received signal. A time zoom in the sampling procedure of E1 is sketched in <figref idref="f0002">Fig. 1c</figref>, a time zoom into exposure E2 in <figref idref="f0002">Fig. 1d</figref>, respectively. The sampling duration is assumed to be half of the period and is integrated over many thousands up to millions of periods. During such a first exposure E1, the samples at 0°and 180° are directed by the switches SW1 and SW2 to the storage nodes C1 and C2, respectively, as sketched in <figref idref="f0002">Fig 1c</figref>. A zoom into the second exposure E2 is sketched in <figref idref="f0002">Fig 1d</figref>. The second exposure E2 has the samplings delayed by 90° compared to the first exposure E1. Samples 90° and 270° are again integrated in storage node C1, C2. Integration time of exposure E1 and exposure E2 are the same. At point in time D, when all four samples have been measured and are available, the phase, amplitude and offset information can be calculated, whereas<!-- EPO <DP n="15"> --> the phase information directly corresponds to the distance information of the measured object.</p>
<p id="p0040" num="0040">However, due to mismatches, most implementations actually acquire four instead of only two images, as proposed in patent <patcit id="pcit0013" dnum="US7462808B2"><text>US7'462'808B2</text></patcit> and sketched in <figref idref="f0003">Fig 2</figref>. The first exposure E1 and the second exposure E2 and their readouts RO1 and RO2 are performed as described in <figref idref="f0001">Fig. 1b</figref> but those exposures are followed by two more exposures E3 and E4 and readouts RO3 and RO4. Exposure E3 acquires the samplings from first exposure E1 but delayed by 180°, and the exposure E4 corresponds to a 180° phase delay of exposure E2. At point in time D, all four samples are available to calculate phase (or depth respectively), offset and amplitude information of the received signal. As described in patent <patcit id="pcit0014" dnum="US7462808B2"><text>US7'462'808B2</text></patcit>, this approach enables to compensate for e.g. pixel circuitry and response mismatches or driving non-symmetries.</p>
<p id="p0041" num="0041">A first general example based on an imaging system 100 is depicted in <figref idref="f0004">Fig. 3a</figref>, its rough timing on <figref idref="f0004">Fig. 3b</figref> and a more detailed timing in <figref idref="f0005">Fig. 3c</figref>. The imaging system includes a time-of-flight image sensor 110 consisting of time-of-flight pixels as illustrated in <figref idref="f0001">Fig. 1a</figref>, an illumination source 120, an optical system 130 and a controller 140. The emitted light 120a is reflected by the object 10. The back-reflected light 120b is imaged by the optical system 130 onto the time-of-flight image sensor 110. The time-of-flight image sensor 110 and the illumination source 120 are temporally modulated and synchronized by the controller 140 such that one storage node of the time-of-flight pixels on the time-of-flight image<!-- EPO <DP n="16"> --> sensor 110 integrate all photo-generated electrons while the illumination source 120 is turned on, and a second storage node collects all photo-generated electrons when the illumination source 120 is turned off. This on/off cycle might be repeated many times. The present invention proposes to apply such low modulation frequencies to the high-speed time-of-flight image sensor and pixels that the actual impact of the time of flight of the emitted and back-reflected light is negligible and all emitted light is preferably captured into a single storage node of each pixel on the time-of-flight image sensor. Further, as depicted in <figref idref="f0004">Fig. 3b</figref>, it is proposed to only capture a reduced number of samples and acquisitions, which does not enable to derive actual time-of-flight measurements. A first exposure E is followed by the readout RO. During the exposure E, photo-generated charges are either transferred to storage node C1 or storage node C2, synchronized with the illumination source 120. In the given example, one single acquisition rather than the at least two or four acquisitions as required to gather all necessary (at least three) samples to derive TOF information is proposed. At the point in time D, the number of samples does not allow to deduce time-of-flight information of the captured signal. On that single exposure example as variant of the invention, differential imaging of the two samples is carried out. The modulation can be done in a pseudo-random way as hinted in <figref idref="f0004">Fig. 3a</figref>. By doing so, disturbing interferences between different imaging systems 100 can be minimized. Next to pseudo-random coding, other known techniques such as phase hopping or frequency hopping, chirping or other division multiple access approaches can be implemented to minimize interferences of the systems.</p>
<p id="p0042" num="0042"><figref idref="f0005">Fig. 3c</figref> shows the timing diagram of the single exposure as sketched in <figref idref="f0004">Fig. 3b</figref> in more detail, including a circuitry subtracting the two storage nodes C1 and C2<!-- EPO <DP n="17"> --> from each other during the integration. F describes the total frame, E is the actual exposure, RO the readout time and RS the reset time. The modulation frequency on the present timing of the example is much lower than required to do reasonable time-of-flight imaging. The light L1 is emitted by the illumination source 120 and synchronized with the time-of-flight image sensor 110, such that during the "light on" time all photo-generated charges are transferred to the first storage node C1, while all during the "light-off" -time photo-generated charges are transferred to the second storage node C2. The back-reflected and received light signal over time t is plotted as L2 and has some background light component LBG. S1 illustrates the integration on the first storage node C1 during the "light on time", S2 shows the integration on storage node C2 during the "light-off' time. Sdiff shows the signal difference, when actual in-pixel background removal is implemented. In-pixel circuitry C builds the difference and, by doing so, removes the background light signal. In the illustrated example, the time-of-flight pixel on the time-of-flight image sensor 110 performs a direct subtraction of the two nodes during integration on Sdiff. In other implementations, the time-of-flight pixel performs the subtraction at the end of the integration time. Both implementations increase the dynamic range of the active imaging system. The "light-on" and "light-off' times are repeated many times during the exposure E. Modulation frequencies in the range of hundreds of Hz to below MHz enable to reduce the impact of the arrival time of the light pulses. The total integration time into the first storage node C1 and into the second storage node C2 of the time-of-flight pixel are preferably equivalent during the exposure, in order to integrate the same amount of background light to the two storage nodes on the time-of-flight image sensor's 110 pixels and to subtract the same background or common mode level on those samples. If the two exposure times are kept the same, the light pulses<!-- EPO <DP n="18"> --> might even be shorter than the sample duration into its storage node. This further ensures that there is no impact on the arrival time of the pulse. In any case, to have best possible background light removal, the total exposure time into the two storage nodes should be the same. After the exposure E, the pixel values are read out RO and transferred to the controller 140. Before starting the next acquisition, the pixels are typically reset RS. In order to reduce mismatches in the photo-response on the two storage nodes, a second exposure with inverted switching compared to the first exposure E, and subtracting the two images may be beneficial depending on the image sensor pixel's photo-response. However, the number of acquired samples would still not suffice to derive time-of-flight information.</p>
<p id="p0043" num="0043"><figref idref="f0006">Fig. 4</figref> shows an example imaging system 100. The imaging system 100 includes a time-of-flight image sensor 110 that consists of time-of-flight pixels as sketched in <figref idref="f0001">Fig.1a</figref>, an optical system 130, a structured illumination source 121 and a controller 140. The controller 140 further synchronizes and temporally modulates the time-of-flight image sensor 110 and the structured illumination source 121. The structured illumination source 121 emits the temporally modulated light 121a, which is reflected by the object 10. The back-reflected light 121b by the scene 10 is projected by an optical system 130 on the time-of-flight image sensor 110, on which the time-of-flight pixels demodulate the incoming signal into two storage nodes. The timing of the imaging system 100 can be similar to the one described in <figref idref="f0004 f0005">Fig. 3</figref>. The structured illumination source and the 3 dimensional mapping techniques can be applied as presented by the <patcit id="pcit0015" dnum="WO2007105205A2"><text>PCT publication WO2007/105205A2</text></patcit>. For the structured illumination source, projection techniques based on speckles<!-- EPO <DP n="19"> --> caused by interference as described in <patcit id="pcit0016" dnum="WO2007105205A2"><text>WO2007/105205A2</text></patcit> might be used, however, other pattern projection techniques might be applied to structured illumination source 121, e.g. with refraction based optics or pattern generating masks. Preferably, a random dot pattern is projected. <patcit id="pcit0017" dnum="EP2519001A2"><text>EP2519001A2</text></patcit> teaches the use of a structured light system based on a sensor with specifically designed sensor pixels with transfer gates to first and second storage nodes and using standard slow photodiodes. The present example in contrary proposes not to apply a specific pixel design, but rather use an existing high-speed time-of-flight image sensor 110 and pixels, apply such low modulation frequency by the controller 140 on the structured illumination source 121 and the time-of-flight image sensor 110, that the impact of the actual travel time of the light (time-of-flight) is negligible, and perform the sampling and storage of the back-reflected light 121b on the time-of-flight image sensor 110. Further, the time-of-flight image sensor 110 is proposed to operate in a mode, that the at least three required samples are not captured, but the reduced number of samples from the time-of-flight image sensor 110 are evaluated. The typical 90° phase shift as it is required for the time-of-flight imaging system is abandoned and only the differential image resulting from the two storage nodes on all pixels on the time-of-flight image sensor 110, is evaluated. All afore referenced high-speed time-of-flight image sensors are capable to demodulate the reflected light temporally into at least two storage nodes at such low frequencies unusable for actual time-of-flight measurement system. Further, all of the afore-referenced high-speed time-of-flight image sensors allow the acquisition of a reduced number of samples than required for time-of-flight image sensing. Therefore all those reference pixel designs can be integrated just as time-of-flight image sensor according to the example. With appropriate in-pixel circuitry, the time-of-flight image sensor 110 can subtract a common signal level<!-- EPO <DP n="20"> --> from the samples, or simply subtract the two storage node samples from each other. On the one hand, this increases the dynamic range of the imaging system and on the other hand, simplifies the correlation algorithms for the depth mapping since the dynamic background light signals are removed in the image. Further, by applying appropriate temporal modulation schemes such as code or frequency division multiple accesses, disturbing interferences of different structured light 3 dimensional imaging systems can be avoided.</p>
<p id="p0044" num="0044">Ideally, the time-of-flight pixels on the time-of-flight image sensor 110 transfer charges during the "light-on" time into a first storage node and charges during the "light-off' time into a second node and perform the subtraction. The temporal modulation and the background cancellation circuit of the time-of-flight pixels add to the structured light system all the advantages described above. The modulation frequency is preferably in the range of a few 100 Hz up to 1 MHz, too low for actual TOF measurements. Such an imaging system 100 as described in <figref idref="f0006">Fig. 4</figref>, including a structured illumination source 121 and a time-of-flight sensor 100, both operated at modulation frequencies far below modulation frequencies required for time-of-flight imaging, will drastically increase the dynamic range of existing 3 dimensional imaging system as described in the <patcit id="pcit0018" dnum="WO2007105205A2"><text>PCT publications WO2007/105205A2</text></patcit> and/or <patcit id="pcit0019" dnum="WO2014014341A1"><text>W2014/014341 A1</text></patcit>.</p>
<p id="p0045" num="0045"><figref idref="f0007">Fig. 5</figref> represents an imaging system 100 according to the invention, used as fatigue sensor. The present embodiment enables to build a highly robust eye tracking system, which can be used e.g. for driver's drowsiness measurement or fatigue estimation. The imaging system 100 according to the invention includes two different illumination sources, each having a wavelength in the near infrared<!-- EPO <DP n="21"> --> range, whereas a first illumination source of a first wavelength 122, is at e.g. around 850 nm, and a second illumination source of a second wavelength 123, is at e.g. around 940 nm. It further includes a time-of-flight image sensor 110 with time-of-flight pixels according to <figref idref="f0001">Fig. 1a</figref>, an optical system 130 and a controller 140. Both illumination sources 122 and 123 are synchronized with the time-of-flight image sensor 110 by the controller 140. During the exposure, when the first illumination source of a first wavelength 122 is turned on, the second illumination source of a second wavelength 123 is turned off and vice versa. The first illumination source of a first wavelength 122 emits the light 122a, the second illumination source of a second wavelength 123 emit the light 123a. The back-reflected light 122b of the first illumination source of a first wavelength 122 will be imaged by the optical system 130 on the time-of-flight pixels of the time-of-flight image sensor 110 and transferred to the first storage nodes on the time-of-flight pixels, while the back-reflected light 123b from the illumination source of a second wavelength 123 will be captured by the same time-of-flight image sensor 110 and transferred into the time-of-flight pixels' second storage nodes. After the exposure and by doing a differential readout or on-pixel signal subtraction, the difference image of the two illumination sources can directly be measured. Since the retina of a human eye still shows direct reflection at around 850 nm and strongly reduced reflection at around 940 nm, the pupils will clearly be seen in the difference image and can be tracked easily and robustly. The pupil can easily be identified in the differential image and the opening and closing of the eyes be detected. Based on the blinking, the PERCLOS value (proportion/percentage of time in a minute that the eye is 80% closed) or a drowsiness factor of e.g. a bus driver can<!-- EPO <DP n="22"> --> be measured and corresponding actions initiated. State-of-the-art methods to determine the drivers drowsiness are mainly based on standard imaging and heavy image processing as described by <patcit id="pcit0020" dnum="US7253739B2"><text>Hammoud et al. in patent US 7'253'739B2</text></patcit>.</p>
<p id="p0046" num="0046">The example of the imaging system 105 sketched in <figref idref="f0008">Fig. 6</figref> illustrates a 3 dimensional imaging system. The imaging system 105 includes a first structured illumination source 121 and a second structured illumination source 125 of a different type of structure. Both illumination sources are at near infrared wavelength. Further, the imaging system includes an image sensor 115, an optical system 130 and a controller 140.The system 105 is setup using a first structured illumination source 121 and a second structured illumination source 125, and both illumination sources are placed next to the image sensor 115. A controller 140 synchronizes the structured illumination sources 121, 125 and the image sensor 115. The two illumination sources 121 and 125 are preferably placed such as to minimize occlusion. The emitted light 121a from the first structured illumination source 121 is reflected and the back-reflected light 121b is projected by the optical system 130 on the image sensor 115. The emitted light 125a from the second structured illumination source 125 is reflected, too, and the back-reflected light 125b is projected by the same optical system 130 on the same image sensor 115, too. During the acquisition, the illumination sources 121 and 125 may be temporally modulated and in that case, are preferably inverted during exposure such that all light from the first structured illumination source 121 can be collected in a first storage node of the pixel on the image sensor 115, and back-reflected light 125b from second structured illumination source 125 is collected in a second storage node of the pixels on the image sensor 115. Temporal modulation such as square wave<!-- EPO <DP n="23"> --> or pseudo noise or others can be imagined. In case the pixels on the image sensor 115 include background subtraction circuitry, the signal from the first structured illumination source 121 will be positive; the signal from the second structured illumination source 125 will be negative. The drawback of this approach and having the two illumination sources physically separated but temporally interleaved is that pixels including signal from the first structured illumination source 121 and the second structured illumination source 125 may cancel each other. Therefore, one might think to have the first structured illumination source 121 e.g. with a random speckle pattern and the second structured illumination source 125 with a stripe shape pattern. Further, it is imaginable that both structured illumination sources are integrated as structured patterns on the same emitting die e.g. a VCSEL array. The VCSEL array may consists of a first group of emitting laser diodes corresponding to the first structured illumination source 121 that can be controlled differently than a second group of emitting laser diodes corresponding to the second structured illumination source 125, both groups are on the same VCSEL array die. An array of VCSEL on the same die, used as projector for an imaging system based on a structured illumination source, with the full array controlled by a single driving signal is presented in publication <patcit id="pcit0021" dnum="US2013038881A1"><text>US2013038881A1</text></patcit>. One can imagine to have the first group of laser diode emitting spots randomly placed on the die, whereas the second group of laser diode emitting spots from the same die and have the same pattern but are all slight shifted. By projecting this VCSEL array pattern the spots projected from the first group of laser diodes or first structured illumination source 121 and the spots projected from the second group of laser diodes or second structured illumination source 125 would not interfere with each other in space, since their spots are physically separated on the<!-- EPO <DP n="24"> --> VCSEL array die, and their emitted light is projected in space by the same projection optics. Further, the two groups of lasers diodes may be controlled in such a way that all back-reflected light 121b of the first structured illumination source 121 is stored on a first storage node of the pixels on the image sensor 115 and all back-reflected light 125b from the second structured illumination source 125 is stored on the second storage node of the pixels on the image senor 115. Further, the pixels on the image sensor 115 may include a background cancelling circuitry to enhance the dynamic range. Further, the image sensor 115 may be a time-of-flight image sensor 110.</p>
<p id="p0047" num="0047"><figref idref="f0009">Fig. 7a to d</figref> illustrates an embodiment according to the invention of an imaging system 105 including a structured illumination source 121, a uniform illumination source 126, an optical system 130, an image sensor 115 and a controller 140. Both illumination sources have a wavelength in the near infrared range. In a first exposure, illustrated in <figref idref="f0009">Fig. 7a</figref>, the structured illumination source 121 is turned on, while the uniform illumination source is turned off. As hinted by the drawing in <figref idref="f0009">Fig. 7a</figref>, the structured illumination source 121 may be modulated and synchronized by the controller 140 with the image sensor 115. The emitted light 121a from the structured illumination source 121 reaches an object 10 and is reflected. The back-reflected light 121b form the structured illumination source 121 is imaged by the optical system 130 onto the image sensor 115. The image from the structured illumination source 121 captured by the images sensor 115 can be used in a traditional way to derive depth information from the scene. However, the acquired greyscale image delivered by the image sensor 115 based on the structured illumination source 121 is not representative and objects and details are only vaguely recognizable in many cases. Such a greyscale image based on<!-- EPO <DP n="25"> --> a structured illumination source 121 with a random dot pattern is illustrated in <figref idref="f0009">Fig. 7c</figref>. The loss of details is clearly visible. To still be able to have a conventional image, state-of-the-art imaging systems based on structured illumination sources add next to the first image sensor 115 a second image sensing device with its own optical path to the image sensor. The second image sensor typically is an RGB sensor and delivers a completely independent colour image. However, a second image acquired through a different optical path is first of all costly and second results in many challenges in mapping the two images. An embodiment of the invented method proposes to acquire a second image with the imaging system 105, having a uniform illumination source 126 turned on and using the same optical system 130 and image sensor 115. The emitted light 126a from a uniform illumination source 126 reaches an object 10 in the scene and is reflected. The back-reflected light 126b from the uniform illumination source 126 is imaged by the optical system 130 on the image sensor 115. The operation of the second image acquisition is depicted in <figref idref="f0009">Fig. 7b</figref>. As hinted in the drawing in <figref idref="f0009">Fig. 7b</figref>, the uniform illumination source 126 and the image sensor 115 may be synchronized and modulated by the controller 140. A resulting greyscale image of this second image based on the uniform illumination source 126 is presented in <figref idref="f0009">Fig. 7d</figref>. Details are now much better visible, mapping the 3 dimensional image from the structured illumination source 121 and the greyscale image from the uniform illumination source 126 is simple.</p>
<p id="p0048" num="0048">In order to implement an appropriate optical bandpass filter in the optical system 130, it is advantageous to have similar wavelengths for the structured illumination source 121 and the uniform illumination source 126. Further, the structured illumination<!-- EPO <DP n="26"> --> source 121 and the uniform illumination source 126 may be synchronized and modulated with the image sensor 115 by the controller 140. The image sensor 115 may further have in-pixel background cancelling circuitry in order to increase the dynamic range.</p>
<p id="p0049" num="0049">Further, the image sensor 115 may be a time-of-flight image sensor 110.</p>
<p id="p0050" num="0050"><figref idref="f0010">Fig. 8</figref> shows an actual embodiment according to the invention. The imaging system 105 includes an optical system 130, a structured illumination source 121, and a uniform illumination source 126, each illumination source having a wavelength in the near infrared range. The image sensor 115 behind the optical system and the controller 140 are hidden behind the housing of the imaging system 105 and therefore cannot be seen in the drawing. In the given example in <figref idref="f0010">Fig. 8</figref>, the uniform illumination source 126 consists of two equal illumination sources, placed next to the structured illumination source 126, which is centred. The image captured by the image sensor 115 based on the uniform illumination source 126, can be used as representative greyscale illustration of the scene, while the image captured by the image sensor 115 based on the structured illumination source 121 can be used to derive depth information based on the triangulation principle. The wavelength of the structured illumination source 121 and the uniform illumination source 126 are preferably similar to enable the implementation of a narrow bandpass filter in the optical system 130 to cut off as much of the background light as possible while passing as much of the light from the structured illumination source 121 and the uniform illumination source 126 as possible onto the image sensor 115. Further, the image sensor 115 and the structured illumination source<!-- EPO <DP n="27"> --> 121 may be synchronized and modulated as explained in <figref idref="f0009">Fig. 7</figref>. Further, the uniform illumination source 126 and the image sensor 115 may be synchronised and modulated as explained in <figref idref="f0009">Fig. 7</figref>. Further, the image sensor 115 might be a time-of-flight image sensor 110 and preferably, includes an in-pixel background cancellation circuitry. Based on the image based on the uniform illumination source 126, appropriate image processing algorithms may improve the depth map reconstructed by the evaluation of the image based on the structured illumination source 121.</p>
<p id="p0051" num="0051">In all of the afore-mentioned implementations with modulated illumination source, the light source pulses have preferably the same or shorter duration than the samplings to one of the storage nodes. Further, the sampling duration to the storage nodes is preferably equal to the two nodes, in order to have equal background light in both storage nodes that is cancelled.</p>
<p id="p0052" num="0052">The structured illumination source is understood as a spatially modulated light source, emitting a fixed pattern. Possible patterns are speckle patterns (random, pseudo-random or regular), stripe patterns, random binary patterns, etc.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="28"> -->
<claim id="c-en-01-0001" num="0001">
<claim-text>An imaging system (105) comprising
<claim-text>a time of flight imaging sensor (115),</claim-text>
<claim-text>a first and a second different illumination sources (122, 123) each having a respective wavelength in the near infrared range; and</claim-text>
<claim-text>a controller (140) operable to apply a modulation frequency to the time of flight imaging sensor (115) and to the first and second illumination sources (122, 123) such that the first and second illumination sources (122, 123) are synchronized with the time of flight imaging sensor, and such that during an exposure, when the first illumination source is on, the second illumination source is off, and when the second illumination source is on, the first illumination source is off, the imaging system (105) being adapted to process an image of a scene being illuminated by the first and second illumination sources (122, 123), wherein the first and second illumination sources (122, 123) include at least one structured illumination source (121), wherein the first and second illumination sources include at least one uniform illumination source (126).</claim-text></claim-text></claim>
<claim id="c-en-01-0002" num="0002">
<claim-text>The imaging system (105) according to claim 1, wherein the wavelengths of the first and second illumination sources (122, 123) differ from one another.</claim-text></claim>
<claim id="c-en-01-0003" num="0003">
<claim-text>The imaging system (105) according to one of claims 1 to 2, wherein at least one of the illumination sources is an illumination source which is temporally modulated.</claim-text></claim>
<claim id="c-en-01-0004" num="0004">
<claim-text>The imaging system (105) according to one of claims 1 to 3, wherein the first and the second illumination sources are on the same light emitting die.</claim-text></claim>
<claim id="c-en-01-0005" num="0005">
<claim-text>The imaging system (105) according to claim 4, wherein the imaging sensor (115) and the first and the second illumination sources (122, 123) are temporally modulated and synchronized.</claim-text></claim>
<claim id="c-en-01-0006" num="0006">
<claim-text>The imaging system (105) according to claim 1, comprising an image sensor (115) which is capable of deriving depth information and generating a representative intensity image by said image sensor.</claim-text></claim>
<claim id="c-en-01-0007" num="0007">
<claim-text>An imaging method using a time of flight imaging sensor (115), comprising<!-- EPO <DP n="29"> --> processing an image of a scene being illuminated by the first and second illumination sources each having a wavelength in the near infrared range, wherein the first and the second illumination sources include at least one structured illumination source (121), wherein the fist and the second illumination sources include at least one uniform illumination source (126); applying a modulation frequency to the time of flight imaging sensor (115) to the first and second illumination source such that the first and second illumination sources (122, 123) are synchronized with the time of flight imaging sensor, and such that during an exposure, when the first illumination source is on, the second illumination source is off, and when the second illumination source is on, the first illumination source is off.</claim-text></claim>
<claim id="c-en-01-0008" num="0008">
<claim-text>The imaging method according to claim 7, wherein the wavelengths of the first and the second illumination sources (122; 123) are different.</claim-text></claim>
<claim id="c-en-01-0009" num="0009">
<claim-text>The imaging method according to claim 7 or 8, wherein the fist and the second illumination sources include at least one spatially modulated illumination source (121).</claim-text></claim>
<claim id="c-en-01-0010" num="0010">
<claim-text>The imaging method according to one of claims 7 to 9, wherein the at least two different illumination sources include at least one illumination source which is temporally modulated during exposure.</claim-text></claim>
<claim id="c-en-01-0011" num="0011">
<claim-text>The imaging method according to one of claims 7 to 10, wherein the at least two different illumination sources are on the same light emitting die.</claim-text></claim>
<claim id="c-en-01-0012" num="0012">
<claim-text>The imaging method according to one of claims 7 to 11, wherein the imaging sensor (115) used is a time of flight sensor.</claim-text></claim>
<claim id="c-en-01-0013" num="0013">
<claim-text>The imaging method according to claim 7, comprising interleaving an acquisition of an image based on the structured illumination source with acquisitions based on the uniform illumination source.</claim-text></claim>
<claim id="c-en-01-0014" num="0014">
<claim-text>The imaging method according to claim 7, comprising deriving depth information and generating a representative intensity image by said image sensor.</claim-text></claim>
</claims>
<claims id="claims02" lang="de"><!-- EPO <DP n="30"> -->
<claim id="c-de-01-0001" num="0001">
<claim-text>Bildgebungssystem (105), umfassend:
<claim-text>einen Flugzeitbildgebungssensor (115),</claim-text>
<claim-text>eine erste und eine zweite unterschiedliche Beleuchtungsquelle (122, 123), welche jeweils eine entsprechende Wellenlänge in dem Nahinfrarotfeld aufweisen; und</claim-text>
<claim-text>eine Steuerung (140), welche betreibbar ist, um eine Modulationsfrequenz auf den Flugzeitbildgebungssensor (115) und auf die erste und die zweite Beleuchtungsquelle (122, 123) anzuwenden, derart, dass die erste und die zweite Beleuchtungsquelle (122, 123) mit dem Flugzeitbildgebungssensor synchronisiert sind, und dass während einer Belichtung, wenn die erste Beleuchtungsquelle eingeschaltet ist, die zweite Beleuchtungsquelle ausgeschaltet ist, und wenn die zweite Beleuchtungsquelle eingeschaltet ist, die erste Beleuchtungsquelle ausgeschaltet ist, wobei das Bildgebungssystem (105) angepasst ist, um ein Bild einer Szene zu verarbeiten, welche durch die erste und die zweite Beleuchtungsquelle (122, 123) beleuchtet wird, wobei die erste und die zweite Beleuchtungsquelle (122, 123) mindestens eine strukturierte Beleuchtungsquelle (121) einschließen, wobei die erste und die zweite Beleuchtungsquelle mindestens eine uniforme Beleuchtungsquelle (126) einschließen.</claim-text></claim-text></claim>
<claim id="c-de-01-0002" num="0002">
<claim-text>Bildgebungssystem (105) nach Anspruch 1, wobei die Wellenlängen der ersten und der zweiten Beleuchtungsquelle (122, 123) voneinander unterschiedlich sind.</claim-text></claim>
<claim id="c-de-01-0003" num="0003">
<claim-text>Bildgebungssystem (105) nach einem der Ansprüche 1 bis 2, wobei mindestens eine der Beleuchtungsquellen eine Beleuchtungsquelle ist, welche zeitlich moduliert wird.</claim-text></claim>
<claim id="c-de-01-0004" num="0004">
<claim-text>Bildgebungssystem (105) nach einem der Ansprüche 1 bis 3, wobei die erste und die zweite Beleuchtungsquelle auf demselben lichtemittierenden Chip liegen.</claim-text></claim>
<claim id="c-de-01-0005" num="0005">
<claim-text>Bildgebungssystem (105) nach Anspruch 4, wobei der Bildgebungssensor (115) und die erste und die zweite Beleuchtungsquelle (122, 123) zeitlich moduliert und synchronisiert sind.</claim-text></claim>
<claim id="c-de-01-0006" num="0006">
<claim-text>Bildgebungssystem (105) nach Anspruch 1, umfassend einen Bildgebungssensor (115), welcher in der Lage ist, Tiefeninformationen abzuleiten und ein repräsentatives Intensitätsbild durch den Bildgebungssensor zu erzeugen.</claim-text></claim>
<claim id="c-de-01-0007" num="0007">
<claim-text>Bildgebungsverfahren, welches einen Flugzeitbildgebungssensor (115) verwendet, umfassend:
<claim-text>Verarbeiten eines Bilds einer Szene, welche durch die erste und die zweite Beleuchtungsquelle beleuchtet wird, welche jeweils eine Wellenlänge in dem Nahinfrarotfeld aufweisen, wobei die erste und die zweite Beleuchtungsquelle mindestens eine strukturierte Beleuchtungsquelle (121) einschließen, wobei die erste und die zweite Beleuchtungsquelle mindestens eine uniforme Beleuchtungsquelle (126) einschließen;</claim-text>
<claim-text>Anwenden einer Modulationsfrequenz auf den Flugzeitbildgebungssensor (115) und auf die erste und die zweite Beleuchtungsquelle, derart, dass die erste und die zweite<!-- EPO <DP n="31"> --> Beleuchtungsquelle (122, 123) mit dem Flugzeitbildgebungssensor synchronisiert sind, und dass, während einer Belichtung, wenn die erste Beleuchtungsquelle eingeschaltet ist, die zweite Beleuchtungsquelle ausgeschaltet ist, und wenn die zweite Beleuchtungsquelle eingeschaltet ist, die erste Beleuchtungsquelle ausgeschaltet ist</claim-text></claim-text></claim>
<claim id="c-de-01-0008" num="0008">
<claim-text>Bildgebungsverfahren nach Anspruch 7, wobei die Wellenlängen der ersten und der zweiten Beleuchtungsquelle (122, 123) unterschiedlich sind.</claim-text></claim>
<claim id="c-de-01-0009" num="0009">
<claim-text>Bildgebungsverfahren nach Anspruch 7 oder 8, wobei die erste und die zweite Beleuchtungsquelle mindestens eine räumlich modulierte Beleuchtungsquelle (121) einschließen.</claim-text></claim>
<claim id="c-de-01-0010" num="0010">
<claim-text>Bildgebungsverfahren nach einem der Ansprüche 7 bis 9, wobei die mindestens zwei unterschiedlichen Beleuchtungsquellen mindestens eine Beleuchtungsquelle einschließen, welche während der Belichtung zeitlich moduliert ist.</claim-text></claim>
<claim id="c-de-01-0011" num="0011">
<claim-text>Bildgebungsverfahren nach einem der Ansprüche 7 bis 10, wobei die mindestens zwei unterschiedlichen Beleuchtungsquellen auf demselben lichtemittierenden Chip liegen.</claim-text></claim>
<claim id="c-de-01-0012" num="0012">
<claim-text>Bildgebungsverfahren nach einem der Ansprüche 7 bis 11, wobei der verwendete Bildgebungssensor (115) ein Flugzeitsensor ist.</claim-text></claim>
<claim id="c-de-01-0013" num="0013">
<claim-text>Bildgebungsverfahren nach Anspruch 7, umfassend das Verschachteln einer Bilderfassung basierend auf der strukturierten Beleuchtungsquelle mit Erfassungen, die auf der uniformen Beleuchtungsquelle basiert sind.</claim-text></claim>
<claim id="c-de-01-0014" num="0014">
<claim-text>Bildgebungsverfahren nach Anspruch 7, umfassend das Ableiten von Tiefeninformationen und das Erzeugen eines repräsentativen Intensitätsbildes durch den Bildgebungssensor.</claim-text></claim>
</claims>
<claims id="claims03" lang="fr"><!-- EPO <DP n="32"> -->
<claim id="c-fr-01-0001" num="0001">
<claim-text>Système de formation d'image (105) comprenant :
<claim-text>un capteur de formation d'image de temps de vol (115) ;</claim-text>
<claim-text>des première et seconde sources d'éclairage différentes (122, 123) dont chacune présente une longueur d'onde respective dans la plage des infrarouges proches ; et</claim-text>
<claim-text>un dispositif de contrôle (140) qui peut être rendu opérationnel pour appliquer une fréquence de modulation sur le capteur de formation d'image de temps de vol (115) et sur les première et seconde sources d'éclairage (122, 123) de telle sorte que les première et seconde sources d'éclairage (122, 123) soient synchronisées avec le capteur de formation d'image de temps de vol et que, pendant une exposition, lorsque la première source d'éclairage est activée, la seconde source d'éclairage soit désactivée et lorsque la seconde source d'éclairage est activée, la première source d'éclairage soit désactivée, le système de formation d'image (105) étant adapté pour traiter une image d'une scène qui est éclairée par les première et seconde sources d'éclairage (122, 123), dans lequel les première et seconde sources d'éclairage (122, 123) incluent au moins une source d'éclairage structurée (121), dans lequel les première et seconde sources d'éclairage incluent au moins une source d'éclairage uniforme (126).</claim-text></claim-text></claim>
<claim id="c-fr-01-0002" num="0002">
<claim-text>Système de formation d'image (105) selon la revendication 1, dans lequel les longueurs d'onde des première et seconde sources d'éclairage (122, 123) diffèrent l'une de l'autre.</claim-text></claim>
<claim id="c-fr-01-0003" num="0003">
<claim-text>Système de formation d'image (105) selon l'une quelconque des revendications 1 et 2, dans lequel au moins une des sources d'éclairage est une source d'éclairage qui est modulée temporellement.</claim-text></claim>
<claim id="c-fr-01-0004" num="0004">
<claim-text>Système de formation d'image (105) selon l'une quelconque des revendications 1 à 3, dans lequel les première et seconde sources d'éclairage sont sur la même puce électroluminescente.</claim-text></claim>
<claim id="c-fr-01-0005" num="0005">
<claim-text>Système de formation d'image (105) selon la revendication 4, dans lequel le capteur de formation d'image (115) et les première et seconde sources d'éclairage (122, 123) sont modulés temporellement et synchronisés.</claim-text></claim>
<claim id="c-fr-01-0006" num="0006">
<claim-text>Système de formation d'image (105) selon la revendication 1, comprenant un capteur d'image (115) qui est capable de dériver des informations de profondeur et de générer une image d'intensité représentative au moyen dudit capteur d'image.</claim-text></claim>
<claim id="c-fr-01-0007" num="0007">
<claim-text>Procédé de formation d'image utilisant un capteur de formation d'image de temps de vol (115), comprenant :
<claim-text>le traitement d'une image d'une scène qui est éclairée par les première et seconde sources d'éclairage dont chacune présente une longueur d'onde dans la plage des infrarouges proches, dans lequel les première et seconde sources d'éclairage incluent au<!-- EPO <DP n="33"> --> moins une source d'éclairage structurée (121), dans lequel les première et seconde sources d'éclairage incluent au moins une source d'éclairage uniforme (126) ;</claim-text>
<claim-text>l'application d'une fréquence de modulation sur le capteur de formation d'image de temps de vol (115) et sur les première et seconde sources d'éclairage de telle sorte que les première et seconde sources d'éclairage (122, 123) soient synchronisées avec le capteur de formation d'image de temps de vol et que, pendant une exposition, lorsque la première source d'éclairage est activée, la seconde source d'éclairage soit désactivée et lorsque la seconde source d'éclairage est activée, la première source d'éclairage soit désactivée.</claim-text></claim-text></claim>
<claim id="c-fr-01-0008" num="0008">
<claim-text>Procédé de formation d'image selon la revendication 7, dans lequel les longueurs d'onde des première et seconde sources d'éclairage (122, 123) sont différentes.</claim-text></claim>
<claim id="c-fr-01-0009" num="0009">
<claim-text>Procédé de formation d'image selon la revendication 7 ou 8, dans lequel les première et seconde sources d'éclairage incluent au moins une source d'éclairage modulée spatialement (121).</claim-text></claim>
<claim id="c-fr-01-0010" num="0010">
<claim-text>Procédé de formation d'image selon l'une quelconque des revendications 7 à 9, dans lequel les au moins deux sources d'éclairage différentes incluent au moins une source d'éclairage qui est modulée temporellement pendant l'exposition.</claim-text></claim>
<claim id="c-fr-01-0011" num="0011">
<claim-text>Procédé de formation d'image selon l'une quelconque des revendications 7 à 10, dans lequel les au moins deux sources d'éclairage différentes sont sur la même puce électroluminescente.</claim-text></claim>
<claim id="c-fr-01-0012" num="0012">
<claim-text>Procédé de formation d'image selon l'une quelconque des revendications 7 à 11, dans lequel le capteur de formation d'image (115) qui est utilisé est un capteur de temps de vol.</claim-text></claim>
<claim id="c-fr-01-0013" num="0013">
<claim-text>Procédé de formation d'image selon la revendication 7, comprenant le fait d'entrelacer une acquisition d'une image sur la base de la source d'éclairage structurée avec des acquisitions sur la base de la source d'éclairage uniforme.</claim-text></claim>
<claim id="c-fr-01-0014" num="0014">
<claim-text>Procédé de formation d'image selon la revendication 7, comprenant la dérivation d'informations de profondeur et la génération d'une image d'intensité représentative au moyen dudit capteur d'image.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="34"> -->
<figure id="f0001" num="1a,1b"><img id="if0001" file="imgf0001.tif" wi="104" he="224" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> -->
<figure id="f0002" num="1c,1d"><img id="if0002" file="imgf0002.tif" wi="133" he="217" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> -->
<figure id="f0003" num="2"><img id="if0003" file="imgf0003.tif" wi="165" he="97" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> -->
<figure id="f0004" num="3a,3b"><img id="if0004" file="imgf0004.tif" wi="132" he="228" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> -->
<figure id="f0005" num="3c"><img id="if0005" file="imgf0005.tif" wi="127" he="202" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> -->
<figure id="f0006" num="4"><img id="if0006" file="imgf0006.tif" wi="159" he="152" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> -->
<figure id="f0007" num="5"><img id="if0007" file="imgf0007.tif" wi="137" he="166" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="41"> -->
<figure id="f0008" num="6"><img id="if0008" file="imgf0008.tif" wi="159" he="159" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="42"> -->
<figure id="f0009" num="7a,7b,7c,7d"><img id="if0009" file="imgf0009.tif" wi="165" he="200" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="43"> -->
<figure id="f0010" num="8"><img id="if0010" file="imgf0010.tif" wi="137" he="136" img-content="drawing" img-format="tif"/></figure>
</drawings>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="US2013002823A1"><document-id><country>US</country><doc-number>2013002823</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0001">[0009]</crossref></li>
<li><patcit id="ref-pcit0002" dnum="US2005190206A1"><document-id><country>US</country><doc-number>2005190206</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0002">[0010]</crossref></li>
<li><patcit id="ref-pcit0003" dnum="US2005162638A1"><document-id><country>US</country><doc-number>2005162638</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0003">[0011]</crossref></li>
<li><patcit id="ref-pcit0004" dnum="EP2017651A2"><document-id><country>EP</country><doc-number>2017651</doc-number><kind>A2</kind></document-id></patcit><crossref idref="pcit0004">[0012]</crossref></li>
<li><patcit id="ref-pcit0005" dnum="US2004263510A1"><document-id><country>US</country><doc-number>2004263510</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0005">[0013]</crossref></li>
<li><patcit id="ref-pcit0006" dnum="US5856667A"><document-id><country>US</country><doc-number>5856667</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0006">[0039]</crossref></li>
<li><patcit id="ref-pcit0007" dnum="EP1009984B1"><document-id><country>EP</country><doc-number>1009984</doc-number><kind>B1</kind></document-id></patcit><crossref idref="pcit0007">[0039]</crossref></li>
<li><patcit id="ref-pcit0008" dnum="EP1513202B1"><document-id><country>EP</country><doc-number>1513202</doc-number><kind>B1</kind></document-id></patcit><crossref idref="pcit0008">[0039]</crossref></li>
<li><patcit id="ref-pcit0009" dnum="US7884310B2"><document-id><country>US</country><doc-number>7884310</doc-number><kind>B2</kind></document-id></patcit><crossref idref="pcit0009">[0039]</crossref></li>
<li><patcit id="ref-pcit0010" dnum="WO2009135952A2"><document-id><country>WO</country><doc-number>2009135952</doc-number><kind>A2</kind></document-id></patcit><crossref idref="pcit0010">[0039]</crossref></li>
<li><patcit id="ref-pcit0011" dnum="US7574190B2"><document-id><country>US</country><doc-number>7574190</doc-number><kind>B2</kind></document-id></patcit><crossref idref="pcit0011">[0039]</crossref></li>
<li><patcit id="ref-pcit0012" dnum="US7897928B2"><document-id><country>US</country><doc-number>7897928</doc-number><kind>B2</kind></document-id></patcit><crossref idref="pcit0012">[0039]</crossref></li>
<li><patcit id="ref-pcit0013" dnum="US7462808B2"><document-id><country>US</country><doc-number>7462808</doc-number><kind>B2</kind></document-id></patcit><crossref idref="pcit0013">[0040]</crossref><crossref idref="pcit0014">[0040]</crossref></li>
<li><patcit id="ref-pcit0014" dnum="WO2007105205A2"><document-id><country>WO</country><doc-number>2007105205</doc-number><kind>A2</kind></document-id></patcit><crossref idref="pcit0015">[0043]</crossref><crossref idref="pcit0016">[0043]</crossref><crossref idref="pcit0018">[0044]</crossref></li>
<li><patcit id="ref-pcit0015" dnum="EP2519001A2"><document-id><country>EP</country><doc-number>2519001</doc-number><kind>A2</kind></document-id></patcit><crossref idref="pcit0017">[0043]</crossref></li>
<li><patcit id="ref-pcit0016" dnum="WO2014014341A1"><document-id><country>WO</country><doc-number>2014014341</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0019">[0044]</crossref></li>
<li><patcit id="ref-pcit0017" dnum="US7253739B2"><document-id><country>US</country><doc-number>7253739</doc-number><kind>B2</kind><name>Hammoud </name></document-id></patcit><crossref idref="pcit0020">[0045]</crossref></li>
<li><patcit id="ref-pcit0018" dnum="US2013038881A1"><document-id><country>US</country><doc-number>2013038881</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0021">[0046]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
