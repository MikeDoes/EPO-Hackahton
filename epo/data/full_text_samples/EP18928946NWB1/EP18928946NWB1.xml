<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP18928946B1" file="EP18928946NWB1.xml" lang="en" country="EP" doc-number="3677995" kind="B1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSK..HRIS..MTNORS..SM..................</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  2100000/0</B007EP></eptags></B000><B100><B110>3677995</B110><B120><B121>EUROPEAN PATENT SPECIFICATION</B121></B120><B130>B1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>18928946.5</B210><B220><date>20180801</date></B220><B240><B241><date>20200330</date></B241></B240><B250>ja</B250><B251EP>en</B251EP><B260>en</B260></B200><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20200708</date><bnum>202028</bnum></B430><B450><date>20211006</date><bnum>202140</bnum></B450><B452EP><date>20210727</date></B452EP></B400><B500><B510EP><classification-ipcr sequence="1"><text>G06F   3/01        20060101AFI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>G06F   3/0481      20130101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>A63F  13/211       20140101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>G06T  19/00        20110101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="5"><text>G02B  27/01        20060101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="6"><text>G06F   3/03        20060101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="7"><text>G06F   3/0346      20130101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="8"><text>G06T  15/20        20110101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="9"><text>A63F  13/213       20140101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="10"><text>A63F  13/5258      20140101ALI20210407BHEP        </text></classification-ipcr><classification-ipcr sequence="11"><text>G02B  27/00        20060101ALI20210407BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G02B  27/0093      20130101 LI20201020BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>A63F  13/5258      20140902 LI20200831BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>G06F   3/0346      20130101 FI20200820BHEP        </text></classification-cpc><classification-cpc sequence="4"><text>G06F   3/012       20130101 LI20200820BHEP        </text></classification-cpc><classification-cpc sequence="5"><text>G06F   3/04815     20130101 LI20200820BHEP        </text></classification-cpc><classification-cpc sequence="6"><text>G06F   3/011       20130101 LI20200820BHEP        </text></classification-cpc><classification-cpc sequence="7"><text>G06F   3/0304      20130101 LI20200820BHEP        </text></classification-cpc><classification-cpc sequence="8"><text>G02B  27/017       20130101 LI20200820BHEP        </text></classification-cpc><classification-cpc sequence="9"><text>A63F  13/211       20140902 LI20200820BHEP        </text></classification-cpc><classification-cpc sequence="10"><text>A63F  13/213       20140902 LI20200831BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>ANZEIGEVORRICHTUNG, ANZEIGEVERFAHREN, PROGRAMM UND NICHT-TEMPORÄRES COMPUTERLESBARES INFORMATIONSSPEICHERMEDIUM</B542><B541>en</B541><B542>DISPLAY DEVICE, DISPLAY METHOD, PROGRAM, AND NON-TEMPORARY COMPUTER-READABLE INFORMATION STORAGE MEDIUM</B542><B541>fr</B541><B542>DISPOSITIF D'AFFICHAGE, PROCÉDÉ D'AFFICHAGE, PROGRAMME ET SUPPORT D'INFORMATIONS NON TEMPORAIRE LISIBLE PAR ORDINATEUR</B542></B540><B560><B561><text>JP-A- 2015 223 334</text></B561><B561><text>JP-A- 2016 158 795</text></B561><B561><text>JP-A- 2018 072 604</text></B561><B561><text>US-A1- 2017 083 084</text></B561><B561><text>US-A1- 2017 357 333</text></B561><B561><text>US-B1- 9 268 136</text></B561><B565EP><date>20200901</date></B565EP></B560></B500><B700><B720><B721><snm>IWASE Hiroaki</snm><adr><str>c/o Rakuten, Inc., 1-14-1 Tamagawa, Setagaya-ku</str><city>Tokyo 158-0094</city><ctry>JP</ctry></adr></B721></B720><B730><B731><snm>Rakuten Group, Inc.</snm><iid>101903846</iid><irf>222 388 a/dst</irf><adr><str>1-14-1 Tamagawa,</str><city>Setagaya-ku,
Tokyo 1580094</city><ctry>JP</ctry></adr></B731></B730><B740><B741><snm>Hoffmann Eitle</snm><iid>100061036</iid><adr><str>Patent- und Rechtsanwälte PartmbB 
Arabellastraße 30</str><city>81925 München</city><ctry>DE</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B860><B861><dnum><anum>JP2018028874</anum></dnum><date>20180801</date></B861><B862>ja</B862></B860><B870><B871><dnum><pnum>WO2020026380</pnum></dnum><date>20200206</date><bnum>202006</bnum></B871></B870></B800></SDOBI>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001">Technical Field</heading>
<p id="p0001" num="0001">The present disclosure relates to a display device, a display method and a non-temporary computer-readable information storage medium.</p>
<heading id="h0002">Background Art</heading>
<p id="p0002" num="0002">For a virtual reality system that provides 10 the user with a virtual space through a head-mounted display, methods for manipulating a character (avatar), which is the user's alter ego, are made available, including, for example, detecting the user's eye direction and moving the avatar to the position at which the user is recognized as gazing on the basis of the detected eye direction (see Patent Literature 1, for example).</p>
<heading id="h0003">Citation List</heading>
<heading id="h0004">Patent Literature</heading>
<p id="p0003" num="0003">PTL 1: <patcit id="pcit0001" dnum="JP2018072992A"><text>Japanese Patent Application Kokai Publication No 2018-72992</text></patcit></p>
<p id="p0004" num="0004"><patcit id="pcit0002" dnum="US2017083084A1"><text>US 2017/083084 A1 </text></patcit>relates to an image display device configured to display a free viewpoint image that tracks a movement of observer's head and trunk. The line-of-sight direction and trunk orientation of the user are individually oriented in a free viewpoint space displayed on a display device based on posture information of the user's head and trunk, which are respectively obtained from a head motion tracking device and a trunk motion tracking device. A behaviour that is natural in real space can be represented by moving a line of sight to a front direction of the trunk on a space in a state where the line of sight does not match the front direction of the trunk. An unnatural UI<!-- EPO <DP n="2"> --> that goes straight to the line-of-sight direction is prevented regardless of orientation of the trunk.</p>
<p id="p0005" num="0005"><patcit id="pcit0003" dnum="US9268136B1"><text>US 9 268 136 B1</text></patcit> relates to a system that involves a head-mountable display (HMD) or an associated device determining the orientation of a person's head relative to their body. To do so, example methods and systems may compare sensor data from the HMD to corresponding sensor data from a tracking device that is expected to move in a manner that follows the wearer's body, such a mobile phone that is located in the HMD wearer's pocket.</p>
<p id="p0006" num="0006"><patcit id="pcit0004" dnum="US2017357333A1"><text>US 2017/357333 A1</text></patcit> relates to an apparatus directed to a wireless hand-held inertial controller with passive optical and inertial tracking in a slim form-factor, for use with a head mounted virtual or augmented reality display device (HMD), that operates with six degrees of freedom by fusing data related to the position of the controller derived from a forward-facing optical sensor located in the HMD with data relating to the orientation of the controller derived from an inertial measurement unit located in the controller.</p>
<heading id="h0005">Summary of Invention</heading>
<heading id="h0006">Technical Problem</heading>
<p id="p0007" num="0007">Meanwhile, the user may sometimes want to move the avatar in a direction different from the user's eye direction, such as, for example, when the avatar is walking along a wall while watching paintings on the wall in a virtual space that reproduces an art gallery. A typical method for controlling movement of a character in a virtual space may be using a terminal device such as a controller operated by the user to give an instruction about a particular moving direction for the character. However, suppose that, for example, the user's body is oriented frontward except the user's head facing sideways,<!-- EPO <DP n="3"> --> and that the user intuitively gives via a terminal device an instruction about the moving direction relative to the orientation of the user's torso. In this case, when the orientation of the head-mounted display is only detectable, the instruction about the moving direction is regarded as having been given relative to the avatar's observation direction that is identified on the basis of the orientation of the head-mounted display. As a result, the avatar may be moved in a direction different from the moving direction that the user has intuitively specified.</p>
<p id="p0008" num="0008">The present disclosure is intended to solve the aforementioned problems, and an objective of the present disclosure is to provide a display device, a display method, a program, and a non-temporary computer-readable information storage medium for enabling the observation position in a virtual space to be moved in accordance with an intuitive instruction input given by a user.</p>
<heading id="h0007">Solution to Problem</heading>
<p id="p0009" num="0009">The object of the invention is achieved by the subject-matter of the independent claims. Advantageous embodiments are defined in the dependent claims.<!-- EPO <DP n="4"> --></p>
<heading id="h0008">Advantageous Effects of Invention</heading>
<p id="p0010" num="0010">The present disclosure enables the observation position in a virtual space to be moved in accordance with an intuitive instruction input given by a user.</p>
<heading id="h0009">Brief Description of Drawings</heading>
<p id="p0011" num="0011">
<ul id="ul0001" list-style="none" compact="compact">
<li><figref idref="f0001">FIG. 1</figref> is a schematic block diagram illustrating a hardware configuration of a display device according to an embodiment;</li>
<li><figref idref="f0002">FIG. 2</figref> is a top view of the display device and a user in a real space before the orientations of the display device and the user are changed;<!-- EPO <DP n="5"> --></li>
<li><figref idref="f0003">FIG. 3A</figref> is a top view of the user and the display device after their orientations are changed in the real space by the user rotating the orientation of the head only;</li>
<li><figref idref="f0003">FIG. 3B</figref> is a top view of the user and the display device after their orientations are changed in the real space by the user rotating the orientation of the torso only;</li>
<li><figref idref="f0004">FIG. 4</figref> is a top view of a virtual space before the orientations of the display device and the user are changed in the real space;</li>
<li><figref idref="f0005">FIG. 5A</figref> is a top view of the virtual space after the orientations of the display device and the user are changed in the real space by the user rotating the orientation of the head only;</li>
<li><figref idref="f0005">FIG. 5B</figref> is a top view of the virtual space after the orientations of the display device and the user are changed in the real space by the user rotating the orientation of the torso only;</li>
<li><figref idref="f0006">FIG. 6</figref> is a schematic block diagram illustrating a functional configuration of the display device according to an embodiment;</li>
<li><figref idref="f0007">FIG. 7</figref> is a front view of a controller according to an embodiment;</li>
<li><figref idref="f0008">FIG. 8</figref> is an external view of the display device worn by the user according to an example not covered by the claimed invention;</li>
<li><figref idref="f0009">FIG. 9A</figref> is a top view of the display device and the controller when a relative position is 0;</li>
<li><figref idref="f0009">FIG. 9B</figref> is an image taken by the controller in the state illustrated in <figref idref="f0009">FIG. 9A</figref>;</li>
<li><figref idref="f0010">FIG. 10A</figref> is a top view of the display device and the controller when the relative position is ϕ<sub>1</sub>;</li>
<li><figref idref="f0010">FIG. 10B</figref> is an image taken by the controller in the state illustrated in <figref idref="f0010">FIG. 10A</figref>;</li>
<li><figref idref="f0011">FIG. 11</figref> is a flowchart illustrating a flow of a display process executed by a control unit included in the display device according to an embodiment; and</li>
<li><figref idref="f0012">FIG. 12</figref> is a top view of the display device and the user in the real space before their orientations are changed according to a variation.</li>
</ul><!-- EPO <DP n="6"> --></p>
<heading id="h0010">Description of Embodiments</heading>
<p id="p0012" num="0012">Embodiments of the present disclosure will now be described. The embodiments are presented for explanatory purposes only and do not limit the scope of the present disclosure. Accordingly, persons skilled in the art can adopt embodiments in which any or all of the elements in the following embodiments are replaced with equivalents thereof, and such adopted embodiments are included in the scope of the present disclosure. For explaining embodiments of the present disclosure referring to the drawings, identical reference symbols are given to identical or equivalent parts throughout the drawings.</p>
<p id="p0013" num="0013"><figref idref="f0001">FIG. 1</figref> is a schematic block diagram illustrating a hardware configuration of a display device 100 according to the present embodiment. The display device 100 includes, for example, a head-mounted display that has various types of sensors and a control device. The head-mounted display may be built by mounting a smart phone, a tablet computer, a phablet, or the like onto an attachment. In this case, the display device 100 is implemented by executing a program causing a computer such as a smart phone to function as the aforementioned components on the computer such as a smart phone. As illustrated in <figref idref="f0001">FIG. 1</figref>, the display device 100 includes a control unit 101, a read-only memory (ROM) 102, a RAM 103, a display 104, a sensor 105, an operator 106, and a communicator 107, which are connected by a bus 108.</p>
<p id="p0014" num="0014">The control unit 101, which includes, for example, a central processing unit (CPU), controls the display device 100 as a whole.</p>
<p id="p0015" num="0015">The ROM 102 is a non-volatile memory storing programs and various types of data needed for the control unit 101 to control the display device 100 as a whole.</p>
<p id="p0016" num="0016">The RAM 103 is a volatile memory for temporarily storing information generated by the control unit 101 and data needed for generating the information.</p>
<p id="p0017" num="0017">The display 104, which includes a liquid crystal display (LCD), a backlight, and so on, is under control of the control unit 101 to show, for example, an image output<!-- EPO <DP n="7"> --> by the control unit 101.</p>
<p id="p0018" num="0018">The sensor 105, which may include an attitude sensor and an acceleration sensor, detects the orientation of the display device 100. The sensor 105 outputs a signal indicating the detected orientation of the display device 100 to the control unit 101.</p>
<p id="p0019" num="0019">The operator 106 includes an input device such as a button, a keyboard, and a touch panel. The operator 106 accepts operations input by the user of the display device 100 and outputs signals corresponding to the accepted input operations to the control unit 101.</p>
<p id="p0020" num="0020">The communicator 107 includes a communication interface for communicating with a controller 220, which will be described later. The communicator 107 communicates with the controller 220 under a wireless communication standard such as Bluetooth®.</p>
<p id="p0021" num="0021">The following describes a positional relationship between the display device 100 and the user 202 in a real space 201 and in a virtual space 301 shown on the display device 100 according to the present embodiment. <figref idref="f0002">FIG. 2</figref> is a top view of the real space 201 (at time t<sub>a</sub>) before the orientations and positions of the display device 100 and the user 202 are changed. <figref idref="f0004">FIG. 4</figref> is a top view of the virtual space 301 (at time ta) before the orientations and positions of the display device 100 and the user 202 are changed in the real space 201. <figref idref="f0003">FIGS. 3A and 3B</figref> are top views of the real space 201 (at time tb) after the orientations and positions of the display device 100 and the user 202 are changed. <figref idref="f0003">FIG. 3A</figref> shows the real space 201 after the orientation of the display device 100 is changed by the user 202 rotating the orientation of the head 203 only. <figref idref="f0003">FIG. 3B</figref> shows the real space 201 after the user 202 changes the orientation of the torso 204 only. <figref idref="f0005">FIGS. 5A and 5B</figref> are top views of the virtual space 301 (at time tb) after the orientations and positions of the display device 100 and the user 202 are changed in the real space 201. <figref idref="f0005">FIG. 5A</figref> shows the virtual space 301 after the user 202 changes the orientation of the display device 100 by rotating the orientation of the head 203 only. <figref idref="f0005">FIG. 5B</figref> shows the<!-- EPO <DP n="8"> --> virtual space 301 after the user 202 changes the orientation of the torso 204 only. The description below is given with reference to these figures. Lower-case alphabetic letters are added to these figures as appropriate for expressing an elapse of time from time ta to time tb. These lower-case alphabetic letters are omitted in the following description if appropriate.</p>
<p id="p0022" num="0022">As illustrated in <figref idref="f0002">FIGS. 2</figref>, <figref idref="f0003">3A, and 3B</figref>, the user 202 wears the display device 100 placed in front of the eyes of the user 202 in the real space 201. When the user 202 wears the display device 100, the display direction 205 of the display device 100 is opposite to the observation direction 303 of an avatar 401, which will be described later, in the virtual space 301 as illustrated in <figref idref="f0004">FIGS. 4</figref>, <figref idref="f0005">5A, and 5B</figref>.</p>
<p id="p0023" num="0023">In the real space 201, the user 202 is holding the controller 220. The controller 220 is a terminal device for manipulating the avatar 401, which is placed in the virtual space 301 and displayed on the display device 100. The avatar 401 is an object created by using computer graphics to represent an alter ego of the user 202 in the virtual space 301. As illustrated in <figref idref="f0004">FIGS. 4</figref>, <figref idref="f0005">5A, and 5B</figref>, in the present embodiment, the avatar 401 is human-shaped, and the position of the head 402 of the avatar 401 corresponds to an observation position 302, the eye direction of the head 402 corresponds to an observation direction 303, and the frontward direction of the torso 403 corresponds to a base direction 304. The controller 220 can wirelessly communicate with the display device 100 using Bluetooth®, for example. For instance, the user 202 inputs an instruction input indicating a moving direction of the avatar 401 to the controller 220 by operating a key or a touch panel included in the controller 220. The controller 220 then transmits the accepted instruction input to the display device 100. As described later, the display device 100 updates the observation position 303, the observation direction 303, and the base direction 304 of the avatar 401 in the virtual space 301, on the basis of the instruction input received from the controller 220 and of the orientation of the display device 100.<!-- EPO <DP n="9"> --></p>
<p id="p0024" num="0024">Note that azimuth directions 211 in the real space 201 may or may not match azimuth directions 311 in the virtual space 301. In the present embodiment, the azimuth directions differ between the real space and the virtual space.</p>
<p id="p0025" num="0025"><figref idref="f0006">FIG. 6</figref> is a schematic block diagram illustrating a functional configuration of the display device 100 according to the embodiment of the present disclosure. As shown in <figref idref="f0006">FIG. 6</figref>, the display device 100 includes an accepter 111, an observation position updater 112, a detector 113, an observation direction updater 114, an obtainer 115, a base direction updater 116, a generator 117, a display unit 118, and a state information storage 121.</p>
<p id="p0026" num="0026">The state information storage 121 stores the observation position 302, the observation direction 303, and the base direction 304 of the avatar 401 in the virtual space 301. As described later, the observation position 302, the observation direction 303, and the base direction 304 are updated by the observation position updater 112, the observation direction updater 114, and the base direction updater 116, respectively. The state information storage 121 is implemented by the RAM 103.</p>
<p id="p0027" num="0027">The accepter 111 accepts an instruction input that is given to the controller 220 to indicate a moving direction. For example, the accepter 111 receives, from the controller 220 via the communicator 107, a signal indicating an instruction input accepted by the controller 220. In the present embodiment, the control unit 101 and the communicator 107 collaborate with each other to function as the accepter 111.</p>
<p id="p0028" num="0028">When the accepter 111 accepts an instruction input, the observation position updater 112 updates the observation position 302 so that the observation position moves in the direction indicated by the input instruction relative to the base direction 304. As an example, suppose that the accepter 111 has received from the controller 220 an instruction input to move forward, that is, to move in the frontward direction 206a of the torso 204a from time ta to time tb, as illustrated in <figref idref="f0002">FIG. 2</figref>. Then, the observation position updater 112 updates, as illustrated in <figref idref="f0005">FIGS. 5A and 5B</figref>, the current observation position<!-- EPO <DP n="10"> --> 302a stored in the state information storage 121 to the observation position 302b, to which the observation position 302a will be moved forward along the base direction 304a at velocity V relative to the current base direction 304a stored in the status information storage 121. In other words, the observation position updater 112 updates to the observation position 302b, which is frontward away from the observation position 302a by a distance V × (tb - ta).</p>
<p id="p0029" num="0029">The time interval between time ta and time tb may be determined according to an intended application. For example, the refresh interval (such as a vertical synchronization period) for the display 104 in the display device 100 may be used as the time interval. Time tb in one repeating unit is taken as time ta in the next repeating unit because the process is repeated as described later.</p>
<p id="p0030" num="0030">In the present embodiment, the control unit 101 functions as the observation position updater 112.</p>
<p id="p0031" num="0031">The detector 113 detects a change in orientation of the display device 100 in the real space 201. For example, from measurements provided by the sensor 105, the detector 113 detects a change in the display direction 205 of the display device 100 relative to the reference axis in the real space 201. As the reference axis in the real space 201, either the direction of gravitational force in the real space 201 or the vertical direction of the display 104 in the display device 100 is typically adopted.</p>
<p id="p0032" num="0032">In the present embodiment, the control unit 101 and the sensor 105 collaborate with each other to function as the detector 113.</p>
<p id="p0033" num="0033">The observation direction updater 114 updates the observation direction 303 in the virtual space 301, in accordance with the orientation of the display device 100 as detected by the detector 113. For example, the observation direction updater 114 begins with obtaining an amount of rotation θ around the reference axis in the real space 201, with regard to the change in the display direction 205 of the display device 100 as detected by the detector 113 from time t<sub>a</sub> to time t<sub>b</sub>. Specifically, <figref idref="f0002">FIG. 2</figref> shows that the<!-- EPO <DP n="11"> --> display device 100a faces the user 202 in the display direction 205a before the user 202 changes the orientation of the head 203a. <figref idref="f0003">FIG. 3A</figref> shows that the display device 100b faces the user 202 in the display direction 205b after the user 202 changes the orientation of the head 203b. The angle formed between the display direction 205a and the display direction 205b is the amount of rotation θ around the reference axis. The amount of rotation θ is equivalent to the so-called yaw angle.</p>
<p id="p0034" num="0034">Then, as illustrated in <figref idref="f0005">FIG. 5A</figref>, the observation direction updater 114 updates the observation direction 303a to the observation direction 303b by rotating the observation direction 303a, which is stored in the state information storage 121, around the reference axis of the observation position 302a in the virtual space 301 by the amount of rotation θ.</p>
<p id="p0035" num="0035">In the present embodiment, the control unit 101 functions as the observation direction updater 114.</p>
<p id="p0036" num="0036">The obtainer 115 obtains from the controller 220 the relative position between the controller 220 and the display device 100.</p>
<p id="p0037" num="0037">The following describes an example method for obtaining the relative position in the present embodiment. <figref idref="f0007">FIG. 7</figref> is a front view of the controller 220 according to the present embodiment. The controller 220 according to the present embodiment is a smart phone including a touch panel 221 and a camera 222, both disposed on the front face as illustrated in <figref idref="f0007">FIG. 7</figref>. The touch panel 221 accepts an instruction input given by the user 202 indicating a moving direction. For example, when the user 202 slides a finger of the user 202 on the touch panel 221, the touch panel 221 detects the sliding direction and recognizes that an instruction input indicating the moving direction corresponding to the detected sliding direction has been accepted. The camera 222, which is an example of a sensor for detecting the display device 100, takes images of a scene in front of the camera 222. Specifically, the camera 222 takes images of a scene including the display device 100, with the camera 222 held by the user 202 as<!-- EPO <DP n="12"> --> illustrated in <figref idref="f0002">FIGS. 2</figref> and <figref idref="f0003">3</figref>. On the basis of the position of the display device appearing in an image taken by the camera 222, the controller 220 identifies the relative position between the controller 220 and the display device 100.</p>
<p id="p0038" num="0038">The following describes in detail an example method not covered by the claimed invention for identifying the relative position based on the position of the display device in an image taken by the camera 222. <figref idref="f0008">FIG. 8</figref> is an external view of the display device 100 worn by the user according to an example not covered by the claimed invention. As shown in <figref idref="f0008">FIG. 8</figref>, the display device 100 includes markers 501 and 502 disposed at the right and left ends of the bottom. The controller 220 recognizes the markers 501 and 502 in an image taken by the camera 220 to identify the relative position based on the positions of the markers 501 and 502 in the image. In the following description, it is assumed that the controller 220 is always positioned in front of the user 202, and the relative position ϕ to be identified is defined as an angle formed between the display direction 205 and a reference direction 207, which is the display direction 205 of the display device 100 worn by the user 202 facing frontward. <figref idref="f0009">FIGS. 9A, 9B</figref>, <figref idref="f0010">10A, and 10B</figref> show examples of the relative position between the display device 100 and the controller 220 and examples of images taken by the camera 220 at the relative position. <figref idref="f0009">FIG. 9A</figref> is a top view of the display device 100 and the controller 220 when the relative position ϕ is 0. At this relative position, the controller 220 takes an image 601 as illustrated in <figref idref="f0009">FIG. 9B. FIG. 9B</figref> shows that the markers 501 and 502 are positioned to be line-symmetrical with respect to a vertical axis of symmetry in the image 601.</p>
<p id="p0039" num="0039"><figref idref="f0010">FIG. 10A</figref> is a top view of the display device 100 and the controller 220 when the relative position ϕ = ϕ<sub>1</sub> (ϕ<sub>1</sub> &gt; 0). At this relative position, the controller 220 takes an image 602 as illustrated in <figref idref="f0010">FIG. 10B. FIG. 10B</figref> shows that the markers 501 and 502 are shifted to the right from the positions of the markers 501 and 502 in the image 601 in <figref idref="f0009">FIG. 9B</figref>.</p>
<p id="p0040" num="0040">The controller 220 is capable of identifying the relative position ϕ from<!-- EPO <DP n="13"> --> positional relationships of the markers 501 and 502 in an image taken by the camera 222, by storing in advance a relative position ϕ and positional relationships of the markers 501 and 502 in an image taken by the camera 222 at the relative position ϕ as illustrated in <figref idref="f0009">FIGS. 9A and 9B</figref> and in <figref idref="f0010">FIGS. 10A and 10B</figref>. The controller 220 transmits the identified relative position ϕ to the display device 100. Thus, the obtainer 115 in the display device 100 can obtain the relative position ϕ from the controller 220 via the communicator 107.</p>
<p id="p0041" num="0041">In the present embodiment, the control unit 101 and the communicator 107 collaborate with each other to function as the obtainer 115.</p>
<p id="p0042" num="0042">The base direction updater 116 updates the base direction 304, on the basis of the observation direction 303 and of the relative position obtained by the obtainer 115.</p>
<p id="p0043" num="0043">By way of example, the following describes an update of the base direction in the case where the obtainer 115 has obtained, from the controller 220, a relative position ϕ, which is an angle formed between the reference direction 207 and the display direction 205. For example, when the obtainer 115 obtains from the controller 220 the relative position ϕ = ϕ<sub>1</sub> as of time tb, the base direction updater 116 determines the base direction 304b so as to form an angle ϕ<sub>1</sub> with the observation direction 303b as illustrated in <figref idref="f0005">FIG. 5B</figref>, because a relative position ϕ corresponds to an angle formed between the observation direction 303 and the base direction 304 in the virtual space 301. In other words, the base direction updater 116 updates the base direction 304a to the base direction 304b by rotating, by a relative position of ϕ<sub>1</sub>, the observation direction 303b, which is stored in the state information storage 121, around the reference axis of the observation position 302a, which is stored in the state information storage 121.</p>
<p id="p0044" num="0044">In the present embodiment, the control unit 101 functions as the base direction updater 116.</p>
<p id="p0045" num="0045">On the basis of the observation position 302 and the observation direction 303 that are stored in the state information 121, the generator 117 generates an image<!-- EPO <DP n="14"> --> of the virtual space 301 as observed from the observation position 302 in the observation direction 303. Specifically, the generator 117 generates an image of objects (not illustrated) placed in the virtual space, as seen from the observation position 302 in the observation direction 303, by using a technique such as perspective projection, on the basis of predetermined positions and shapes of the objects in the virtual space 301. For example, as illustrated in <figref idref="f0003">FIG. 3A</figref>, in the case where the user 202 changes the orientation of the display device 100 by changing the orientation of the head 203 only, the generator 117 generates, at time ta, an image of the virtual space 301 observed from the observation position 302a in the observation direction 303a indicated in <figref idref="f0005">FIG. 5A</figref>, and generates, at time tb, an image of the virtual space 301 observed from the observation position 302b in the observation direction 303b indicated in <figref idref="f0005">FIG. 5A</figref>. In the case where the user 202 changes the orientation of the torso 204 only as illustrated in <figref idref="f0003">FIG. 3B</figref>, the generator 117 generates, at time ta, an image of the virtual space 301 observed from the observation position 302a in the observation direction 303a indicated in <figref idref="f0005">FIG. 5B</figref>, and generates, at time tb, an image of the virtual space 301 observed from the observation position 302b in the observation direction 303b indicated in <figref idref="f0005">FIG. 5B</figref>.</p>
<p id="p0046" num="0046">In the present embodiment, the control unit 101 functions as the generator 117.</p>
<p id="p0047" num="0047">The display unit 118 shows an image generated by the generator 117 on the display 104.</p>
<p id="p0048" num="0048">In the present embodiment, the control unit 101 and the display 104 collaborate with each other to function as the display unit 118.</p>
<p id="p0049" num="0049">The following describes operations of the display device 100 according to the embodiment of the present disclosure. <figref idref="f0011">FIG. 11</figref> is a flowchart illustrating a flow of a display process executed by the control unit 101 in the display device 100. Start of the process is triggered by, for example, acceptance of an instruction to start the process via the operator 106.<!-- EPO <DP n="15"> --></p>
<p id="p0050" num="0050">First, the display device 100 initializes the observation position 302 and the observation direction 303 in the virtual space 301 to a predetermined position and direction, and stores the observation position 302 and the observation direction 303 into the state information storage 121 (step S101).</p>
<p id="p0051" num="0051">Next, the display device 100 initializes the virtual space 301 (step S102). During the initialization, the display device 100 makes settings, including, for example, obtaining and setting the position, shape, orientation, and appearance of an object to be placed in the virtual space 301, and obtaining and setting an image of the background supposed to be placed at an infinite distance in the virtual space 301.</p>
<p id="p0052" num="0052">The display device 100 then determines whether an instruction input has been accepted from the controller 220 (step S103). If no instruction input has been accepted from the controller (No in step S103), the display device 100 goes to step S105.</p>
<p id="p0053" num="0053">If an instruction input has been accepted from the controller (Yes in step S103), the display device 100 updates the observation position 302, which is stored in the state information storage 121, to the observation position 302, to which the observation position will be moved by a certain distance in the moving direction indicated by the accepted instruction input (step S104).</p>
<p id="p0054" num="0054">Then, the display device 100 determines whether a change in orientation of the display device 100 has been detected (step S105). If no change in orientation of the display device 100 has been detected (No in step S105), the display device 100 goes to step S107.</p>
<p id="p0055" num="0055">If a change in orientation of the display device 100 has been detected (Yes in step S105), the display device 100 updates the observation direction 303, which is stored in the state information storage 121, in accordance with the amount of rotation θ representing the detected change in orientation of the display device 100 (step S106).</p>
<p id="p0056" num="0056">Next, the display device 100 obtains the relative position between the controller 220 and the display device 100 (step S107). On the basis of the observation<!-- EPO <DP n="16"> --> direction 303, which is stored in the state information storage 121, and of the obtained relative position, the display device 100 updates the base direction 304, which is stored in the state information storage 121 (step S108).</p>
<p id="p0057" num="0057">Then, the display device 100 generates an image of the virtual space 301 observed in the observation direction 303 from the observation position 302, as stored in the state information storage 121 (step S109).</p>
<p id="p0058" num="0058">After that, the display device 100 waits until a vertical synchronization interrupt occurs on the display 104 (step S110), and then transfers the generated image to the display 104 to present the image to the user (step S111).</p>
<p id="p0059" num="0059">Then, the display device 100 updates the state of the virtual space 301 (step S112). For example, in the case where the virtual space 301 is created by computer graphics that change with time, the display device 100 conducts a physical simulation by which the position and orientation of an object are updated in accordance with the velocity, acceleration, angular velocity, angular acceleration, and the like that are set to the object, or deforms an object in accordance with predetermined conditions.</p>
<p id="p0060" num="0060">The display device 100 then returns to the processing in step S103. The display device 100 repeats the foregoing process until, for example, an instruction to exit the process is accepted via the operator 106. Since the display device 100 waits in step S110, the cycle period for this process is a vertical synchronization period.</p>
<p id="p0061" num="0061">As described above, upon acceptance of an instruction input indicating the moving direction given via the controller 220, the display device 100 according to the embodiment of the present disclosure updates the observation position of the avatar so that the observation position moves in the moving direction indicated by the instruction input relative to the base direction of the avatar 401. In addition, the display device 100 obtains the relative position between the controller 220 and the display device 100 by identifying the position of the display device 100 appearing in an image taken by the camera 222 included in the controller 220. Then, the display device 100 updates the<!-- EPO <DP n="17"> --> base direction, on the basis of the observation direction of the avatar 401 as updated in accordance with the change in orientation of the display device detected by the sensor 105 and of the obtained relative position. Thus, the observation direction 302, which represents the orientation of the head 402 of the avatar 401 in the virtual space 301, and the base direction 304, which represents the orientation of the torso 403 of the avatar, are managed independently by the display device 100. Therefore, even when the user intuitively inputs to the controller 220 an instruction input indicating the moving direction relative to the frontward direction 206 of the torso 204 of the user, the display device 100 is capable of moving the observation position 302 in the virtual space in accordance with such intuitive instruction input given by the user.</p>
<p id="p0062" num="0062">In addition, the display device 100 obtains from the controller 220 the relative position identified by the controller 220 on the basis of the position of the display device 100 appearing in an image taken by the camera 222. Thus, the display device 100 can obtain the relative position without using an external device such as a camera for taking an image that includes images of the controller 220 and the display device 100. Therefore, the display device 100 can be implemented in a lower-cost configuration.</p>
<p id="p0063" num="0063">Embodiments of the present disclosure have been described above, but these embodiments are examples only and the scope of the present disclosure is not limited thereto. In other words, the present disclosure allows for various applications and every possible embodiment is included in the scope of the present disclosure.</p>
<p id="p0064" num="0064">For example, in an example not covered by the claimed invention, the display device 100 includes two markers 501 and 502, but the number of markers included in the display device 100 is not limited to two. The display device 100 may include a single marker or may include at least three markers.</p>
<p id="p0065" num="0065">In the mentioned example not covered by the claimed invention, the camera 222 included in the controller 220 recognizes the markers 501 and 502 included in the display device 100, whereby the controller 220 identifies the relative position between the<!-- EPO <DP n="18"> --> controller 220 and the display device 100 and transmits the identified relative position to the display device 100. However, according to the claimed invention, the display device 100 includes, instead of the markers 501 and 502, a light emitter such as a light-emitting diode (LED). In this case, the display device 100 causes the light emitter to blink light in a predetermined pattern so that the controller 220 can recognize the light emitter in an image taken by the camera 222, thereby identifying the relative position based on the position of the light emitter.</p>
<p id="p0066" num="0066">According to the claimed invention, the controller 220 identifies the relative position and transmits the identified relative position to the display device 100, whereby the display device 100 obtains the relative position. In another example not covered by he claimed invention, the display device 100 includes a camera and the controller 220 includes markers, the camera included in the display device 100, instead of the camera 222 included in the controller 220, may take an image of the controller 220, and the display device 100 may identify the relative position from the positions of the markers appearing in the taken image. In other words, a sensor included in one of the controller 220 and the display device 100 detects the other one of the controller 220 and the display device 100, the sensor being included in at least one of the controller 220 and the display device 100, whereby the relative position between the controller 220 and the display device 100 can be obtained.</p>
<p id="p0067" num="0067">When the camera 222 in the controller 220 is unable to detect the display device 100 in the foregoing embodiment, such as, for example, when the markers 501 and 502 are unrecognized based on an image taken by the camera 222 in the controller 220, the display device 100 may obtain a relative position estimated from a difference between a first orientation detected by an attitude sensor included in the controller 220<!-- EPO <DP n="19"> --> and a second orientation detected by an attitude sensor included in the sensor 105 in the display device 100. Suppose that, as illustrated in an example in <figref idref="f0012">FIG. 12</figref>, at time ta, the attitude sensor in the controller 220 detected the first orientation, which is a display direction 208a of the controller 220, while the attitude sensor included in the sensor 105 in the display device 100 detected a display direction 205a. Then, the controller 220 may transmit the detected display direction 208a to the display device 100, and the display device 100 can estimate the relative position ϕ from a difference between the received display direction 208a and the detected display direction 205a. Alternatively, the display device 100 may transmit the detected display direction 205a to the controller 220, and the controller 220 may estimate the relative position ϕ from a difference between the detected display direction 208a and the received display direction 205a. In this case, the controller 220 transmits the estimated relative position ϕ to the display device 100, whereby the display device 100 can obtain the relative position ϕ.</p>
<p id="p0068" num="0068">Functions according to the present disclosure can be provided in the form of the display device 100 pre-configured to implement these functions, and furthermore, an existing apparatus such as a personal computer or an information terminal device can function as the display device 100 according to the present disclosure when a program is applied to the apparatus. In other words, by applying a program for implementing the individual functional components of the display device 100 illustrated in the foregoing embodiment to an existing personal computer or information terminal device in such a way that the program can be executed by the CPU or the like that controls the existing personal computer or information terminal device, the personal computer or the information terminal device can function as the display device 100 according to the present disclosure. A display method according to the present disclosure can be implemented by using the display device 100.</p>
<p id="p0069" num="0069">In addition to the aforementioned method, the program can be applied by any appropriate method. For example, the program stored in a computer-readable<!-- EPO <DP n="20"> --> storage medium (a compact disc read-only memory (CD-ROM), a digital versatile disc (DVD), a magneto-optical disc (MO), and so on) can be applied, or the program stored in a storage on a network such as the Internet can be downloaded to be applied.</p>
<p id="p0070" num="0070">The specification and drawings are to be regarded in an illustrative rather than a restrictive sense. This detailed description, therefore, is not to be taken in a limiting sense, and the scope of the invention is defined only by the included claims.</p>
<heading id="h0011">Industrial Applicability</heading>
<p id="p0071" num="0071">The present disclosure achieves providing a display device, a display method, a program, and a non-temporary computer-readable information storage medium for displaying images.</p>
<heading id="h0012">Reference Signs List</heading>
<p id="p0072" num="0072">
<dl id="dl0001" compact="compact">
<dt>100 (100a, 100b)</dt><dd>Display device</dd>
<dt>101</dt><dd>Control unit</dd>
<dt>102</dt><dd>ROM</dd>
<dt>103</dt><dd>RAM</dd>
<dt>104</dt><dd>Display</dd>
<dt>105</dt><dd>Sensor</dd>
<dt>106</dt><dd>Operator</dd>
<dt>107</dt><dd>Communicator</dd>
<dt>108</dt><dd>Bus</dd>
<dt>111</dt><dd>Accepter</dd>
<dt>112</dt><dd>Observation position updater<!-- EPO <DP n="21"> --></dd>
<dt>113</dt><dd>Detector</dd>
<dt>114</dt><dd>Observation direction updater</dd>
<dt>115</dt><dd>Obtainer</dd>
<dt>116</dt><dd>Base direction updater</dd>
<dt>117</dt><dd>Generator</dd>
<dt>118</dt><dd>Display unit</dd>
<dt>121</dt><dd>State information storage</dd>
<dt>201</dt><dd>Real space</dd>
<dt>202</dt><dd>User</dd>
<dt>203 (203a, 203b)</dt><dd>User's head</dd>
<dt>204 (204a, 204b)</dt><dd>User's torso</dd>
<dt>205 (205a, 205b)</dt><dd>Display direction of display device</dd>
<dt>206 (206a, 206b)</dt><dd>Frontward direction</dd>
<dt>207 (207a, 207b)</dt><dd>Reference direction</dd>
<dt>208a</dt><dd>Display direction of controller</dd>
<dt>211</dt><dd>Azimuth directions in real space</dd>
<dt>220</dt><dd>Controller</dd>
<dt>221</dt><dd>Touch panel</dd>
<dt>222</dt><dd>Camera</dd>
<dt>301</dt><dd>Virtual space</dd>
<dt>302 (302a, 302b)</dt><dd>Observation position</dd>
<dt>303 (303a, 303b)</dt><dd>Observation direction</dd>
<dt>304 (304a, 304b)</dt><dd>Base direction</dd>
<dt>311</dt><dd>Azimuth directions in virtual space</dd>
<dt>401 (401a, 401b)</dt><dd>Avatar</dd>
<dt>402 (402a, 402b)</dt><dd>Avatar's head</dd>
<dt>403 (403a, 403b)</dt><dd>Avatar's torso<!-- EPO <DP n="22"> --></dd>
<dt>501, 502</dt><dd>Marker</dd>
<dt>601, 602</dt><dd>Image</dd>
</dl></p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="23"> -->
<claim id="c-en-01-0001" num="0001">
<claim-text>A display device (100) comprising:
<claim-text>an accepter (111) that accepts an instruction input indicating a moving direction, the instruction input being given to an external controller (220);</claim-text>
<claim-text>a detector (113) that detects a change in orientation of the display device (100);</claim-text>
<claim-text>a storage (121) that stores an avatar observation position, an avatar observation direction, and an avatar base direction in a virtual space;</claim-text>
<claim-text>an observation position updater (112) that updates, upon acceptance of the instruction input, the avatar observation position so that the avatar observation position moves in the moving direction indicated by the instruction input relative to the avatar base direction;</claim-text>
<claim-text>an observation direction updater (114) that updates the avatar observation direction in accordance with the detected change in orientation of the display device (100);</claim-text>
<claim-text>an obtainer (115) that obtains a relative position between the external controller (220) and the display device (100);</claim-text>
<claim-text>a base direction updater (116) that updates the avatar base direction, based on the avatar observation direction and on the obtained relative position;</claim-text>
<claim-text><b>characterized by</b> further comprising</claim-text>
<claim-text>a light emitter that shines light in a predetermined pattern,</claim-text>
<claim-text>wherein the obtainer (115) obtains from the external controller (220) the relative position that is identified by the external controller (220) based on a position of the light emitter in an image taken by a camera (222) included in the external controller (220) and on the pattern.</claim-text></claim-text></claim>
<claim id="c-en-01-0002" num="0002">
<claim-text>The display device (100) according to claim 1,<br/>
wherein the obtainer (115) obtains the relative position through detection performed by a sensor (222) included in the external controller (220) to detect<!-- EPO <DP n="24"> --> the display device (100).</claim-text></claim>
<claim id="c-en-01-0003" num="0003">
<claim-text>The display device according to claim 1,<br/>
wherein the obtainer (115) obtains the relative position estimated from a difference between a first orientation detected by a first attitude sensor included in the external controller (220) and a second orientation detected by a second attitude sensor included in the display device (100).</claim-text></claim>
<claim id="c-en-01-0004" num="0004">
<claim-text>A display method executed by a display device (100) that comprises a storage (121) storing an avatar observation position, an avatar observation direction, and an avatar base direction in a virtual space, the display method comprising:
<claim-text>an accepting step of accepting an instruction input indicating a moving direction, the instruction input being given to an external controller (220);</claim-text>
<claim-text>a detecting step of detecting a change in orientation of the display device (100);</claim-text>
<claim-text>an observation position updating step of updating, upon acceptance of the instruction input, the avatar observation position so that the avatar observation position moves in the moving direction indicated by the instruction input relative to the avatar base direction;</claim-text>
<claim-text>an observation direction updating step of updating the avatar observation direction in accordance with the detected change in orientation of the display device (100);</claim-text>
<claim-text>an obtaining step of obtaining a relative position between the external controller (220) and the display device (100);</claim-text>
<claim-text>a base direction updating step of updating the avatar base direction, based on the avatar observation direction and on the obtained relative position;</claim-text>
<claim-text><b>characterized by</b> further comprising</claim-text>
<claim-text>an emitting step of shining, by a light emitter, light in a predetermined pattern,</claim-text>
<claim-text>wherein in the obtaining step the relative position is obtained based on a position of the light emitter in an image taken by a camera (222)<!-- EPO <DP n="25"> --> included in the external controller (220) and on the pattern.</claim-text></claim-text></claim>
<claim id="c-en-01-0005" num="0005">
<claim-text>A non-temporary computer-readable information storage medium recording a program causing a computer that comprises a storage (121) storing an avatar observation position, an avatar observation direction, and an avatar base direction in a virtual space to function as:
<claim-text>an accepter (111) that accepts an instruction input indicating a moving direction, the instruction input being given to an external controller (220);</claim-text>
<claim-text>a detector (113) that detects a change in orientation of a display device;</claim-text>
<claim-text>an observation position updater (112) that updates, upon acceptance of the instruction input, the avatar observation position so that the avatar observation position moves in the moving direction indicated by the instruction input relative to the avatar base direction;</claim-text>
<claim-text>an observation direction updater (114) that updates the avatar observation direction in accordance with the detected change in orientation of the display device (100);</claim-text>
<claim-text>an obtainer (115) that obtains a relative position between the external controller (220) and the display device (100);</claim-text>
<claim-text>a base direction updater (116) that updates the avatar base direction, based on the avatar observation direction and on the obtained relative position;</claim-text>
<claim-text><b>characterized by</b> further comprising</claim-text>
<claim-text>a light emitter that shines light in a predetermined pattern,</claim-text>
<claim-text>wherein the obtainer (115) obtains from the external controller (220) the relative position that is identified by the external controller (220) based on a position of the light emitter in an image taken by a camera (222) included in the external controller (220) and on the pattern.</claim-text></claim-text></claim>
</claims>
<claims id="claims02" lang="de"><!-- EPO <DP n="26"> -->
<claim id="c-de-01-0001" num="0001">
<claim-text>Anzeigevorrichtung (100), umfassend:
<claim-text>einen Abnehmer (111), der eine Befehlseingabe abnimmt, die eine Bewegungsrichtung angibt, wobei die Befehlseingabe einer externen Steuereinheit (220) gegeben wird;</claim-text>
<claim-text>einen Detektor (113), der eine Ausrichtungsänderung der Anzeigevorrichtung (100) detektiert;</claim-text>
<claim-text>einen Speicher (121), der eine Avatarbeobachtungsposition, eine Avatarbeobachtungsrichtung und eine Avatarbasisrichtung in einem virtuellen Raum speichert;</claim-text>
<claim-text>einen Beobachtungspositionsaktualisierer (112), der bei Abnahme der Befehlseingabe die Avatarbeobachtungsposition aktualisiert, sodass sich die Avatarbeobachtungsposition in der Bewegungsrichtung, die von der Befehlseingabe angegeben wird, relativ zu der Avatarbasisrichtung bewegt;</claim-text>
<claim-text>einen Beobachtungsrichtungsaktualisierer (114), der die Avatarbeobachtungsrichtung in Übereinstimmung mit der detektierten Ausrichtungsänderung der Anzeigevorrichtung (100) aktualisiert;</claim-text>
<claim-text>einen Erhalter (115), der eine relative Position zwischen der externen Steuereinheit (220) und der Anzeigevorrichtung (100) erhält;</claim-text>
<claim-text>einen Basisrichtungsaktualisierer (116), der die Avatarbasisrichtung basierend auf der Avatarbeobachtungsrichtung und der erhaltenen relativen Position aktualisiert;</claim-text>
<claim-text><b>dadurch gekennzeichnet, dass</b> sie weiter umfasst</claim-text>
<claim-text>einen Lichtemitter, der Licht in einer vorgegebenen Struktur ausscheint,</claim-text>
<claim-text>wobei der Erhalter (115) von der externen Steuereinheit (220) die relative Position erhält, die von der externen Steuereinheit (220) basierend auf einer Position des Lichtemitters in einem Bild, das von einer Kamera (222) aufgenommen wird, die in der externen Steuereinheit (220) beinhaltet ist, und auf der Struktur identifiziert wird.</claim-text></claim-text></claim>
<claim id="c-de-01-0002" num="0002">
<claim-text>Anzeigevorrichtung (100) nach Anspruch 1,<br/>
wobei der Erhalter (115) die relative Position durch Detektion erhält, die von einem Sensor (222) durchgeführt wird, der in der externen Steuereinheit (220) beinhaltet ist, um die Anzeigevorrichtung (100) zu detektieren.<!-- EPO <DP n="27"> --></claim-text></claim>
<claim id="c-de-01-0003" num="0003">
<claim-text>Anzeigevorrichtung nach Anspruch 1,<br/>
wobei der Erhalter (115) die relative Position erhält, die von einem Unterschied zwischen einer ersten Ausrichtung, die von einem ersten Lagesensor detektiert wird, der in der externen Steuereinheit (220) beinhaltet ist, und einer zweiten Ausrichtung, die von einem zweiten Lagesensor detektiert wird, der in der Anzeigevorrichtung (100) beinhaltet ist, geschätzt wird.</claim-text></claim>
<claim id="c-de-01-0004" num="0004">
<claim-text>Anzeigeverfahren, das von einer Anzeigevorrichtung (100) ausgeführt wird, die einen Speicher (121) umfasst, der eine Avatarbeobachtungsposition, eine Avatarbeobachtungsrichtung und eine Avatarbasisrichtung in einem virtuellen Raum speichert, wobei das Anzeigeverfahren umfasst:
<claim-text>einen Abnahmeschritt zum Abnehmen einer Befehlseingabe, die eine Bewegungsrichtung angibt, wobei die Befehlseingabe einer externen Steuereinheit (220) gegeben wird;</claim-text>
<claim-text>einen Detektionsschritt zum Detektieren einer Ausrichtungsänderung der Anzeigevorrichtung (100);</claim-text>
<claim-text>einen Beobachtungspositionsaktualisierungsschritt zum Aktualisieren, bei Abnahme der Befehlseingabe, der Avatarbeobachtungsposition, sodass sich die Avatarbeobachtungsposition in der Bewegungsrichtung, die von der Befehlseingabe angegeben wird, relativ zu der Avatarbasisrichtung bewegt;</claim-text>
<claim-text>einen Beobachtungsrichtungsaktualisierungsschritt zum Aktualisieren der Avatarbeobachtungsrichtung in Übereinstimmung mit der detektierten Ausrichtungsänderung der Anzeigevorrichtung (100);</claim-text>
<claim-text>einen Erhaltungsschritt zum Erhalten einer relativen Position zwischen der externen Steuereinheit (220) und der Anzeigevorrichtung (100);</claim-text>
<claim-text>einen Basisrichtungsaktualisierungsschritt zum Aktualisieren der Avatarbasisrichtung, basierend auf der Avatarbeobachtungsrichtung und der erhaltenen relativen Position;</claim-text>
<claim-text><b>dadurch gekennzeichnet, dass</b> es weiter umfasst</claim-text>
<claim-text>einen Emittierungsschritt zum Scheinen, durch einen Lichtemitter, von Licht in einer neubestimmten Struktur,<!-- EPO <DP n="28"> --></claim-text>
<claim-text>wobei in dem Erhaltungsschritt die relative Position basierend auf einer Position des Lichtemitters in einem Bild, das von einer Kamera (222), die in der externen Steuereinheit (220) beinhaltet ist, und auf der Struktur erhalten wird.</claim-text></claim-text></claim>
<claim id="c-de-01-0005" num="0005">
<claim-text>Nichtflüchtiges computerlesbares Informationsspeichermedium, das ein Programm aufzeichnet, das einen Computer, der einen Speicher (121) umfasst, der eine Avatarbeobachtungsposition, eine Avatarbeobachtungsrichtung und eine Avatarbasisrichtung in einem virtuellen Raum speichert, veranlasst, zu fungieren als:
<claim-text>ein Abnehmer (111), der eine Befehlseingabe abnimmt, die eine Bewegungsrichtung angibt, wobei die Befehlseingabe einer externen Steuereinheit (220) gegeben wird;</claim-text>
<claim-text>ein Detektor (113), der eine Ausrichtungsänderung der Anzeigevorrichtung (100) detektiert;</claim-text>
<claim-text>ein Beobachtungspositionsaktualisierer (112), der bei Abnahme der Befehlseingabe die Avatarbeobachtungsposition aktualisiert, sodass sich die Avatarbeobachtungsposition in der Bewegungsrichtung, die von der Befehlseingabe angegeben wird, relativ zu der Avatarbasisrichtung bewegt;</claim-text>
<claim-text>ein Beobachtungsrichtungsaktualisierer (114), der die Avatarbeobachtungsrichtung in Übereinstimmung mit der detektierten Ausrichtungsänderung der Anzeigevorrichtung (100) aktualisiert;</claim-text>
<claim-text>ein Erhalter (115), der eine relative Position zwischen der externen Steuereinheit (220) und der Anzeigevorrichtung (100) erhält;</claim-text>
<claim-text>ein Basisrichtungsaktualisierer (116), der die Avatarbasisrichtung basierend auf der Avatarbeobachtungsrichtung und der erhaltenen relativen Position aktualisiert;</claim-text>
<claim-text><b>dadurch gekennzeichnet, dass</b> es weiter umfasst</claim-text>
<claim-text>einen Lichtemitter, der Licht in einer vorgegebenen Struktur ausscheint,</claim-text>
<claim-text>wobei der Erhalter (115) von der externen Steuereinheit (220) die relative Position erhält, die von der externen Steuereinheit (220) basierend auf einer Position des Lichtemitters in einem Bild, das von einer Kamera (222) aufgenommen wird, die in der externen Steuereinheit (220) beinhaltet ist, und auf der Struktur identifiziert wird.</claim-text></claim-text></claim>
</claims>
<claims id="claims03" lang="fr"><!-- EPO <DP n="29"> -->
<claim id="c-fr-01-0001" num="0001">
<claim-text>Dispositif d'affichage (100) comprenant :
<claim-text>une unité d'acceptation (111) qui accepte une entrée d'instruction indiquant une direction de mouvement, l'entrée d'instruction étant fournie à un dispositif de commande externe (220) ;</claim-text>
<claim-text>une unité de détection (113) qui détecte un changement d'une orientation du dispositif d'affichage (100) ;</claim-text>
<claim-text>une unité de stockage (121) qui stocke une position d'observation d'avatar, une direction d'observation d'avatar et une direction de base d'avatar dans un espace virtuel ;</claim-text>
<claim-text>une unité de mise à jour de position d'observation (112) qui met à jour, à l'acceptation de l'entrée d'instruction, la position d'observation d'avatar de telle manière que la position d'observation d'avatar se déplace dans la direction de mouvement indiquée par l'entrée d'instruction par rapport à la direction de base d'avatar ;</claim-text>
<claim-text>une unité de mise à jour de direction d'observation (114) qui met à jour la direction d'observation d'avatar conformément au changement détecté d'orientation du dispositif d'affichage (100) ;</claim-text>
<claim-text>une unité d'acquisition (115) qui acquiert une position relative entre le dispositif de commande externe (220) et le dispositif d'affichage (100) ;</claim-text>
<claim-text>une unité de mise à jour de direction de base (116) qui met à jour la direction de base d'avatar, sur la base de la direction d'observation d'avatar et de la position relative acquise ;</claim-text>
<claim-text><b>caractérisé en ce qu'</b>il comprend en outre</claim-text>
<claim-text>un émetteur de lumière qui rayonne une lumière selon un motif prédéterminé,</claim-text>
<claim-text>dans lequel l'unité d'acquisition (115) acquiert à partir du dispositif de commande externe (220) la position relative qui est identifiée par le dispositif de commande externe (220) sur la base d'une position de l'émetteur de lumière dans une image prise par une caméra (222) incluse dans le dispositif de commande externe (220) et du motif.</claim-text></claim-text></claim>
<claim id="c-fr-01-0002" num="0002">
<claim-text>Dispositif d'affichage (100) selon la revendication 1,<br/>
dans lequel l'unité d'acquisition (115) acquiert la position relative au moyen d'une détection effectuée par un capteur (222) inclus dans le dispositif de commande externe (220) pour détecter le dispositif d'affichage (100).<!-- EPO <DP n="30"> --></claim-text></claim>
<claim id="c-fr-01-0003" num="0003">
<claim-text>Dispositif d'affichage selon la revendication 1,<br/>
dans lequel l'unité d'acquisition (115) acquiert la position relative estimée à partir d'une différence entre une première orientation détectée par un premier capteur d'attitude inclus dans le dispositif de commande externe (220) et une seconde orientation détectée par un second capteur d'attitude inclus dans le dispositif d'affichage (100).</claim-text></claim>
<claim id="c-fr-01-0004" num="0004">
<claim-text>Procédé d'affichage exécuté par un dispositif d'affichage (100) qui comprend une unité de stockage (121) stockant une position d'observation d'avatar, une direction d'observation d'avatar et une direction de base d'avatar dans un espace virtuel, le procédé d'affichage comprenant :
<claim-text>une étape d'acceptation pour accepter une entrée d'instruction indiquant une direction de mouvement, l'entrée d'instruction étant fournie à un dispositif de commande externe (220) ;</claim-text>
<claim-text>une étape de détection pour détecter un changement d'une orientation du dispositif d'affichage (100) ;</claim-text>
<claim-text>une étape de mise à jour de position d'observation pour mettre à jour, à l'acceptation de l'entrée d'instruction, la position d'observation d'avatar de telle manière que la position d'observation d'avatar se déplace dans la direction de mouvement indiquée par l'entrée d'instruction par rapport à la direction de base d'avatar ;</claim-text>
<claim-text>une étape de mise à jour de direction d'observation pour mettre à jour la direction d'observation d'avatar conformément au changement détecté d'orientation du dispositif d'affichage (100) ;</claim-text>
<claim-text>une étape d'acquisition pour acquérir une position relative entre le dispositif de commande externe (220) et le dispositif d'affichage (100) ;</claim-text>
<claim-text>une étape de mise à jour de direction de base pour mettre à jour la direction de base d'avatar, sur la base de la direction d'observation d'avatar et de la position relative acquise ;</claim-text>
<claim-text><b>caractérisé en ce qu'</b>il comprend en outre</claim-text>
<claim-text>une étape d'émission pour rayonner, par un émetteur de lumière, une lumière selon un motif redéterminé,<!-- EPO <DP n="31"> --></claim-text>
<claim-text>dans lequel, dans l'étape d'acquisition, la position relative est acquise sur la base d'une position de l'émetteur de lumière dans une image prise par une caméra (222) incluse dans le dispositif de commande externe (220) et du motif.</claim-text></claim-text></claim>
<claim id="c-fr-01-0005" num="0005">
<claim-text>Support de stockage d'informations lisible par ordinateur non temporaire enregistrant un programme amenant un ordinateur, qui comprend une unité de stockage (121) stockant une position d'observation d'avatar, une direction d'observation d'avatar et une direction de base d'avatar dans un espace virtuel, à fonctionner comme :
<claim-text>une unité d'acceptation (111) qui accepte une entrée d'instruction indiquant une direction de mouvement, l'entrée d'instruction étant fournie à un dispositif de commande externe (220) ;</claim-text>
<claim-text>une unité de détection (113) qui détecte un changement d'une orientation d'un dispositif d'affichage ;</claim-text>
<claim-text>une unité de mise à jour de position d'observation (112) qui met à jour, à l'acceptation de l'entrée d'instruction, la position d'observation d'avatar de telle manière que la position d'observation d'avatar se déplace dans la direction de mouvement indiquée par l'entrée d'instruction par rapport à la direction de base d'avatar ;</claim-text>
<claim-text>une unité de mise à jour de direction d'observation (114) qui met à jour la direction d'observation d'avatar conformément au changement détecté d'orientation du dispositif d'affichage (100) ;</claim-text>
<claim-text>une unité d'acquisition (115) qui acquiert une position relative entre le dispositif de commande externe (220) et le dispositif d'affichage (100) ;</claim-text>
<claim-text>une unité de mise à jour de direction de base (116) qui met à jour la direction de base d'avatar, sur la base de la direction d'observation d'avatar et de la position relative acquise ;</claim-text>
<claim-text><b>caractérisé en ce qu'</b>il comprend en outre</claim-text>
<claim-text>un émetteur de lumière qui rayonne une lumière selon un motif prédéterminé,</claim-text>
<claim-text>dans lequel l'unité d'acquisition (115) acquiert à partir du dispositif de commande externe (220) la position relative qui est identifiée par le dispositif de commande externe (220) sur la base d'une position de l'émetteur de lumière dans une image prise par une caméra (222) incluse dans le dispositif de commande externe (220) et du motif.</claim-text></claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="32"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="108" he="184" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="110" he="144" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> -->
<figure id="f0003" num="3A,3B"><img id="if0003" file="imgf0003.tif" wi="120" he="225" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> -->
<figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="107" he="164" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> -->
<figure id="f0005" num="5A,5B"><img id="if0005" file="imgf0005.tif" wi="102" he="231" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> -->
<figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="111" he="187" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> -->
<figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="77" he="164" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> -->
<figure id="f0008" num="8"><img id="if0008" file="imgf0008.tif" wi="155" he="209" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> -->
<figure id="f0009" num="9A,9B"><img id="if0009" file="imgf0009.tif" wi="148" he="230" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="41"> -->
<figure id="f0010" num="10A,10B"><img id="if0010" file="imgf0010.tif" wi="141" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="42"> -->
<figure id="f0011" num="11"><img id="if0011" file="imgf0011.tif" wi="153" he="211" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="43"> -->
<figure id="f0012" num="12"><img id="if0012" file="imgf0012.tif" wi="110" he="144" img-content="drawing" img-format="tif"/></figure>
</drawings>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="JP2018072992A"><document-id><country>JP</country><doc-number>2018072992</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0001">[0003]</crossref></li>
<li><patcit id="ref-pcit0002" dnum="US2017083084A1"><document-id><country>US</country><doc-number>2017083084</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0002">[0004]</crossref></li>
<li><patcit id="ref-pcit0003" dnum="US9268136B1"><document-id><country>US</country><doc-number>9268136</doc-number><kind>B1</kind></document-id></patcit><crossref idref="pcit0003">[0005]</crossref></li>
<li><patcit id="ref-pcit0004" dnum="US2017357333A1"><document-id><country>US</country><doc-number>2017357333</doc-number><kind>A1</kind></document-id></patcit><crossref idref="pcit0004">[0006]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
