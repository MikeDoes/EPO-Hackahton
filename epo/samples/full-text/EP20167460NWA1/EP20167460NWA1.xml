<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP20167460A1" file="EP20167460NWA1.xml" lang="en" country="EP" doc-number="3888556" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3888556</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>20167460.3</B210><B220><date>20200401</date></B220><B250>en</B250><B251EP>en</B251EP><B260>en</B260></B200><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>A61B   8/00        20060101AFI20200807BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>G01S  15/89        20060101ALI20200807BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>G01S   7/52        20060101ALI20200807BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>G01S   7/539       20060101ALI20200807BHEP        </text></classification-ipcr><classification-ipcr sequence="5"><text>G06K   9/32        20060101ALI20200807BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G01S  15/8915      20130101 LI20201020BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>A61B   8/12        20130101 LA20200803BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>A61B   8/54        20130101 LI20200804BHEP        </text></classification-cpc><classification-cpc sequence="4"><text>G01S   7/5205      20130101 LA20200803BHEP        </text></classification-cpc><classification-cpc sequence="5"><text>A61B   8/585       20130101 FI20200803BHEP        </text></classification-cpc><classification-cpc sequence="6"><text>G01S   7/52046     20130101 LI20201020BHEP        </text></classification-cpc><classification-cpc sequence="7"><text>G01S   7/52036     20130101 LI20200803BHEP        </text></classification-cpc><classification-cpc sequence="8"><text>G06K   9/6262      20130101 LI20200901BHEP        </text></classification-cpc><classification-cpc sequence="9"><text>A61B   8/5223      20130101 LA20200804BHEP        </text></classification-cpc><classification-cpc sequence="10"><text>G06N   3/02        20130101 LI20200901BHEP        </text></classification-cpc><classification-cpc sequence="11"><text>G06K2209/05        20130101 LA20200804BHEP        </text></classification-cpc><classification-cpc sequence="12"><text>G01S  15/895       20130101 LI20200803BHEP        </text></classification-cpc><classification-cpc sequence="13"><text>G06K   9/20        20130101 LI20201104BHEP        </text></classification-cpc><classification-cpc sequence="14"><text>G06K   9/036       20130101 LI20201104BHEP        </text></classification-cpc><classification-cpc sequence="15"><text>G06K   9/6273      20130101 LI20200901BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>FOKUSOPTIMIERUNG ZUR VORHERSAGE IN DER MEHRFREQUENZULTRASCHALLBILDGEBUNG</B542><B541>en</B541><B542>FOCUS OPTIMIZATION FOR PREDICTION IN MULTI-FREQUENCY ULTRASOUND IMAGING</B542><B541>fr</B541><B542>OPTIMISATION DE MISE AU POINT POUR LA PRÉDICTION EN IMAGERIE À ULTRASONS MULTIFRÉQUENCE</B542></B540><B590><B598>1</B598></B590></B500><B700><B710><B711><snm>Koninklijke Philips N.V.</snm><iid>101851185</iid><irf>2019P00856EP</irf><adr><str>High Tech Campus 52</str><city>5656 AG Eindhoven</city><ctry>NL</ctry></adr></B711></B710><B720><B721><snm>SCHNELLBÄCHER, Nikolas David</snm><adr><str>c/o Philips International B.V. - Intellectual
Property &amp; Standards High Tech Campus 5</str><city>5656 AE Eindhoven</city><ctry>NL</ctry></adr></B721><B721><snm>WISSEL, Tobias</snm><adr><str>c/o Philips International B.V. - Intellectual
Property &amp; Standards High Tech Campus 5</str><city>5656 AE Eindhoven</city><ctry>NL</ctry></adr></B721><B721><snm>NICKISCH, Hannes</snm><adr><str>c/o Philips International B.V. - Intellectual
Property &amp; Standards High Tech Campus 5</str><city>5656 AE Eindhoven</city><ctry>NL</ctry></adr></B721><B721><snm>GRAß, Michael</snm><adr><str>c/o Philips International B.V. - Intellectual
Property &amp; Standards High Tech Campus 5</str><city>5656 AE Eindhoven</city><ctry>NL</ctry></adr></B721></B720><B740><B741><snm>Philips Intellectual Property &amp; Standards</snm><iid>101808802</iid><adr><str>High Tech Campus 5</str><city>5656 AE Eindhoven</city><ctry>NL</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">An imaging system (IS), comprising an image acquisition unit (AQ) for acquisition of image data (I1) of an object (OB). The image acquisition is based on an imaging signal imitable by the unit (AQ) to interact with the object. The image acquisition unit (AQ) is adjustable to operate at different acquisition parameters that determine a property of the imaging signal. A predictor component (PC) predicts, based at least on the acquired image data (I1), one or more properties of the object. An acquisition parameter adjuster (PA) adjusts, based on the predicted object properties, the acquisition parameter at which the image acquisition unit (AQ) is to acquire follow-up image data (12).
<img id="iaf01" file="imgaf001.tif" wi="83" he="89" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001">FIELD OF THE INVENTION</heading>
<p id="p0001" num="0001">The invention relates to an imaging system, to an imaging method, to a training method for a predictor component, to a computer program element, and to a computer readable medium.</p>
<heading id="h0002">BACKGROUND OF THE INVENTION</heading>
<p id="p0002" num="0002">Intravascular ultrasound (IVUS) plays an increasing role for vascular imaging in coronary as well as peripheral vessels.</p>
<p id="p0003" num="0003">Applications include evaluation of plaque or calcium burden, stenosis level or verifying implant placement.</p>
<p id="p0004" num="0004">Current US transducer technologies include PMUTs or CMUTs. Either one can be designed for multi-frequency IVUS applications.</p>
<p id="p0005" num="0005">In such technologies, one or more vibrating elements ("active source"), configurable as a piezo-resistor or a capacitive membrane, transmit and receive different carrier frequencies. In many system, a discrete set or array of such transducer elements, each tunable to a certain frequency.</p>
<p id="p0006" num="0006">In IVUS applications, image quality (IQ) such as image resolution, noise level, but other properties such as the size of the field-of-view (FoV) are a function of the used transducer frequency. For example, low frequencies decrease image resolution but enlarge the FoV and result in low noise characteristics. With higher frequencies, the resolution increases whereas the FoV decreases. This renders lower frequencies more suitable to detect and delineate calcium arcs and lumen borders, while higher frequencies work better for the detection of for example TCFA (thin-cap fibroatheroma), a special type of plaque, since the fiber cap is only a few µm thick. High frequency settings might also be better suited to detect thrombi, because they provide an enlarged view on the vessel lumen and possibly improved discrimination from normal blood.</p>
<p id="p0007" num="0007">There is hence a tradeoff between imaging properties or IQ, such as image resolution, penetration depth and noise characteristics, in dependence on the used signal<!-- EPO <DP n="2"> --> frequency, as different imaging tasks may call for different frequency regimes for best performance, such as classification or other.</p>
<heading id="h0003">SUMMARY OF THE INVENTION</heading>
<p id="p0008" num="0008">There may be a need for improving imaging.</p>
<p id="p0009" num="0009">The object of the present invention is solved by the subject matter of the independent claims where further embodiments are incorporated in the dependent claims. It should be noted that the following described aspect of the invention equally applies to the imaging method, to the training method, to the computer program element, and to the computer readable medium.</p>
<p id="p0010" num="0010">According to a first aspect of the invention there is provided an imaging system, comprising:
<ul id="ul0001" list-style="none" compact="compact">
<li>an image acquisition unit for acquisition of image data of at least a part of an object, the said acquisition being based on an imaging signal imitable by the unit to interact with the object, the image acquisition unit adjustable to operate at different acquisition parameters that determine at least in part a property of the imaging signal;</li>
<li>a predictor component configured to predict, based at least on the acquired image data, one or more properties of the object; and</li>
<li>an acquisition parameter adjuster configured to adjust, based on the predicted one or more properties, the acquisition parameter at which the image acquisition unit is to acquire follow-up image data.</li>
</ul></p>
<p id="p0011" num="0011">The system is preferably operable in real-time with prediction and/or adjustment done on the fly.</p>
<p id="p0012" num="0012">In embodiments of the medical field mainly envisaged herein, the object includes one or more human (internal) anatomies, or part thereof, such an organ or organ group. The acquisition is preferably configured to acquire imagery of internal anatomies.</p>
<p id="p0013" num="0013">The properties of the object include any one or more of: an identity/name or type, a finding or diagnosis, one or more measurements, and so on. Because certain object properties are of higher interest to certain imaging objectives/purposes or clinical tasks than to others, the object properties can be said to be linked more broadly to a type of clinical task/imaging purposes to be performed with respect of the object.</p>
<p id="p0014" num="0014">The prediction may include a classification, segmentation, delineation, or regression into measurement(s), or a combination of some or all of the foregoing.<!-- EPO <DP n="3"> --></p>
<p id="p0015" num="0015">The proposed system overcomes the above describe task-specificity and tradeoff of a single imaging parameter setting by automatically "focusing" the tunable imaging parameter to the best, or better, value for a given object property, and hence clinical task to be performed. For example, the system may help automatically "focus" the tunable IVUS frequency to the best value for a given segmentation.</p>
<p id="p0016" num="0016">Instead of acting on the image data directly, the system analyses the image data first to make predictions in relation to the object properties, and then adjusts the acquisition parameters accordingly based on the object property prediction to increase image quality on-line, in a feedback loop, and with better robustness. The system furnishes an autofocus capability where the acquisition unit are adjusted to improve an imaging objective as per the predicted object property.</p>
<p id="p0017" num="0017">The proposed system and method improves in particular (but not only) multi-frequency IVUS systems for intravascular imaging or other imaging purposes. The proposed system provides an automated frequency steering based on prediction performance and prediction uncertainty. This removes potential human bias from the acquisition and frequency selection. In addition, the system results in a better overall prediction performance and allows an optimized workflow for multi frequency IVUS imaging applications.</p>
<p id="p0018" num="0018">In embodiments, the acquisition parameter is adjusted automatically by the acquisition parameter adjuster so as to increase image quality.</p>
<p id="p0019" num="0019">In embodiments, the predictor component is to predict plural object properties associated with a respective uncertainty value, wherein the acquisition parameter adjuster is to adjust the acquisition parameter based on the uncertainty value or on a gradient thereof.</p>
<p id="p0020" num="0020">In embodiments, the acquisition parameter adjuster is to adjust the acquisition parameter so as to decrease the uncertainty value of the predicted one or more properties. Specifically, the predictor component may automatically provide an uncertainty estimation alongside with its prediction, to allow the said parameter adjuster to "steer" the acquisition parameter according to lowest / optimal prediction uncertainty.</p>
<p id="p0021" num="0021">In embodiments, the predictor component is to predict the one or more object properties based on a current acquisition parameter.</p>
<p id="p0022" num="0022">In more detail, in some such embodiments, the predictor component computes predictions and associated uncertainty measures for a pre-specified discrete set of acquisition parameters (eg, the US operation frequencies). The adjuster then selects the image or label associated with the lowest uncertainty measure (or with uncertainty measure less than as threshold), and stores the related frequency setting for follow-up image acquisitions. New<!-- EPO <DP n="4"> --> predictions and their associated uncertainties are then made based on imagery acquired with the so stored imaging parameter. If the associated uncertainty is below a set threshold, the frequency settings is maintained. If not, a new scan of the frequency range is performed to find the new best minimal uncertainty and the associated imaging parameter setting which is then again stored for the follow-up acquisition, and so on.</p>
<p id="p0023" num="0023">In another embodiment, the predictor component initializes the system by making an initial prediction based on a reference parameter, such as mid-range operation frequency or other reference frequency. The parameter adjuster may be configured to compute the gradient of the label uncertainty with respect to the frequency input, and by moving along the gradient to adjust the frequency so as to decrease the prediction uncertainty.</p>
<p id="p0024" num="0024">In other embodiments, the predictor component first predicts the presence of a particular label, the parameter adjuster adjusts the imaging parameter (eg, the transducer frequency) to a better, or even optimal one for those labels as known from prior knowledge. The prior knowledge may be represented in a pre-set data structure such as a look-up table other where prediction labels are stored in association with respective best frequencies. The prior knowledge data structure can be dynamically built up by the system itself in a preparatory or exploratory phase where scan is performed by acquiring a series of images whilst changing the imaging parameter in a range.</p>
<p id="p0025" num="0025">In embodiments, the uncertainty value is provided by a user via a user interface. Specifically, the parameter adjustment is based not only on the predictor component output, but is further based on a human expert interaction signal ("human in the loop"), to iteratively steer the acquisition process based on human expert knowledge.</p>
<p id="p0026" num="0026">In embodiments, the predictor component includes a pre-trained machine learning model.</p>
<p id="p0027" num="0027">In embodiments, the pre-trained machine learning model includes a neural network. The neural network may include one or more hidden layers, in particular one or more convolutional layers.</p>
<p id="p0028" num="0028">In embodiments, the image acquisition unit includes a multi-frequency ultrasound imaging device, wherein the imaging signal is an ultrasound signal. However, other imaging modalities, preferably those based on non-ionizing imaging signals are also envisaged herein, such as optical coherence tomography or others still.</p>
<p id="p0029" num="0029">In embodiments, the acquisition parameter includes a frequency of the ultrasound signal. Other imaging parameters of an US imaging system that are determinative<!-- EPO <DP n="5"> --> of the US-signal are also included herein, in addition or instead of the US frequency. When other imaging systems other US are used, the imaging parameter relates to other quantities that are adjustable and determinative of properties of the imaging signal, and hence of IQ and/or imageable properties of interest of the respective object.</p>
<p id="p0030" num="0030">In embodiments, the ultrasound imaging device is an intravascular ultrasound, IVUS, imaging device</p>
<p id="p0031" num="0031">According to another aspect of the invention there is provided a training system configured to train the predictor component.</p>
<p id="p0032" num="0032">According to another aspect of the invention there is provided an imaging method, comprising the steps of:
<ul id="ul0002" list-style="none" compact="compact">
<li>acquiring image data of at least a part of an object, the said acquiring being based on an imaging signal emittable by an image acquisition unit to interact with the object, the image acquisition unit adjustable to operate at different acquisition parameters that determine at least in part a property of the imaging signal;</li>
<li>predicting, based at least on the acquired image data, one or more properties of the object; and</li>
<li>adjusting, based on the predicted one or more properties, the acquisition parameter at which the image acquisition unit is to acquire follow-up image data.</li>
</ul></p>
<p id="p0033" num="0033">In an optional step of the method, the predicted labels and/or findings are visually localized in the current image by graphical elements. The graphical elements may indicate tissue boundaries etc. The graphical elements may be displayed on a display device concurrently with the current image. For example, the graphical element(s) may be overlaid on the currently displayed image.</p>
<p id="p0034" num="0034">According to another aspect of the invention there is provided a training method configured to train the predictor component.</p>
<p id="p0035" num="0035">In another aspect there is provided a computer program element, which, when being executed by at least one processing unit, is adapted to cause the processing unit to perform the method as per any one of the above mentioned embodiments.</p>
<p id="p0036" num="0036">In another aspect still, there is provided a computer readable medium having stored thereon the program element.</p>
<p id="p0037" num="0037"><i>"user"</i> relates to a person, such as medical personnel or other, operating the imaging system or overseeing the imaging procedure. In other words, the user is in general not the patient.<!-- EPO <DP n="6"> --></p>
<p id="p0038" num="0038"><i>"object"</i> is used herein in the general sense to include animate "objects" such as a human or animal patient, or anatomic parts thereof but may also include inanimate objects such as an item of baggage in security checks or a product in non-destructive testing. However, the proposed system will be discussed herein with main reference to the medical field, so we will be referring to the "object" as "the patient".</p>
<p id="p0039" num="0039"><i>"ID, "2D",</i> or "3D" or <i>"nD",</i> refers to dimension <i>n, n=1,2,3,</i> .., so <i>"ID"</i> means "one-dimension(al), "2D" means "two-dimension(al)", and so on.</p>
<p id="p0040" num="0040"><i>"US"</i> is shorthand for ultrasound, the imaging modality mainly (but not exclusively) envisaged herein.</p>
<p id="p0041" num="0041"><i>"GPU"</i> relates to processors with architectures that support parallel processing such as graphics processing units or others.</p>
<p id="p0042" num="0042">In general, the <i>"machine learning component"</i> is a computerized arrangement that implements a machine learning ("ML") algorithm that is configured to perform a task. In an ML algorithm, task performance improves measurably after having provided the arrangement with more training data. The task's performance may be measured by objective tests when feeding the system with test data. The task's performance may be defined in terms of a certain error rate to be achieved for the given test data. See for example, <nplcit id="ncit0001" npl-type="b"><text>T. M. Mitchell, "Machine Learning", page 2, section 1.1, McGraw-Hill, 1997</text></nplcit>.</p>
<heading id="h0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p0043" num="0043">Exemplary embodiments of the invention will now be described with reference to the following drawings, which, unless stated otherwise, are not to scale, wherein:
<ul id="ul0003" list-style="none" compact="compact">
<li><figref idref="f0001">Figure 1</figref> shows a schematic block diagram of an imaging system;</li>
<li><figref idref="f0002">Figure 2</figref> shows a schematic block diagram of a processing components of the imaging system;</li>
<li><figref idref="f0003">Figure 3</figref> shows a schematic block diagram of a processing component according to one embodiment;</li>
<li><figref idref="f0004">Figure 4</figref> shows a schematic block diagram of a processing component according to a second embodiment;</li>
<li><figref idref="f0005">Figure 5</figref> shows a schematic block diagram of a processing component according to a third embodiment;</li>
<li><figref idref="f0006">Figure 6</figref> shows a flow chart of an imaging method;</li>
<li><figref idref="f0007">Figure 7A</figref> shows a schematic block diagram of a machine learning model;<!-- EPO <DP n="7"> --></li>
<li><figref idref="f0007">Figure 7B</figref> shows a schematic block diagram of a training system for training a machine learning model; and</li>
<li><figref idref="f0008">Figure 8</figref> shows a flow chart of a method of training a machine learning model.</li>
</ul></p>
<heading id="h0005">DETAILED DESCRIPTION OF EMBODIMENTS</heading>
<p id="p0044" num="0044">With reference to <figref idref="f0001">Figure 1</figref> there is shown a schematic block diagram of an imaging system IS.</p>
<p id="p0045" num="0045">The imaging system IS may be configured for medical imaging, but other application outside the medical field are not excluded herein.</p>
<p id="p0046" num="0046">Broadly, the imaging system IS includes an image acquisition unit AQ. The image acquisition unit AQ is operable to emit an imaging or "interrogating" signal S. The imaging signal S is configured to "interrogate" an object OB or certain of its one or more properties of interest. Specifically, the imaging signal S is to interact with an object of interest OB such as a patient or an anatomic part thereof. The imaging signal S interacts with patient tissue OB. The interrogation signal S is modified as a result of this interaction, and a modified signal S' is then received at the image acquisition unit AQ.</p>
<p id="p0047" num="0047">The incoming, modified signal S' as received at the acquisition unit AQ is digitized by a converter C into numerical data. The numerical data can be further processed or converted into image date. The image data can be stored on a database DB, visualized on a display unit MT, or can be otherwise processed.</p>
<p id="p0048" num="0048">The acquisition unit AQ is preferably operable to emit a non-ionizing imaging signal S. The image acquisition unit AQ and the image processor PR may be integrated into a single housing (not shown), but this is not necessarily the case in all embodiments. The processing unit PR may be situated outside the housing, away and remote from the acquisition unit AQ, and the two may communicate through a wireless or a wired communication arrangement. In addition, the display device or monitor MT on which the acquired imagery may be displayed, may be integrated into the housing, together with the acquisition unit and/or the processing unit PR. But again, such an integration of components is not necessarily the case in all embodiments. Specifically, the monitor MT may instead be situated outside the housing, and the acquired imagery is forwarded to the monitor MT for display through a wired or wireless connection. The acquisition unit AQ may be operable to acquire a stream of images which can then be displayed as a video feed on the monitor MT.</p>
<p id="p0049" num="0049">In embodiments mainly envisaged herein the image system is arranged as an ultrasound imaging system. However, other non-ionizing-signal-based imaging systems IS<!-- EPO <DP n="8"> --> are also envisaged, such as optical coherence tomography OCT, in particular, but not only, intra-vascular IVOCT. The non-ionizing imaging system preferably envisaged herein are capable of acquiring real time imagery.</p>
<p id="p0050" num="0050">A particular field of application envisaged herein in some embodiments is intravascular ultrasound, IVUS, but this does not to exclude other ultrasound imaging systems such as externally applied systems. The imaging system may be cart-based, portable, hand-held or, as said, endoscopic and/or intravascular.</p>
<p id="p0051" num="0051">In ultrasound systems, but also in other imaging systems, the acquisition unit AQ includes a transducer TR. It is through the transducer TR that the US signal S is emitted and received as the modified signal S'. Transducer T us coupled to transmitter T and receiver R circuitry of the acquisition unit AQ. The transmit and receive circuitry T,R may be integrated into a single sub-unit, a transceiver circuitry, or may be arranged separately. The transmit and receive circuitry operate as switches in turns. The transmit circuitry T switches the transducer TR in a mode to emit the US signal S. The receive circuitry R switches the transducer in a mode to receive the modified (eg, reflected) signal S'. Whilst transducer TR can act to receive and transmit signals, this is not a necessary requirement herein for all embodiments. Alternatively, the outgoing signal S may be generated by one component, whilst the incoming signal is received by another (detector) component. This in particular the case for other imaging modalities, such as OCT or other still, but, although less preferred, may also be practiced in US imaging.</p>
<p id="p0052" num="0052">In US imaging, the transducer TR includes preferably plural active sources or transducer elements (not shown) that are each individually addressable to cause respective ultrasonic pressure waves at specific frequencies. Plural such sources may be arranged in a line or in a 2D array. The array or linear arrangement may be curved. Phased arrays are also envisaged.</p>
<p id="p0053" num="0053">Imaging characteristics such as image quality, in particular image resolution, noise level, size of field-of-view (FoV), etc. are a function of certain imaging acquisition parameters (referred to herein briefly as acquisition parameters) that are set automatically or by the user.</p>
<p id="p0054" num="0054">In ultrasound imaging as mainly envisaged herein, one such acquisition parameter is the ultrasound frequency. The ultrasound frequency describes the frequency with which the transducer element(s) is/are vibrating to cause the pressure waves that are coupled into the patient as imaging signals S to image the internals of the patient. The pressure waves at the given frequencies pass through patient tissue and are reflected off<!-- EPO <DP n="9"> --> boundaries of different tissue types. A fraction of the incoming ultrasound wave S is absorbed whilst another fraction is scattered off or reflected S' and is then detected at the transducer of the acquisition unit AQ. Based on a time difference between emission of outgoing sonic wave S and reception of the back-reflected sonic wave S', a distance can be computed for different positions to so build up the image.</p>
<p id="p0055" num="0055">Whilst the transducer TR is mainly envisaged as an array comprised of a plural active sources, this is not necessarily the case in all embodiments and "mono"-source transducer with a single transducer element are also envisaged. Preferably however, a multi-source transducer is envisaged. The transducer elements may be arranged in PMUT or CEMUT technologies, or other.</p>
<p id="p0056" num="0056">Preferably, the imaging system is capable of multi-frequency operation. More particularly, the operating ultrasound frequency as one of the acquisition parameters can be changed either by the user or, preferably automatically and dynamically, as will be described more fully below. The ultrasound (US) frequency range envisaged herein depends on the application and equipment but will typical lie in the MHz-range, such as 1-40 MHz. For example, a typical US range for IVUS is about 10-40 MHz.</p>
<p id="p0057" num="0057">In embodiments, the acquisition parameter, such as the US frequency, can be dynamically changed in a feedback loop, in dependence on the acquired US imagery, to be explained in more fully below. Because the main focus herein is on ultrasound imaging the acquisition parameter may also be simply referred to herein as "the frequency", with the understanding however that all that has been said above, and all that will be explored below, is of equal application to imaging parameters other than US frequency.</p>
<p id="p0058" num="0058">The US frequency may describe a global frequency that describes the frequency of a combined ultrasound wave that is created by interference of individual "wavelets" generated by the individual transducer elements or sources. In addition or instead, the "frequency" is a matrix or vector quantity of local frequencies that describes, respectively, individual frequencies of the local waves, or wavelets, generated by the individual transducer elements in a phased, linear or curved array. In the notation <i>f</i>=(f<i>i</i>), <i>i</i> are the transducer element positions <i>i</i> in the array of the transducer elements TR. The transducer element position <i>i</i> is associated with a respective pixel positon in the image. As the array may be ID or 2D, the space index <i>i</i> is either ID (a scalar) or 2D (<i>i=i<sub>1</sub>,i<sub>2</sub></i>)). In others words, frequency is a space dependent variable, depending on source and hence pixel position. In the following, whenever reference is made to "<i>f</i>" this notion is understood to include the case where <i>f</i> is a vector or matrix.<!-- EPO <DP n="10"> --></p>
<p id="p0059" num="0059">As explained earlier, the image characteristics are a function of the frequency <i>f</i> and there are usually trade-offs between different imaging tasks and objectives as observed earlier. The imaging system IS as proposed herein is configured to dynamically adapt. preferably in real time, in between individual image acquisitions of in imaging stream, the frequency for a given image task or objective as deduced by the ogic of the processing unit PR, based on the "semantics" of what is being imaged at a given instant. This semantics based processing makes the frequency adaptation more robust, as compared to approaches of adaption based on noisy raw image information as such, without such a "semantic layer". This semantic aspect will be explored more fully below.</p>
<p id="p0060" num="0060">Preferably, the ultrasound imaging system IS operates in a dynamic feed-back loop where a current image is analyzed by the processing unit PR and, based on this analysis, the frequency is adapted accordingly during acquisition of the stream of images. In particular, the frequency is adapted based on a given image in the stream, and the adapted frequency may then be used for the follow-up image and so on. If the analysis by the processing unit PR so reveals, the frequency may be kept constant as long as certain imaging objectives are met which is supervised by the processing unit PR. As will be explained more fully, the processing unit may be based particularly on a machine learning algorithm. In other words, the processing unit PR may incorporate a pre-trained machine learning model such as an artificial neural network or other that has been previously trained on training data. In addition to analyzing merely a current image, the processor may in addition base its analysis on the current frequency used for the current image to adjust the follow-up frequency for the next image. Whilst the imaging system as described is mainly envisaged to operate in a dynamic feed-back loop over the image stream, a user interface UI may still be provided to allow the human operator to "remain in the loop". The user can thus provide feedback-input, such as rating, on the image quality for instance, and this input is then taken into account by the processing unit PR to compute the new frequency which is then used to control the transducer to acquire the next follow-up image.</p>
<p id="p0061" num="0061">In general, the processing by the image processer PR proceeds for each image transducer element <i>i</i> (scalar or 2D) separately to compute for each transducer element <i>i</i> the associated frequency at which the respective transducer element <i>i</i> is to be driven.</p>
<p id="p0062" num="0062">Reference is now made to the block diagram in <figref idref="f0002">Figure 2</figref>, which shows more details of the processing unit PR. For clarity, and to better explain interaction, parts of the acquisition unit AQ are also shown in <figref idref="f0002">Figure 2</figref>. In particular, as shown on the right of <figref idref="f0002">Figure 2</figref>, the acquisition module AQ includes the transducer TR. The frequency or frequencies of<!-- EPO <DP n="11"> --> the transducer is controlled by a controller CON communicatively coupled to the transducer TR. The ultrasound controller CON energizes the transducer accordingly to generate the ultrasound signal at the prescribed frequency. The frequency is prescribed by a frequency controller FC and this is adjustable based on output received from the processing unit PR which is shown in the center potion of <figref idref="f0002">Figure 2</figref>.</p>
<p id="p0063" num="0063">In operation, images are acquired using a tunable transducer frequency via the acquisition module AQ. The acquired imagery is then passed on to prediction module PC. The prediction module PC may include a suitable trained machine learning ("ML") component, such as a pre-trained artificial neural network (NN). Other machine learning models or methodology is not excluded herein. The prediction module PC analyses the acquired imagery and generates predictions or labels ℓ<sub>1</sub>, ℓ<sub>2</sub> which may be visualized by an optional visualization module VM together with the acquired imagery on the display unit MT. Graphical overlays may be used to visualize ℓ<sub>1</sub>, ℓ<sub>2</sub> as indicated schematically in <figref idref="f0002">Figure 2</figref> by circles in dashed and solid circles. The labels can be of all kinds as they can for example be generated from classification, segmentation or tracking algorithms based on the acquired imagery.</p>
<p id="p0064" num="0064">A feed-back parameter adjuster module PA receives and analyzes the imagery and/or the labels and may then propose a new frequency f' which is then passed on to the frequency controller. The frequency controller FC re-adjust the imaging frequency in real time accordingly to drive the transducer elements to produce US waves at the said new frequency f' to acquire the next image(s), and so on. Preferably, the operation of the feed-back module PA is based on the predicted labels, for instance segmentations of image structures in the received current image. In embodiments, the predictor PC attaches uncertainty values (or score or measures) to the predicted labels to quantify the prediction quality of the predicted labels. Preferably, the feed-back module PA uses the uncertainty scores produced by the prediction module PC to optimize the transducer frequency <i>f</i> for better and more certain predictions. The feed-back module PA preferably adjusts the new frequency so as to reduce or minimize uncertainty values.</p>
<p id="p0065" num="0065">The prediction operation includes in particular computing of the labels ℓ<sub>1</sub>, ℓ<sub>2</sub><i>.</i> The predictions may comprise a global label(s) for the whole image that represents for instance the overall anatomy. In extreme cases a single label for the whole images is predicted. Alternatively, or in addition, the prediction may include a plurality of labels ℓ<sub>1</sub>, ℓ<sub>2</sub> for some or each image, where the labels relate to certain pixel/voxel sub-sets of the image. Single elements sub-sets are also envisaged. In other words, a labeling (such as a<!-- EPO <DP n="12"> --> classification), with granularity down to individual pixels or voxel level is envisaged so that each pixel receives its own label. In coarser graining, the labeling may refer to sub-sets of more than one pixel/voxel. The prediction may hence be coarser or finer grained, and the granularity may be user adjustable or is pre-set. There may exists a plurality of pre-sets for various predictions tasks envisaged in this context.</p>
<p id="p0066" num="0066">Operation of the proposed processor PR is based on at least two functional relationships, concatenated with each other S ° M: First there is a predictor function: <maths id="math0001" num="(1)"><math display="block"><mrow><mi>P</mi><mo>:</mo><mi mathvariant="normal">I</mi><mo>−</mo><mo>&gt;</mo><mo>ℓ</mo></mrow></math><img id="ib0001" file="imgb0001.tif" wi="97" he="6" img-content="math" img-format="tif"/></maths> preferably implementable as a machine learning algorithm, that maps imagery to labels. The labels in label space represents the semantics referred to above. The label indicates what each pixel, region, or whole image means in a clinical sense, which anatomy or part thereof is represented, what finding is indicated, etc, and similar for applications in other fields where different semantics may apply. The predictor Function P is implemented by predictor component PC.</p>
<p id="p0067" num="0067">Second, there is a selector function: <maths id="math0002" num="(2)"><math display="block"><mrow><mi>S</mi><mo>:</mo><mo>ℓ</mo><mo>−</mo><mo>&gt;</mo><mi>f</mi></mrow></math><img id="ib0002" file="imgb0002.tif" wi="97" he="6" img-content="math" img-format="tif"/></maths> implemented by the parameter adjuster PA. The selector maps labels onto the associated imaging parameter, in particular the US operating frequency <i>f</i> mainly envisaged herein.</p>
<p id="p0068" num="0068">As the acquisition unit acquires a stream of imagery ("frames") in time and because each frequency is associated with a respective transducer source and hence pixel position, the selector function is in embodiments a space and time function S: (ℓ<i>,i,t) -&gt;</i> (<i>f,i,t</i>)<i>,</i> with <i>i</i> indicating, as introduced above, pixel/transducer element positon, and <i>t</i> indicative of acquisition time.</p>
<p id="p0069" num="0069">In embodiments, the labels ℓ as supplied by the predictor PC are associated herein with respective predictor uncertainties, denoted herein as "σ", and explained in more detail below at <figref idref="f0006">Figure 6</figref>. This uncertainty may be written as σ<sub>ℓ</sub>, the uncertainty associated with the respective label. In such embodiments, the selector function S may be configured as a function of uncertainty versus imaging parameter:<!-- EPO <DP n="13"> --> <maths id="math0003" num="(3)"><math display="block"><mrow><mi mathvariant="normal">S</mi><mo>:</mo><mfenced separators=""><mo>ℓ</mo><mo>,</mo><mi>i</mi><mo>,</mo><mi>t</mi></mfenced><mo>−</mo><mo>&gt;</mo><mfenced separators=",,"><mi>σ</mi><mi>i</mi><mi>t</mi></mfenced><mo>−</mo><mo>&gt;</mo><mfenced separators=",,"><mi>f</mi><mi>i</mi><mi>t</mi></mfenced></mrow></math><img id="ib0003" file="imgb0003.tif" wi="110" he="6" img-content="math" img-format="tif"/></maths></p>
<p id="p0070" num="0070">The selector function S may represent prior knowledge, and may be implemented as a look-up-data structure, for instance a look-up table (LUT), as interpolation in a LUT, as an analytic expression, or may be learned from a second machine learning model. The uncertainty values may be produced by the machine learning or other algorithm implemented by the predictor component PC when predicting the labels. Alternatively, the uncertainty values are produced by post-processing of the output produced by the predictor component. The notation "ℓ" will be used herein to refer top output of the predictor PC and is to be construed broadly as any identifier of any of the above mentioned types of predictions envisaged herein.</p>
<p id="p0071" num="0071">The selector function S is in instances implicitly known, as the predictor function M may be understood as function of the imaging parameter. This is because the input image is itself a function of the imaging parameter, eg US frequency.</p>
<p id="p0072" num="0072">In embodiments the system PR may be configured to dynamically built up the LUT during acquisition, by storing all labels and/or uncertainties of some, preferably all, previously used frequencies. This LUT then represents prior knowledge about the best-frequency setting for a given label or detection task. Other implementations are also envisaged and will be discussed more fully below at <figref idref="f0003 f0004 f0005">Figures 3-5</figref>.</p>
<p id="p0073" num="0073">Preferably, the selector function S is smooth over σ, in particular is differentiable, and may also be smooth in space and/or time. The collection of all possible uncertainties values may be referred to herein as the "uncertainty space".</p>
<p id="p0074" num="0074">Explaining the operation of the imaging system IS in more detail, it is proposed to implement a predictive algorithm in particular for improved multi-frequency IVUS operation and label prediction. Preferably, the algorithm is based on a differentiable selector function S for improved multi-frequency IVUS operation and prediction. Broadly, the system takes acquired IVUS images (or image stacks) and optionally the current US frequency as inputs and returns predicted labels for the whole image (e.g. image classification) or on the single-pixel level (e.g. segmentation) as outputs. The label prediction is further complemented by providing a predictive uncertainty, following, e.g., a Bayesian uncertainty approach or other. By optimizing for low uncertainty predictions, the system solves the task of selecting an improved (e.g. the best) frequency setting per current image and/or prediction task, and dynamically tracking and/or automatically adapting the optimal operation frequency over time/space. Tracking allows dynamically building up the above<!-- EPO <DP n="14"> --> mentioned prior knowledge in form of a LUT or other memory structure. It is assumed herein that the optimal frequency is a smooth function over space and time, which seems valid in most applications due to expected spatial correlations of images proximal in space and/or time.</p>
<p id="p0075" num="0075">Referring now to <figref idref="f0003 f0004 f0005">Figures 3-5</figref>, these show respective block diagrams of different embodiments and aspects of the imaging system's logic, in particular the inter-play between the prediction module PC and feed-back parameter adjuster PA.</p>
<p id="p0076" num="0076">In more detail, <figref idref="f0003">Figure 3</figref> shows an embodiment that uses a "low-weight" pre-prediction model to detect global labels for an initial image(s). Based on prior knowledge, implemented for example in a LUT or similar data structure, the best frequency for some or each label is identified. New images may then be acquired, using the so determined frequency setting f. The prediction module PC may then in turn, thanks to a better IQ, predict improved (now possibly more detailed and better localized) predictions for each image.</p>
<p id="p0077" num="0077">In more detail, a mid-range frequency <b><i><o ostyle="single">f</o></i></b> may be used to generate the initial IVUS image <b><i><o ostyle="single">I</o></i></b> in an initial phase (<b>t</b>=<b>0</b>). From this input, a pre-prediction module PC' provides a set of predictions ℓ<i><sub>i</sub></i> which are stored together with their optimal frequencies in a look up table (LUT) to gradually build up prior clinical knowledge. This information is then exploited in a second phase (<b><i>t</i></b>=<b>1</b>) to tune the frequency, leading to an updated image <b><i>I<sub>f</sub></i></b> with enhanced predictions by operation of predictor PC. Alternatively, the LUT, where the labels are stored versus best frequencies, is pre-defined by a human expert.</p>
<p id="p0078" num="0078">Whilst <figref idref="f0003">Figure 3</figref> shows the predictor PC separate from pre-predictor PC' as two components, and whilst this is indeed so envisaged in embodiments, such as separation into two components is not a requirement and pre-predictor PC' is merely a representation of predictor PC operating in preparatory phase in a different mode to build up the LUT. In alternative embodiments, there is neither such a pre-prediction module PC' nor such a mode, and the LUT is assembled in other ways. The pre-predictor PC', or the pre-predictor mode, is low-weight in terms of expected computing power as the demands are lower compared to the predictor or predictor mode PC. For instance, in this embodiment the pre-predictor PC' is merely tasked with building up the LUT as opposed to the higher computational demands on the predictor PC in predicting the imaging parameter <i>f</i>.</p>
<p id="p0079" num="0079">Referring now to <figref idref="f0004">Figure 4</figref> this shows a different embodiment where the predictions for the frequencies <i>f</i> are computed based on a pre-specified discrete set of operation frequencies. Then the image is selected S, with the lowest uncertainty measure. The frequency associated with this uncertainty is then used to acquire the next images. The<!-- EPO <DP n="15"> --> transducer frequency is preferably only updated if the prediction uncertainty σ for this frequency exceeds a pre-defined threshold value. In this case, some or all frequencies are revisited as described for the initial step, and the frequency may be adjusted as before. The system PR proceeds over some or all follow-up images in this iterative manner.</p>
<p id="p0080" num="0080">In more detail, in an initial iteration (<b><i>t</i></b> = <b>0</b>), multiple images <b><i>I<sub>fi</sub></i></b> are acquired in an exploratory phase by scanning over a range of available frequencies <b><i>f<sub>i</sub></i></b> using initial or test images. For all or some of these test images (now associated with frequencies <b><i>f<sub>i</sub></i></b>), the prediction module PC outputs the predictions <b>ℓ<i><sub>i</sub></i></b> and preferably their associated uncertainties <b>σ<i><sub>i</sub></i></b>. The parameter adjuster PA, implementable in embodiments as a multiplexing element (MUX), selects S the label ℓ = <b><i>p<sub>s</sub></i></b> with lowest uncertainty or an uncertainty below a threshold, and stores the associated frequency setting <b><i>f<sub>s</sub></i></b> for the next iteration (<b><i>t</i></b> = <b>1</b>). A new image <b><i>I<sub>fs</sub></i></b> may then be acquired with <b><i>f<sub>s</sub></i></b>, and the new image is then passed on to the prediction module PC. If the corresponding prediction uncertainty is below a threshold <b><i>T</i></b>, the prediction <b>ℓ<i><sub>i</sub></i></b> is kept. Otherwise, a new scan over the frequency range is performed to find the new best minimal uncertainty setting and this is then stored for the following iteration.</p>
<p id="p0081" num="0081">Whilst the embodiments in <figref idref="f0003">Figures 3</figref>,<figref idref="f0004">4</figref> may require operation in respective preparatory phases, this can be implemented with very little delay in terms of user experience by using fast processors as such as GPUs or others.</p>
<p id="p0082" num="0082">Referring now to <figref idref="f0005">Figure 5</figref>, this shows an embodiment where an initial prediction label, associated with an uncertainty, is predicted, by using an image acquired at an initial mid-range US frequency.</p>
<p id="p0083" num="0083">A gradient (indicated as "nabla" ∇ in <figref idref="f0005">Figure 5</figref>) is then computed (by the adjuster PA or by another computational entity) for selector function S with respect to the frequency. As mentioned above, function S is implicitly known because the image is a function of the imaging parameter, in this case frequency <i>f</i>. The frequency is then adjusted by moving at a pre-defined or adaptively adjusted step (width) along the gradient direction in uncertainty space to a point of lower prediction uncertainty. This new frequency may then be used to acquire a new image, with better prediction performance. For the following images, this gradient based <i>f</i>-adjustment is repeated. In embodiments, the previously found location along in uncertainty space (optimal frequency for the previous slice) may then be used as the initial starting point for the current iteration. The previously found location represents the optimal frequency for the previous image.<!-- EPO <DP n="16"> --></p>
<p id="p0084" num="0084">This gradient based method can be applied for continuously tunable transducers, preferably after clipping to appropriate frequency boundary conditions. The gradient-based scheme may also be applied to discretely tunable transducers. For discretely tunable transducers, the improved frequency is then selected from the discrete set of frequencies as the frequency closest to the location in uncertainty space as found in the above described gradient step.</p>
<p id="p0085" num="0085">In one embodiment of <figref idref="f0005">Figure 5</figref>, both, the current image <b><i>I<sub>fj</sub></i></b> and the current frequency setting <b><i>f<sub>j</sub></i></b> are fed into the prediction module PC and used to compute the gradient of the selector function S with respect to frequency. The error is fed back to the frequency adjuster PA. Adjuster PA then adjusts the current frequency based in the gradient. The adjuster then instructs frequency controller FC to drive the transducer at the adjusted frequency, thus reducing prediction uncertainty.</p>
<p id="p0086" num="0086">Using the current image along with the current frequency may also be used in the embodiments of <figref idref="f0003">Figures 3</figref>, <figref idref="f0004">4</figref>.</p>
<p id="p0087" num="0087">In an alternative to the embodiments in <figref idref="f0003 f0004 f0005">Figures 3-5</figref>, or in addition, user input on the predictions as provided by the prediction module can be provided through a user interface UI. In other words, as an alternative to relying on the prediction uncertainty measure for optimizing the frequency, one can rely on feedback of a human operator in the loop. By interactively providing labels for certain anatomies, one can use the predicted label accuracy as an optimization criterion. In one embodiment, merely providing qualitative, binary feedback such as <i>"separation</i>/<i>segmentation or (overall) image quality of structure got better</i>/<i>worse</i> / <i>or current frame got better</i>/<i>worse"</i> can help to optimize for the frequency.</p>
<p id="p0088" num="0088">The above described embodiments in <figref idref="f0003 f0004 f0005">Figures 3-5</figref>, including the user feedback assisted option, provide a scheme that will dynamically shift the optimal operation frequency of the US-imaging system, and hence implements a frequency "auto-focus" for optimal prediction performance.</p>
<p id="p0089" num="0089">It will be understood that the predictor component PC and the parameter adjuster PA are arranged as two distinct functional components as shown in <figref idref="f0002 f0003 f0004 f0005">Figures 2-5</figref>. Alternatively, the two functionalities are merged and/or integrated into a single functional component.</p>
<p id="p0090" num="0090">Reference is now made to <figref idref="f0006">Figure 6</figref> that shows a flow chart of an imaging method that may underlie operation of the above described embodiments of the imaging system IS. However, it will be understood that the following described steps of the imaging<!-- EPO <DP n="17"> --> method are not necessarily tied to the architecture given in <figref idref="f0001 f0002 f0003 f0004 f0005">Figures 1-5</figref>, and may be understood as a teaching in their own right.</p>
<p id="p0091" num="0091">At step S610 an image at first instant <i>t</i>= 0 is acquired at a first acquisition parameter <i>f</i> by an image acquisition unit of an imaging system. Imaging is based on the acquisition unit emitting an imaging or imaging signal that interacts with the object to be imaged and is then detected after interaction at the acquisition unit, and converted into imagery.</p>
<p id="p0092" num="0092">The acquisition parameter <i>f</i> is adjustable and determines certain characteristics of the imaging signal, and hence is determinative of image properties such as IQ, FoV, etc.</p>
<p id="p0093" num="0093">In embodiments, the image acquisition unit is one of an ultrasound imaging system and the imaging parameter is the US frequency of this multi-frequency ultrasound system. Other imaging modalities are not excluded herein, so long as the imaging signal is preferably non-ionizing.</p>
<p id="p0094" num="0094">In an ultrasound system the imaging or imaging signal is an ultrasound wave. In embodiments a transducer of the image acquisition unit includes multiple active sources, each addressable to emit its own sound wave so as to image different portions of the current field of view in the image acquisition.</p>
<p id="p0095" num="0095">The acquisition parameter includes the frequencies of the respective sound waves to be emitted by each source. The acquisition parameter is hence not necessarily a single parameter, but is a matrix or a vector that describes the respective ultrasound frequency of each of the active sources of the transducer. Not excluded herein, although less preferred, are transducers with a mono-source, in which case the frequency parameter is indeed a single scalar value describing the frequency of the pressure sound wave causable by said single source. The frequency as the acquisition parameter will be referred to herein as "<i>f</i>", no matter whether this is a scalar, vector or matrix quantity.</p>
<p id="p0096" num="0096">The image so acquired at the current frequency, is one of an object such as an internal anatomy or parts thereof of in a human or animal patient. For instance, in IVUS the transducer is attached to an endoscopic or catheter which is introducible into a blood vessel of the patient. Cardiac vessels may be imaged this way. In IVUS embodiments, the imaged object is then a part of the blood vessel, such as a cardiac vessel or other. The image provided by the IVUS image acquisition unit allows the user to obtain image-based information pertaining to the internal structure of the vessel wall for example. One exemplary imaging objective or task may be to ascertain and to distinguish between wall tissue of the vessel and layers of deposits on said wall, such as plaque, calcium, etc. This information may be used to<!-- EPO <DP n="18"> --> assess severity of a stenosis where the vessel lumen is constricted due to the deposits. However, the following and the above is not confined to internally applicable ultrasound but is also applicable to ultrasound applied from the outside of the patient. Such external application may include coupling a lead portion of the transducer TR through a gel or other substance to the patient's body so as to admit the ultrasound wave inside the patient's body, such as is done for example during pregnancy examinations. But again, as mentioned above, imaging modalities other than ultrasound such as OCT or others are also envisaged. The imaging parameter as used herein is then construed accordingly to indicate other relevant image settings, such as the frequency of the light (typically near IR light) used in OCT, etc.</p>
<p id="p0097" num="0097">At step S620, based on the image acquired at the first parameter, one or more properties of one or multiple objects in the current field of view are predicted. The said properties will depend on the imaging task at hand but will include, for instance, identification of the one or more objects found in the current field of view and/or certain findings such stenosis "yes or no?", etc. as deducible from the predicted object properties.</p>
<p id="p0098" num="0098">As envisaged herein, the result of a prediction is a labeling or a segmentation/delineation of recognized image structures. The labels are encodings of the said identity or type of the object found in the image and/or the labels code for findings, etc. task envisaged, etc. For instance, as shown above in <figref idref="f0002">Figure 2</figref>, the labels may distinguish the deposition ℓ2 from the actual wall ℓ1 of the vessel. Respective segmentations may be formed based on the labels.</p>
<p id="p0099" num="0099">The prediction operation S620 is preferably implemented as a machine learning algorithm. In embodiments, a pre-trained artificial neural-network is used as has been mentioned above. Further particulars of a machine learning algorithm as envisaged herein will be explained in more detail below at <figref idref="f0007">Figures 7</figref> and <figref idref="f0008">8</figref>.</p>
<p id="p0100" num="0100">In an optional step, a visual indication such a color or otherwise coded overlay graphic is displayed overlaid on the current image on a display device.</p>
<p id="p0101" num="0101">At step S630 based on the predicted one or more labels, the current imaging parameter such as the frequency <i>f</i> is adjusted, preferably for each source i, to obtain for some or each source <i>i</i>, a respective frequency <i>f<sub>i</sub>.</i></p>
<p id="p0102" num="0102">The newly adjusted imaging parameter <i>f</i> may then be used in step S640 to acquire a follow-up image which is then again received at step S610 and the method proceeds with the new image as described above in a new loop, and so on.</p>
<p id="p0103" num="0103">The method can thus be seen to implement a dynamic feed-back loop that is based on an ongoing adaption of the image acquisition parameter at the chosen frame rate.<!-- EPO <DP n="19"> --></p>
<p id="p0104" num="0104">The adjustments at step S630 furnishes an "auto-focus" ability that dynamically updates the imaging parameter based on the properties of the one or more objects as predicted at prediction step S620. In other words, as the user is changing a direction of the current field of view, the parameters are co-adapted in response to the FoV change, in quasi-real time, as new properties (of possibly new objects) are predicted, which may hence call for the current acquisition parameter <i>f</i> to be updated when further imaging for the now newly predicted object properties.</p>
<p id="p0105" num="0105">The adjustment at step S630 of the imaging parameters is preferably automatic and is done so as to increase an image quality such as resolution and/or to lower noise level. In addition or instead, the image parameter is to increase the FoV, depending on the image task at hand. In one embodiment the imaging task itself is deduced by the predictor component from the predicted labels, and the image acquisition parameter is then adapted accordingly. In other words, a smart imaging device can be provided by the proposed method where the system knows from the predicted properties of the image, what task the user wishes to accomplish and the image acquisition parameters are adjusted accordingly so as to be optimized to the deduced task.</p>
<p id="p0106" num="0106">Some machine learning algorithms as may be used in step S620 furnish one or more uncertainty values that are associated with the respective labels. The uncertainty values measure the amount of uncertainty associated with the prediction of the respective label. Machine learning methods with Bayesian processing are an example where such uncertainty values are co- or post-computed with the predicted labels, and such Bayesian processing is envisaged herein in embodiments. An example of Bayesian processing in ML is described in <nplcit id="ncit0002" npl-type="s"><text>A. Kendall et al in "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?", published in NIPS'17: Proceedings of the 31st International Conference on Neural Information Processing Systems, December 2017, pp 5580-5590</text></nplcit>. Other methods which provide measures for the predictive uncertainty are equally applicable in this context. Here different probabilistic neural network approaches, Bayesian- and non-Bayesian uncertainty estimation schemes as well as stochastic-gradient Monte-Carlo Markov Chain (MCMC) methods or simpler ensemble techniques can be used depending on the envisaged prediction task.</p>
<p id="p0107" num="0107">It is then the objective in one embodiment to adjust the acquisition parameters at step S630 so as to decrease, in instances minimize, the uncertainty value. The uncertainty values σ associated with the respective predictions ℓ are a function (see (3) above) of the frequency <i>f</i>. Instead of minimizing for uncertainties, a "dual" formulation is also envisaged<!-- EPO <DP n="20"> --> herein, wherein the task is to maximize for certainties. Both optimizations are equivalent for present purposes.</p>
<p id="p0108" num="0108">Where the frequencies are drawn from a discrete set, the adjustments at step S630 involves selecting the frequency with the lowest uncertainty, or at least with an uncertainty below a defined threshold. If more than one frequencies satisfy this threshold-condition, a random choice may be made, or the user is offered through the user interface UI, a choice to select from among the qualifying frequencies.</p>
<p id="p0109" num="0109">If the frequency is a continuous function S of the uncertainty σ, a gradient of this function may be computed, and the frequency is adjusted at a certain step width along the gradient so as to decrease the uncertainty.</p>
<p id="p0110" num="0110">In some embodiments, in addition to basing the prediction on the current image, it is also the current frequency that is used to predict S620 the one or more object properties. This further constraint allows making the prediction yet more robust and/or may allow quicker training of the implementing ML model.</p>
<p id="p0111" num="0111">In some embodiments, the adjustment step S630 is not based on a single image, the current image in the stream, but is based on a "stack", or sequence of images, pre-acquired in an exploratory phase and it is this stack of images that is then used in the adjustment step S630. An exemplary embodiment for said stack-based parameter <i>f</i> adjustment S630 is as in the embodiments of <figref idref="f0004">Figure 4</figref>, but other stack-based embodiments are also envisaged. Stack based processing at S630 yet further improves robustness as this allows to account for spatial and temporal correlation in the imaged object.</p>
<p id="p0112" num="0112">The adjustment step at S630 of choosing the best frequency may be based on minimizing the uncertainty-versus-frequency function <i>S</i>. The minimization is preferably done on the fly. The frequencies at S630 may be obtained through prior knowledge by look-up in a look-up table or other memory, where each label is pre-associated with a respective best frequency. One embodiment for how this step may be implemented is as described above in <figref idref="f0003">Figure 3</figref>.</p>
<p id="p0113" num="0113">In the proposed method, the frequency can be computed, in parallel or in series, for some or each active source of the transducer separately, per pixel. Alternatively, at a coarser graining, the frequencies are computed per sub-sets of the image. In an extreme case, instead of operating locally, global embodiments are also envisaged where a global label with associated frequency is computed for the whole of the image.</p>
<p id="p0114" num="0114">As mentioned earlier, the method of concatenating prediction and adjustment as proposed in the method provides for good robustness. Specifically, the proposed prediction<!-- EPO <DP n="21"> --> S620, on which the parameter adjustment S630 is based, forms a semantic layer. The semantic layer represents an "abstraction away" from the particular pixel value pattern of a given image that is prone to noise corruption, thus making the proposed label based frequency adaptation more robust.</p>
<p id="p0115" num="0115">As mentioned earlier, according to one embodiment a machine learning model M used has a neural-network (NN architecture), in particular a convolutional neural-network (CNN) architecture as shown in <figref idref="f0007">Figure 7A</figref>, to which reference is now made.</p>
<p id="p0116" num="0116">Broadly, the NN structure of the machine learning component includes a plurality of nodes, at least partly inter-connected and arranged in different layers. The layers are arranged in one or more sequences. Each node is an entry capable of assuming a value and/or can produce an output based on input it receives from one or more nodes of an earlier layer.</p>
<p id="p0117" num="0117">Each node is associated with a certain function which can be a simple scalar value (node weight) but can also be with more complex linear or non-linear functions. A "connection" between nodes in two different layers means that the node in the later layer can receive an input from the node in the earlier layer. If there is no connection defined between two nodes, no output of one of the two nodes can be received by the other node as input. The node produces its output by applying its function to the input. This can be implemented as a multiplication of the received input by the scalar value (the weight) of the node. The interconnected layered nodes with their weights, layer size etc. forms a NN (neural network) model as one embodiment of the ML model envisaged herein. The model may be stored in a matrix or tensor structure in a memory. Once trained, this structure forms the trained machine learning component which can be held on one or more memories SM.</p>
<p id="p0118" num="0118"><figref idref="f0007">Figure 7A</figref> is an exemplary embodiment of a suitable CNN configuration for inferring the object property labels ℓ<i><sub>k</sub></i> from a current input image <i>I<sub>k</sub></i>, or from a sequence/stack of such images.</p>
<p id="p0119" num="0119">Model M is preferably a deep neural network including one or more hidden layers. The layers L<i>i</i> are hidden layers as there are arranged between an input layer and an output layer. The network M consists of several convolutional filter layers <i>L</i><sub>i</sub>, some or each employing one, or a multitude of, convolutional filter kernels. Some or each of the convolutional filter layers are followed by an activation layer and in some embodiments, a pooling layer (not shown). Optionally, there is also one or both of a batch normalization and dropout layer. Further optionally, there are in addition one or several fully connected layers. The above mentioned series of layers terminates in a classification layer producing the<!-- EPO <DP n="22"> --> prediction of the labels ℓ<i><sub>k</sub></i>. In addition, a probability estimate may be provided for each label. In one embodiments this is achieved by including parametrized probability distributions or densities, and these parameters are estimated alongside the weights of the model M, and may then be used to compute, for example, a respective probability for ℓ<sub>k</sub> that represents the respective uncertainty. In addition or instead, the uncertainty may be represented as a variance or standard deviation. In the <figref idref="f0007">Figure 7A</figref>, "σ" is used to indicate the said uncertainty, but this is merely symbolic and is to include any suitable statistical quantity capable of representing the uncertainty such as, as said, a probability or entropy, higher moments, or any other.</p>
<p id="p0120" num="0120">The activation layers determine the (typically non-linear) function with which output values from one layer are modified and provided as input to the next layer. The pooling layer combines outputs from multiple elements of one layer to serve as input to a single element in the next layer. Combined, each convolutional, activation and pooling layer serves to process data non-linearly, change its dimensionality, and pass it on to the next layer. The parameters of each of the convolutional layers are "learned" (optimized) in the training phase. The number of parameters in each convolutional layer depends on the size of the convolutional kernel, number of kernels, and the step size "stride" when moving over the image processed at a given layer. The number of parameters for fully connected layers is determined by the number of elements in the previous and current layers.</p>
<p id="p0121" num="0121">Optionally, additional input parameters <i>a<sub>k</sub></i> may be incorporated into the model to improve the accuracy and generalization ability of the model. These non-image parameters - usually categorical, sparsely represented data - can be represented by low-dimensional embeddings (e.g. one-hot encoding). For instance, such embeddings can be processed by one or more fully connected layers, and are then point-wisely added or concatenated to an arbitrary intermediate feature map (layer) by tiling the output from the one or more fully connected layer over the spatial dimensions of the feature map. In one embodiment, additional input parameters <i>a<sub>k</sub></i> include the current acquisition parameter <i>f<sub>k</sub>,</i> such as US frequency, for the current image.</p>
<p id="p0122" num="0122">Once trained, during deployment, real data, such as a current image <i>I<sub>k</sub></i> [or <i>(I<sub>k</sub>, f<sub>k</sub></i>)] is applied to an input layer INL. From the input layer INL, the image data <i>I<sub>k</sub></i> propagates through application of the filters represented by the hidden layers <i>L<sub>1</sub></i> - <i>L</i><sub>N</sub>. The number or layers <i>L<sub>1</sub></i> - <i>L</i><sub>N</sub> are two, three or much more in the order of tens (e.g., 20-50 or other). The image data <i>I<sub>k</sub></i> is transformed during the propagation to then emerge as a feature vector OUTL. The non-image contextual data <i>a<sub>k</sub></i> may be propagated through a different strand of the model<!-- EPO <DP n="23"> --> (not shown) including one or more fully connected layers. The outputs of these two strands, the strand <i>L<sub>1</sub></i> - <i>L</i><sub>N</sub> that processes the image data <i>I<sub>k</sub></i> and the strand that processes the (possibly non-image) contextual data <i>a<sub>k</sub></i> may be merged as will be explained later in more detail below with further reference to embeddings.</p>
<p id="p0123" num="0123">The feature vector OUTL could be considered as a low-dimensional ID representation of the input image <i>I<sub>k</sub>.</i> The feature vector OUTL may then be followed by a single, or a plurality of, task-specific layers (not shown) such as fully connected or convolutional layers with an activation function (e.g. any one or more of linear, sigmoid, softmax, tangent hyperbolic) depending on the task. The task is a regression or a classification, preferably the latter. For classification tasks, these feature vectors are encodings of the classes contained in the input image and can be mapped to class probabilities by appropriate normalization / mapping functions (e.g. softmax etc.). For segmentation or other regression tasks, the abstract feature vector is further processed by additional (network) layers. These task specific layer(s) is/are applied to the (encoded) feature vector layer OUTL to predict e.g. the position/contour and class labels of object properties as detected. The predicted labels are represented as output ℓ̂<i><sub>k</sub></i>. The output labels may be represented as a vector, with entries for one type of property, or the classification output is represented as a matrix the size of the input image, each entry representing a label per pixel, eg pixel (<i>i,j</i>) is a plaque pixel, whilst pixel (<i>i,j</i>+<i>1</i>) is a vessel pixel, etc. A smaller matrix feature map may implement a coarser prediction, with labels per regions (subset of pixels). etc. Whilst classification or classification-type tasks are mainly envisaged, regression or regression-type tasks such as predicted measurements in relation to a certain anatomy, eg lumen diameter etc, are also envisaged, instead of, or in addition to, the classification.</p>
<p id="p0124" num="0124">One, more than one, or all hidden Layers <i>L<sub>i</sub></i> are convolutional layers and their operation can be defined in terms of the filters. Convolutional layers as used in model M are to be distinguished from fully connected layers. In convolutional layers, each node is connected only to a sub-set of (pooled) nodes of the previous layer. The node in a follow-up layer is computed by forming a filter operation based on a sub selection of nodes from the previous layer. For different nodes of the follow-up layer, different sub-sets of the previous layer are processed, preferably with the same filter. The filter is a collection of numbers <i>n<sub>ij</sub></i> arrangeable in a matrix. In some embodiments, the filter operation includes forming sums of products <i>n<sub>ij</sub></i> · <i>m<sub>jk</sub>, m<sub>jk</sub></i> being a node of the previous layer. This sum-of-product operation is hence akin to traditional convolutional or filter operation where a filter mask is slid over matrix data. In general, the layers are implemented as matrices. The filter operations that<!-- EPO <DP n="24"> --> implement the propagation of the data through the layers are implemented as matrix operations and are preferably run on GPUs or other processors capable of parallel processing such as those with multi-core designs or others. In fully connected layers, each node of the previous layer is connected to a node in the current layer using a learnable activation weight.</p>
<p id="p0125" num="0125">As mentioned above, fully connected layers are also envisaged herein for processing of non-image data, such as bio-characteristics of the patients. The fully connected layers are envisaged to form a separate strand of processing layers (not shown) in which one, two or more of such fully connected layers are arranged in sequence and through which the non-image data , is passed. Optionally, this data is also passed through a convolutional layer, separate and different from the above described convolutional layers for processing of the image. The output of the last of those fully connected layers for non-imaging data can be concatenated to one of the hidden layers in the CNN branch of the model thereby merging non-image data with image data. In this way the output, that is the labels, not only takes into account the spatial data in the image data but also the contextual data such as the current acquisition parameter.</p>
<p id="p0126" num="0126">As mentioned, embeddings such as one-hot-encodings, can be used to represent categorical sparse data as vectors. The non-image data processing strands may include a combination of fully connected and convolutional layers. Also, the image-data processing strand as shown in <figref idref="f0007">Figure 7A</figref> may include sequences of one or more fully connected layers, in particular when non-image data <i>a<sub>k</sub>,</i> is jointly processed and passed through the layers as augmented data (<i>I</i><sub>k</sub>, <i>a<sub>k</sub>,</i>) In addition to convolutional layers and fully connected layers, either one or both strands may further include at any one or more of pooling layers, activation layers drop-out layers, in any combination. In addition, there may be deconvolutions layers that may be thought to represent an approximation of a reverse operation to the convolutional layers. The number of nodes produced at each layer may change in dimension, so size (number of rows and columns) and/or depth an output layer may grow or shrink. The dimension of the output of a convolutional layer is in general the same as the dimension of the input, or less, depending on its "stride" which represents how many nodes the filter is effectively slid past for each operation. Thus a down-sampling may occur. An up-sampling is also envisaged with deconvolutions layers. Some architectures include one or more runs of both, up-sampling layers followed by down-sampling or the other way around. In general, the architecture of <figref idref="f0007">Figure 7A</figref> is a net down-sampling as the network acts as a mapping that maps the input image <i>I<sub>k</sub></i> down to labels ℓ̂<i><sub>k</sub></i>, if this is vector.<!-- EPO <DP n="25"> --></p>
<p id="p0127" num="0127">Referring now to the training phase or process, this is implementable by a training system TS shown in <figref idref="f0007">Figure 7B</figref>. The training is based on suitable training data. Training data can be obtained by acquiring real imagery or by obtaining simulated US-images, both denoted herein as <i>I</i><sub>k</sub>. Real US-imagery may be retrieved as historical image data from patient records such as PACS or from other image repositories.</p>
<p id="p0128" num="0128">In more detail, the training involves acquisition of real or simulated X-ray images <i>I<sub>k</sub></i> (<i>k</i> ∈ 1...<i>K</i>), paired with "ground truth" labels ℓ<i><sub>k</sub></i> that encode properties in relation to the image one or more objects such as anatomies, etc.</p>
<p id="p0129" num="0129">The pairs {<i>I<sub>k</sub>, ℓ<sub>k</sub></i>} are used in the training process, in which the images <i>I<sub>k</sub></i> are provided as input to a convolutional neural network (CNN), and the network parameters are optimized to infer the prediction vector <i>p<sub>k</sub></i> as output. Optionally, the images can be paired with additional patient-specific parameter vectors (such as height, weight, age, gender, ethnicity, BMI, anatomical abnormalities, implanted devices, etc.) as input. Although one suitable CNN configuration for the present object property label prediction is the one shown in <figref idref="f0007">Figure 7A</figref> other architectures are also envisaged herein, and so are models other than NN such as support vector machines (SVM), decision trees, random forest and others still.</p>
<p id="p0130" num="0130">With continued and more detailed reference to <figref idref="f0007">Figure 7B</figref>, the computerized training system TS as may be used for training the machine learning model M based on training data (<i>X, Y</i>)<i><sup>j</sup>. "X"</i> represents suitable training imagery <i>I<sub>k</sub></i>, whilst "<i>Y</i>" represents the associated label ℓ.</p>
<p id="p0131" num="0131">The training data pair can be obtained in a manual labelling exercise by a human expert. Alternatively, and preferably, the labeling is retrieved in an automated manner. For example, the labels associated with given training image may be assembled from metadata such as DICOM header data or may be retrieved from patient records as held in databases. A suitably programmed scripting tool may be used to accomplish finding the associated object property/properties label(s) Y=<i>ℓ<sub>k</sub></i> for each image training <i>X=I<sub>k</sub>.</i></p>
<p id="p0132" num="0132">The training data is thus organized in pairs <i>j</i> in particular for supervised learning schemes as mainly envisaged herein. However, it should be noted that non-supervised learning schemes are not excluded herein.</p>
<p id="p0133" num="0133">The machine learning model M, such as the shown CNN network in <figref idref="f0004">Figure 4A</figref> or a hybrid network comprising CNN strands for processing image data <i>I</i><sub>k</sub> and fully connected layer strands for processing of non-image data <i>a<sub>k</sub></i> such as associated acquisition parameter <i>f</i> is chosen, and the weights are pre-populated with some initial, possibly random<!-- EPO <DP n="26"> --> or uniform values. For instance, the weights for all nodes may all be populated by "<i>l</i>'s" or other numbers.</p>
<p id="p0134" num="0134">The weights θ of the model M represent a parameterization M<i><sup>θ</sup></i>, and it is the object of the training system TS to optimize and hence adapt the parameters <i>θ</i> based on the training data (<i>X<sup>j</sup>, Y<sup>j</sup></i>) pairs. In other words, the learning can be formalized mathematically as an optimization scheme where a cost function is minimized although the dual formulation of maximizing the utility function <i>F</i> may be used instead.</p>
<p id="p0135" num="0135">Assuming for now the paradigm of a cost function <i>F</i>, this measures the aggregated residue(s), that is, the error incurred between data estimated by the model M and the targets as per some or all of the training data pairs <i>j</i>: <maths id="math0004" num="(4)"><math display="block"><mrow><msub><mi mathvariant="italic">argmin</mi><mi mathvariant="normal">θ</mi></msub><mi>F</mi><mo>=</mo><mstyle displaystyle="false"><mrow><munder><mo>∑</mo><mi>j</mi></munder><mrow><mrow><mo>‖</mo><msup><mi>M</mi><mi mathvariant="normal">θ</mi></msup><mfenced><msup><mi>X</mi><mi>j</mi></msup></mfenced><mo>−</mo><msup><mi>Y</mi><mi>j</mi></msup><mo>‖</mo></mrow></mrow></mrow></mstyle></mrow></math><img id="ib0004" file="imgb0004.tif" wi="110" he="7" img-content="math" img-format="tif"/></maths></p>
<p id="p0136" num="0136">More specifically, the image data <i>X</i> of a training pair is propagated through the initialized network M. X for a first pair "<i>j</i>" is received at an input IN, passed through the model and then received at output OUT as output training data <i>M<sup>θ</sup>(X)</i>. A Suitable measure<br/>
|| · || is used such as a <i>p</i>-norm, squared differences, or other, to measure the difference between the actual output <i>M<sup>θ</sup>(X)</i> and the desired output Y.</p>
<p id="p0137" num="0137">The output training data <i>M(X)</i> is an estimate for the label ℓ given the input training image data X. In general, there is an error between this output <i>M(X)</i> and the associated target <i>Y</i> of the presently considered pair. An optimization scheme such as backward/forward propagation or other gradient based methods may then be used to adapt the parameters θ of the model M<sup>θ</sup> so as to decrease the residue for the considered pair (<i>X, Y)<sup>j</sup></i> or a subset of training pairs from the full training data set.</p>
<p id="p0138" num="0138">After one or more iterations in a first, inner, loop in which the parameters <i>θ</i> of the model are updated based on the optimization scheme used, the training system TS enters a second, an outer, loop where a next training data pair X<sup>j+1</sup>, Y<sup>j+1</sup> is processed accordingly. This inner loop may be implemented by one or more forward and backward passes in the forward/backpropagation algorithm. However, this time while adapting the residue, it is not only the individual residue for the current pair that is adapted but the aggregated, summed, residues of all the training pairs considered up to this point in time are adapted if required to improve the objective function. The aggregated residue can be formed by configuring the objective function <i>f</i> as a squared summed (or other algebraic combination) such as in eq (4) of some or all considered residues for each pair.<!-- EPO <DP n="27"> --></p>
<p id="p0139" num="0139">The generalized training system as shown in <figref idref="f0007">Figure 7B</figref> can be considered for all learning schemes, in particular supervised schemes. Unsupervised learning schemes that perform suitable clusterings in phase space, augmented or not, may also be envisaged. GPUs may be used to implement the training system.</p>
<p id="p0140" num="0140">Referring now the flow chart in <figref idref="f0008">Figure 8</figref>, this shows a computerized method for training a machine learning model MLC, in particular as explained above in <figref idref="f0004">Figures 4A, B</figref>.</p>
<p id="p0141" num="0141">At step S810 training data is received in the form of pairs including US or OCT imagery and associated labels that identify ROIs, findings, or other properties of one or more objects. For present purposes, the label may in addition or instead code for an imaging purpose or task given the image, the said task or purpose being thus a "property" of a particular object, such as anatomy.</p>
<p id="p0142" num="0142">At step S820, the imagery, and optionally an associated acquisition parameter <i>f</i> or other non-image contextual data, is applied to an initialized machine learning model M to produce an output.</p>
<p id="p0143" num="0143">A deviation of the training output from the associated label is quantified by a cost function <i>F</i>. One or more parameters of the model are adapted at step S830 in one or more iterations in an inner loop to improve the cost function. For instance, residues as measured by the cost function are decreased when adapting the model parameters.</p>
<p id="p0144" num="0144">The training method then returns to step S810 where the next pair of training data is fed in to enter an outer loop. In step S820, the aggregated residues of all pairs considered up to that point are decreased, in particular minimized.</p>
<p id="p0145" num="0145">More generally, the parameters of the model M are adjusted to improve an objective function <i>F</i> which is either a cost function or a utility function.</p>
<p id="p0146" num="0146">The components of the image processor PR may be implemented as one or more software modules, run on one or more general-purpose processing units TS such as a workstation associated with the imager IS, or on a server computer associated with a group of imagers.</p>
<p id="p0147" num="0147">Alternatively, some or all components of the image processor PR, in particular the predictor component PA and the parameter adjuster PA, may be arranged in hardware such as a suitably programmed microcontroller or microprocessor, such an FPGA (field-programmable-gate-array) or as a hardwired IC chip, an application specific integrated circuitry (ASIC), integrated into the imaging system IS. In a further embodiment still, the image processor PR may be implemented in both, partly in software and partly in hardware.<!-- EPO <DP n="28"> --></p>
<p id="p0148" num="0148">The different components of the image processor PR may be implemented on a single data processing unit. Alternatively, some or more components are implemented on different processing units, possibly remotely arranged in a distributed architecture and connectable in a suitable communication network such as in a cloud setting or client-server setup, etc.</p>
<p id="p0149" num="0149">One or more features described herein can be configured or implemented as or with circuitry encoded within a computer-readable medium, and/or combinations thereof. Circuitry may include discrete and/or integrated circuitry, a system-on-a-chip (SOC), and combinations thereof, a machine, a computer system, a processor and memory, a computer program.</p>
<p id="p0150" num="0150">In another exemplary embodiment of the present invention, a computer program or a computer program element is provided that is characterized by being adapted to execute the method steps of the method according to one of the preceding embodiments, on an appropriate system.</p>
<p id="p0151" num="0151">The computer program element might therefore be stored on a computer unit, which might also be part of an embodiment of the present invention. This computing unit may be adapted to perform or induce a performing of the steps of the method described above. Moreover, it may be adapted to operate the components of the above-described apparatus. The computing unit can be adapted to operate automatically and/or to execute the orders of a user. A computer program may be loaded into a working memory of a data processor. The data processor may thus be equipped to carry out the method of the invention.</p>
<p id="p0152" num="0152">This exemplary embodiment of the invention covers both, a computer program that right from the beginning uses the invention and a computer program that by means of an up-date turns an existing program into a program that uses the invention.</p>
<p id="p0153" num="0153">Further on, the computer program element might be able to provide all necessary steps to fulfill the procedure of an exemplary embodiment of the method as described above.</p>
<p id="p0154" num="0154">According to a further exemplary embodiment of the present invention, a computer readable medium, such as a CD-ROM, is presented wherein the computer readable medium has a computer program element stored on it which computer program element is described by the preceding section.</p>
<p id="p0155" num="0155">A computer program may be stored and/or distributed on a suitable medium (in particular, but not necessarily, a non-transitory medium), such as an optical storage medium or a solid-state medium supplied together with or as part of other hardware, but may<!-- EPO <DP n="29"> --> also be distributed in other forms, such as via the internet or other wired or wireless telecommunication systems. Transitory storage media are also envisaged in embodiments.</p>
<p id="p0156" num="0156">However, the computer program may also be presented over a network like the World Wide Web and can be downloaded into the working memory of a data processor from such a network. According to a further exemplary embodiment of the present invention, a medium for making a computer program element available for downloading is provided, which computer program element is arranged to perform a method according to one of the previously described embodiments of the invention.</p>
<p id="p0157" num="0157">It has to be noted that embodiments of the invention are described with reference to different subject matters. In particular, some embodiments are described with reference to method type claims whereas other embodiments are described with reference to the device type claims. However, a person skilled in the art will gather from the above and the following description that, unless otherwise notified, in addition to any combination of features belonging to one type of subject matter also any combination between features relating to different subject matters is considered to be disclosed with this application. However, all features can be combined providing synergetic effects that are more than the simple summation of the features.</p>
<p id="p0158" num="0158">While the invention has been illustrated and described in detail in the drawings and foregoing description, such illustration and description are to be considered illustrative or exemplary and not restrictive. The invention is not limited to the disclosed embodiments. Other variations to the disclosed embodiments can be understood and effected by those skilled in the art in practicing a claimed invention, from a study of the drawings, the disclosure, and the dependent claims.</p>
<p id="p0159" num="0159">In the claims, the word "comprising" does not exclude other elements or steps, and the indefinite article "a" or "an" does not exclude a plurality. A single processor or other unit may fulfill the functions of several items re-cited in the claims. The mere fact that certain measures are re-cited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage. Any reference signs in the claims, be they numerals, alphanumerical, or a combination of one or more letters, or a combination of any of the foregoing, should not be construed as limiting the scope.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="30"> -->
<claim id="c-en-0001" num="0001">
<claim-text>An imaging system (IS), comprising:
<claim-text>an image acquisition unit (AQ) for acquisition of image data (11) of at least a part of an object (OB), the said acquisition being based on an imaging signal imitable by the unit (AQ) to interact with the object, the image acquisition unit (AQ) adjustable to operate at different acquisition parameters that determine at least in part a property of the imaging signal;</claim-text>
<claim-text>a predictor component (PC) configured to predict, based at least on the acquired image data (11), one or more properties of the object; and</claim-text>
<claim-text>an acquisition parameter adjuster (PA) configured to adjust, based on the predicted one or more properties, the acquisition parameter at which the image acquisition unit (AQ) is to acquire follow-up image data (12).</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>System of claim 1, wherein the acquisition parameter is adjusted automatically by the acquisition parameter adjuster (PA) so as to increase image quality.</claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>System of claim 1 or 2, wherein the predictor component (PC) is to predict plural object properties associated with a respective uncertainty value, wherein the acquisition parameter adjuster (PA) is to adjust the acquisition parameter based on the uncertainty value or on a gradient thereof.</claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>System of claim 3, wherein the acquisition parameter adjuster (PA) is to adjust the acquisition parameter so as to decrease the uncertainty value of the predicted one or more properties.</claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>System of any one of the previous claims, wherein the predictor component (PC) is to predict the one or more object properties based on a current acquisition parameter.</claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>System of claims 3 to 5, wherein the uncertainty value is provided by a user via a user interface (UI).<!-- EPO <DP n="31"> --></claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>System of any one of the previous claims, wherein the predictor component (PC) includes a pre-trained machine learning model.</claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>System of claim 7, wherein the pre-trained machine learning model includes a neural network.</claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>System of any one of the previous claims, wherein the image acquisition unit (AQ) includes a multi-frequency ultrasound imaging device, wherein the imaging signal is an ultrasound signal.</claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>System of claim 9, wherein the acquisition parameter includes a frequency of the ultrasound signal.</claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>System of claim 9, wherein the ultrasound imaging device is an intravascular ultrasound, IVUS, imaging device.</claim-text></claim>
<claim id="c-en-0012" num="0012">
<claim-text>An imaging method, comprising the steps of:
<claim-text>acquiring (S610) image data (11) of at least a part of an object (OB), the said acquiring being based on an imaging signal imitable by an image acquisition unit (AQ) to interact with the object, the image acquisition unit (AQ) adjustable to operate at different acquisition parameters that determine at least in part a property of the imaging signal;</claim-text>
<claim-text>predicting (S620), based at least on the acquired image data (11), one or more properties of the object; and</claim-text>
<claim-text>adjusting (S630), based on the predicted one or more properties, the acquisition parameter at which the image acquisition unit (AQ) is to acquire follow-up image data (12).</claim-text></claim-text></claim>
<claim id="c-en-0013" num="0013">
<claim-text>A training method configured to train the predictor component (PC) as per claim 7.</claim-text></claim>
<claim id="c-en-0014" num="0014">
<claim-text>A computer program element, which, when being executed by at least one processing unit (PR,TS), is adapted to cause the at least one processing unit (PR,TS) to perform the method as per claim 12 or 13.<!-- EPO <DP n="32"> --></claim-text></claim>
<claim id="c-en-0015" num="0015">
<claim-text>A computer readable medium having stored thereon the program element of claim 14.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="33"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="139" he="142" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="140" he="125" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> -->
<figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="118" he="124" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> -->
<figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="142" he="120" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> -->
<figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="129" he="86" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> -->
<figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="72" he="146" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> -->
<figure id="f0007" num="7A,7B"><img id="if0007" file="imgf0007.tif" wi="142" he="198" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> -->
<figure id="f0008" num="8"><img id="if0008" file="imgf0008.tif" wi="74" he="87" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="157" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="155" he="233" type="tif"/></search-report-data><search-report-data date-produced="20200805" id="srepxml" lang="en" srep-office="EP" srep-type="ep-sr" status="n"><!--
 The search report data in XML is provided for the users' convenience only. It might differ from the search report of the PDF document, which contains the officially published data. The EPO disclaims any liability for incorrect or incomplete data in the XML for search reports.
 -->

<srep-info><file-reference-id>2019P00856EP</file-reference-id><application-reference><document-id><country>EP</country><doc-number>20167460.3</doc-number></document-id></application-reference><applicant-name><name>Koninklijke Philips N.V.</name></applicant-name><srep-established srep-established="yes"/><srep-invention-title title-approval="yes"/><srep-abstract abs-approval="yes"/><srep-figure-to-publish figinfo="by-applicant"><figure-to-publish><fig-number>1</fig-number></figure-to-publish></srep-figure-to-publish><srep-info-admin><srep-office><addressbook><text>MN</text></addressbook></srep-office><date-search-report-mailed><date>20200813</date></date-search-report-mailed></srep-info-admin></srep-info><srep-for-pub><srep-fields-searched><minimum-documentation><classifications-ipcr><classification-ipcr><text>A61B</text></classification-ipcr><classification-ipcr><text>G01S</text></classification-ipcr><classification-ipcr><text>G06K</text></classification-ipcr></classifications-ipcr></minimum-documentation></srep-fields-searched><srep-citations><citation id="sr-cit0001"><patcit dnum="WO2020020770A1" id="sr-pcit0001" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=WO2020020770&amp;CY=ep"><document-id><country>WO</country><doc-number>2020020770</doc-number><kind>A1</kind><name>KONINKLIJKE PHILIPS NV [NL]</name><date>20200130</date></document-id></patcit><category>X</category><rel-claims>1-15</rel-claims><rel-passage><passage>* paragraphs [0001],  [0022],  [0023],  [0028],  [0030],  [0033],  [0034],  [0042],  [0060] *</passage><passage>* figures 1,2 *</passage></rel-passage></citation><citation id="sr-cit0002"><patcit dnum="US8784318B1" id="sr-pcit0002" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US8784318&amp;CY=ep"><document-id><country>US</country><doc-number>8784318</doc-number><kind>B1</kind><name>NAPOLITANO DAVID J [US] ET AL</name><date>20140722</date></document-id></patcit><category>X</category><rel-claims>1</rel-claims><rel-passage><passage>* column 5, line 44 - column 6, line 27 *</passage><passage>* figure 3 *</passage></rel-passage></citation><citation id="sr-cit0003"><patcit dnum="WO2018130370A1" id="sr-pcit0003" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=WO2018130370&amp;CY=ep"><document-id><country>WO</country><doc-number>2018130370</doc-number><kind>A1</kind><name>CONTEXTVISION AB [SE]</name><date>20180719</date></document-id></patcit><category>X</category><rel-claims>1</rel-claims><rel-passage><passage>* page 11, line 24 - page 12, line 26 *</passage><passage>* figure 3 *</passage></rel-passage></citation></srep-citations><srep-admin><examiners><primary-examiner><name>Willig, Hendrik</name></primary-examiner></examiners><srep-office><addressbook><text>Munich</text></addressbook></srep-office><date-search-completed><date>20200805</date></date-search-completed></srep-admin><!--							The annex lists the patent family members relating to the patent documents cited in the above mentioned European search report.							The members are as contained in the European Patent Office EDP file on							The European Patent Office is in no way liable for these particulars which are merely given for the purpose of information.							For more details about this annex : see Official Journal of the European Patent Office, No 12/82						--><srep-patent-family><patent-family><priority-application><document-id><country>WO</country><doc-number>2020020770</doc-number><kind>A1</kind><date>20200130</date></document-id></priority-application><family-member><document-id><country>CN</country><doc-number>112513674</doc-number><kind>A</kind><date>20210316</date></document-id></family-member><family-member><document-id><country>EP</country><doc-number>3827282</doc-number><kind>A1</kind><date>20210602</date></document-id></family-member><family-member><document-id><country>WO</country><doc-number>2020020770</doc-number><kind>A1</kind><date>20200130</date></document-id></family-member></patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>8784318</doc-number><kind>B1</kind><date>20140722</date></document-id></priority-application><family-member><document-id><country>US</country><doc-number>8784318</doc-number><kind>B1</kind><date>20140722</date></document-id></family-member><family-member><document-id><country>US</country><doc-number>2015073276</doc-number><kind>A1</kind><date>20150312</date></document-id></family-member></patent-family><patent-family><priority-application><document-id><country>WO</country><doc-number>2018130370</doc-number><kind>A1</kind><date>20180719</date></document-id></priority-application><text>NONE</text></patent-family></srep-patent-family></srep-for-pub></search-report-data>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Non-patent literature cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><nplcit id="ref-ncit0001" npl-type="b"><article><atl/><book><author><name>T. M. MITCHELL</name></author><book-title>Machine Learning</book-title><imprint><name>McGraw-Hill</name><pubdate>19970000</pubdate></imprint><location><pp><ppf>2</ppf><ppl/></pp></location></book></article></nplcit><crossref idref="ncit0001">[0042]</crossref></li>
<li><nplcit id="ref-ncit0002" npl-type="s"><article><author><name>A. KENDALL et al.</name></author><atl>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</atl><serial><sertitle>NIPS'17: Proceedings of the 31st International Conference on Neural Information Processing Systems</sertitle><pubdate><sdate>20171200</sdate><edate/></pubdate></serial><location><pp><ppf>5580</ppf><ppl>5590</ppl></pp></location></article></nplcit><crossref idref="ncit0002">[0106]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
