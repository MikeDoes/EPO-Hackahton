<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP19910606A1" file="EP19910606NWA1.xml" lang="en" country="EP" doc-number="3889846" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3889846</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121><B121EP>published in accordance with Art. 153(4) EPC</B121EP></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>19910606.3</B210><B220><date>20190124</date></B220><B240><B241><date>20210630</date></B241></B240><B250>zh</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>201910041235</B310><B320><date>20190116</date></B320><B330><ctry>CN</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>G06N   3/08        20060101AFI20200724BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G06N   3/08        20130101 LI20200813BCEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>VERFAHREN UND SYSTEM ZUM TRAINIEREN VON TIEFENLERNMODELLEN</B542><B541>en</B541><B542>DEEP LEARNING MODEL TRAINING METHOD AND SYSTEM</B542><B541>fr</B541><B542>PROCÉDÉ ET SYSTÈME D'ENTRAÎNEMENT DE MODÈLE D'APPRENTISSAGE PROFOND</B542></B540><B590><B598>3</B598></B590></B500><B700><B710><B711><snm>Huawei Technologies Co., Ltd.</snm><iid>101846758</iid><irf>MTH01022EP</irf><adr><str>Huawei Administration Building 
Bantian</str><city>Longgang
Shenzhen, Guangdong 518129</city><ctry>CN</ctry></adr></B711></B710><B720><B721><snm>BAI, Xiaolong</snm><adr><str>Huawei Administration Building Bantian, Longgang</str><city>Shenzhen, Guangdong 518129</city><ctry>CN</ctry></adr></B721><B721><snm>LI, Pengfei</snm><adr><str>Huawei Administration Building Bantian, Longgang</str><city>Shenzhen, Guangdong 518129</city><ctry>CN</ctry></adr></B721><B721><snm>ZHANG, Zhenyu</snm><adr><str>Huawei Administration Building Bantian, Longgang</str><city>Shenzhen, Guangdong 518129</city><ctry>CN</ctry></adr></B721></B720><B740><B741><snm>Gill Jennings &amp; Every LLP</snm><iid>101574570</iid><adr><str>The Broadgate Tower 
20 Primrose Street</str><city>London EC2A 2ES</city><ctry>GB</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP><B860><B861><dnum><anum>CN2019072895</anum></dnum><date>20190124</date></B861><B862>zh</B862></B860><B870><B871><dnum><pnum>WO2020147142</pnum></dnum><date>20200723</date><bnum>202030</bnum></B871></B870></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">This application provides a deep learning model training method. The method includes: generating N first gradient sets in BP calculation in the j<sup>th</sup> iteration of N deep learning models; adjusting a communication sequence of gradients included in each of the first gradient sets, where the gradients included in each of the first gradient sets are not sent to parameter storage space in a generation sequence of the gradients included in each of the first gradient sets; and separately sending, according to an adjusted communication sequence of the gradients, the gradients included in each of the N first gradient sets to the parameter storage space. According to the method, a sequence of transmitting a gradient obtained in a current iteration process to the parameter storage space is adjusted, to increase training efficiency of the deep learning model.<img id="iaf01" file="imgaf001.tif" wi="101" he="75" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<p id="p0001" num="0001">This application claims priority to <patcit id="pcit0001" dnum="CN201910041235" dnum-type="L"><text>Chinese Patent Application No. 201910041235.8</text></patcit>, filed with the Chinese Patent Office on January 16, 2019 and entitled "DEEP LEARNING MODEL DISTRIBUTED TRAINING METHOD AND SYSTEM", which is incorporated herein by reference in its entirety.</p>
<heading id="h0001"><b>TECHNICAL FIELD</b></heading>
<p id="p0002" num="0002">This application relates to the field of artificial intelligence, and more specifically, to a deep learning model training method and a system for performing the training method.</p>
<heading id="h0002"><b>BACKGROUND</b></heading>
<p id="p0003" num="0003">Artificial intelligence (artificial intelligence, AI) is a theory, a method, a technology, or an application system that simulates, extends, and expands human intelligence by using a digital computer or a machine controlled by the digital computer, to sense an environment, obtain knowledge, and obtain an optimal result by using the knowledge. In other words, the artificial intelligence is a branch of computer science, and is intended to understand essence of intelligence and produce a new intelligent machine that can react in a manner similar to the human intelligence. The artificial intelligence is to study design principles and implementation methods of various intelligent machines, to enable the machines to have perception, inference, and decision-making functions. Researches in the artificial intelligence field include a robot, natural language processing, computer vision, decision-making and inference, human-computer interaction, recommendation and search, an AI basic theory, and the like.</p>
<p id="p0004" num="0004">In the AI field, deep learning is a learning technology based on a deep neural network algorithm. A deep learning model includes forward propagation (forward propagation, FP) calculation and back propagation (back propagation, BP) calculation. The FP calculation is used to calculate an output of a neuron at each layer based on a parameter matrix corresponding to the neuron at the layer, and the BP calculation is used to calculate a gradient corresponding to the neuron at each layer based on an error between a predicted value generated based on the FP calculation and prior knowledge, so that in FP calculation in a next iteration, the parameter matrix corresponding to the neuron at each layer is corrected based on the gradient obtained through the BP calculation.</p>
<p id="p0005" num="0005">There is usually a relatively huge amount of training data, training of the deep learning model is generally<!-- EPO <DP n="2"> --> performed in a distributed manner, and the training is completed based on the training data in the distributed manner by using a plurality of deep learning models. Therefore, gradients generated based on each time of BP calculation need to be synchronized between the deep learning models, to implement synchronous training. A conventional gradient synchronization method in a training process of a distributed deep learning model leads to low training efficiency.</p>
<heading id="h0003"><b>SUMMARY</b></heading>
<p id="p0006" num="0006">This application provides a deep learning model training method. A sequence of transmitting gradients obtained through BP calculation in a current iteration process to parameter storage space is adjusted, to increase training efficiency of a deep learning model.</p>
<p id="p0007" num="0007">According to a first aspect, a deep learning model training method is provided. The method is applied to a training system, the training system includes N deep learning models, and each of the deep learning models includes n layers of neurons. A training process of each of the deep learning models includes a plurality of iterations, and each iteration includes forward propagation FP calculation and back propagation BP calculation, where N is a positive integer greater than 1, and n is a positive integer greater than 1. The method includes: generating N first gradient sets in BP calculation in the j<sup>th</sup> iteration of the N deep learning models; in a process of generating the N first gradient sets, adjusting a communication sequence of gradients included in each first gradient set; separately sending, to parameter storage space of the training system according to an adjusted communication sequence of the gradients included in each first gradient set, the gradients included in each of the N first gradient sets; then obtaining a second gradient set based on the N first gradient sets stored in the parameter storage space; and correcting a parameter matrix of a neuron at each layer of each deep learning model based on a gradient included in the second gradient set, to perform FP calculation in the (j + 1)<sup>th</sup> iteration on each deep learning model.</p>
<p id="p0008" num="0008">It should be understood that each first gradient set includes a gradient corresponding to a parameter matrix of a neuron at each layer of one deep learning model, and j is a positive integer greater than 0.</p>
<p id="p0009" num="0009">It should be further understood that after the gradient included in each of the N first gradient sets is separately sent to the parameter storage space of the training system according to the adjusted communication sequence of the gradients included in each first gradient set, an average value of gradients corresponding to the parameter matrix of the neuron at each layer of each of the N deep learning models can be calculated.</p>
<p id="p0010" num="0010">In a possible implementation, weighted average calculation may be performed on the gradients of the neuron at each layer that are included in each of the N first gradient sets, so that the average value of the gradients<!-- EPO <DP n="3"> --> corresponding to the parameter matrix of the neuron at each layer of each of the N deep learning models can be calculated. Average values of gradients corresponding to parameter matrices of neurons at all layers constitute the second gradient set. In other words, the second gradient set includes the average values of the gradients corresponding to the parameter matrices of the neurons at all the layers of the N deep learning models.</p>
<p id="p0011" num="0011">In the foregoing technical solution, a sequence of transmitting <maths id="math0001" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0001" file="imgb0001.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> obtained through BP calculation to the parameter storage space in a current iteration process may be adjusted, to reduce an iteration time of the deep learning model in the current iteration process, and increase iteration efficiency of the deep learning model.</p>
<p id="p0012" num="0012">In a possible implementation, a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the a<sup>th</sup> layer to the parameter storage space is adjusted to be before a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the b<sup>th</sup> layer to the parameter storage space, where b is less than or equal to n, a is less than b, and a is a positive integer greater than 0.</p>
<p id="p0013" num="0013">In the foregoing technical solution, the sequence of sending the gradient corresponding to the parameter matrix of the neuron at the a<sup>th</sup> layer to the parameter storage space is adjusted to before the sequence of sending the gradient corresponding to the parameter matrix of the neuron at the b<sup>th</sup> layer to the parameter storage space. In this way, a time difference between an end time for BP calculation in this iteration and a start time for FP calculation in a next iteration can be reduced, and the iteration time of the deep learning model can be reduced.</p>
<p id="p0014" num="0014">In another possible implementation, the communication sequence of the gradients included in each first gradient set may be adjusted according to a gradient communication policy. The gradient communication policy is set based on at least one of the following parameters: a communication bandwidth between the deep learning model and the parameter storage space, a value of the gradient corresponding to the parameter matrix of the neuron at each layer of the deep learning model, and a time required by the neuron at each layer of the deep learning model in FP calculation.</p>
<p id="p0015" num="0015">It should be noted that the deep learning model is any one or more of the N deep learning models.</p>
<p id="p0016" num="0016">Specifically, before a sending sequence of the gradient corresponding to the parameter matrix of the neuron at the a<sup>th</sup> layer is adjusted, the gradient communication policy may be first calculated based on the communication bandwidth between the deep learning module and the parameter storage space, a value of the gradient corresponding to the parameter matrix of the neuron at the b<sup>th</sup> layer, and a time period between a moment at which the gradient corresponding to the parameter matrix of the neuron at the b<sup>th</sup> layer starts to be sent to the parameter storage space and a time at which FP calculation corresponding to a neuron at the (b - 1)<sup>th</sup> layer in the (j + 1)<sup>th</sup> iteration of the deep learning model is completed. Then, according to a gradient adjustment policy, the sequence of sending the gradient corresponding to the parameter matrix of the neuron at the a<sup>th</sup> layer to the parameter storage space is adjusted to be before the sequence of sending the gradient corresponding to the parameter matrix of the neuron at the b<sup>th</sup> layer<!-- EPO <DP n="4"> --> to the parameter storage space.</p>
<p id="p0017" num="0017">It should be noted that the gradient communication policy includes a sequence of transmitting the gradients in the first gradient set to a parameter storage area.</p>
<p id="p0018" num="0018">In the foregoing technical solution, the gradient communication policy may be determined based on the communication bandwidth between the deep learning module and the parameter storage space, the value of the gradient corresponding to the parameter matrix of the neuron at each layer of the deep learning model, and the time required by the neuron at each layer of the deep learning model in the FP calculation. Therefore, the communication sequence of the gradients in the first gradient set of the deep learning model may be adjusted according to an optimal gradient communication policy, so that a subsequent iterative training speed is faster, and training efficiency of the deep learning model is increased.</p>
<p id="p0019" num="0019">In a possible implementation, the sequence of sending the gradient corresponding to the parameter matrix of the neuron at the a<sup>th</sup> layer is adjusted to be before a sequence of sending the gradient corresponding to the parameter matrix of the neuron at the b<sup>th</sup> layer to the parameter storage space, the gradient corresponding to the parameter matrix of the neuron at the b<sup>th</sup> layer is sent to the parameter storage space as far as possible before a neuron at the (b - 1)<sup>th</sup> layer in the (j + 1)<sup>th</sup> iteration completes corresponding FP calculation.</p>
<p id="p0020" num="0020">In another possible implementation, the method further includes: obtaining the iteration time of the deep learning model; and adjusting the gradient communication policy based on the iteration time.</p>
<p id="p0021" num="0021">It should be understood that the obtained iteration time of the deep learning model may be a sum of a time for BP calculation in a current iteration process and a time for FP calculation in a next iteration process. To be specific, the iteration time of the deep learning model includes a time for BP calculation in the L<sup>th</sup> iteration of the deep learning model and a time for FP calculation in the (L + 1)<sup>th</sup> iteration of the deep learning model, where L is a positive integer greater than j.</p>
<p id="p0022" num="0022">It should be noted that the deep learning model is any one or more of the N deep learning models.</p>
<p id="p0023" num="0023">In the foregoing technical solution, the gradient communication policy of the deep learning model may be adjusted based on the fed-back iteration time of the deep learning model. In this way, the optimal gradient communication policy can be determined based on an actual iteration time of the deep learning model, and an iterative training speed of the deep learning model can be increased.</p>
<p id="p0024" num="0024">According to a second aspect, a deep learning model training system is provided. The training system includes N deep learning models, a gradient communications module, a gradient update module, a correction module, and parameter storage space. Each of the deep learning models includes n layers of neurons, and a training process of each of the deep learning models includes a plurality of iterations. Each iteration includes forward propagation FP<!-- EPO <DP n="5"> --> calculation and back propagation BP calculation, where N is a positive integer greater than 1, and n is a positive integer greater than 1.</p>
<p id="p0025" num="0025">Each deep learning module of the N deep learning models is configured to generate a first gradient set in BP calculation in the j<sup>th</sup> iteration. Each first gradient set includes a gradient corresponding to a parameter matrix of a neuron at each layer of each of the deep learning models, and j is a positive integer greater than 0.</p>
<p id="p0026" num="0026">The gradient communications module is configured to: adjust a communication sequence of the gradients included in each first gradient set, and separately send, to the parameter storage space of the training system according to an adjusted communication sequence of the gradients included in each first gradient set, the gradient included in each of the N first gradient sets.</p>
<p id="p0027" num="0027">The gradient update module is configured to obtain a second gradient set based on the N first gradient sets stored in the parameter storage space.</p>
<p id="p0028" num="0028">The correction module is configured to correct the parameter matrix of the neuron at each layer of each deep learning model based on a gradient included in the second gradient set, to perform FP calculation in the (j + 1)<sup>th</sup> iteration of each deep learning model.</p>
<p id="p0029" num="0029">It should be noted that the gradient communications module may include two submodules. One submodule is an adjustment submodule, configured to adjust the communication sequence of the gradients included in each first gradient set. The other submodule is a communications submodule, configured to separately send, to the parameter storage space of the training system according to the adjusted communication sequence of the gradients included in each first gradient set, the gradient included in each of the N first gradient sets.</p>
<p id="p0030" num="0030">It should be further noted that in a distributed model training system that includes at least one model training server and one parameter server, the correction module may be a module in the parameter server, or may be a module in the at least one model training server. In an example, the correction module is in the parameter server, and the correction module is configured to correct a parameter matrix of a neuron at each layer of any one of the deep learning models based on the gradient included in the second gradient set. In addition, a corrected parameter matrix corresponding to the neuron at each layer is stored in parameter storage space of the parameter server, so that the at least one model training server obtains the corrected parameter matrix from the parameter storage space in a model training process in a next iteration. In another example, the correction module is in the at least one model training server, and after the at least one model training server obtains the second gradient set from the parameter storage space of the parameter server, the correction module may correct the parameter matrix of the neuron at each layer of any one of the deep learning models based on the second gradient set, so that the parameter matrix can be used in FP calculation in the (j + 1)<sup>th</sup> iteration of the any one of the deep learning models of the training system.<!-- EPO <DP n="6"> --></p>
<p id="p0031" num="0031">In the foregoing technical solution, a sequence of transmitting <maths id="math0002" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0002" file="imgb0002.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> obtained through BP calculation to the parameter storage space in a current iteration process may be adjusted, to reduce an iteration time of the deep learning model in the current iteration process, and increase iteration efficiency of the deep learning model.</p>
<p id="p0032" num="0032">In a possible implementation, the gradient communications module is specifically configured to: adjust a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the a<sup>th</sup> layer to the parameter storage space to be before a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the b<sup>th</sup> layerto the parameter storage space, where b is less than or equal to n, a is less than b, and a is a positive integer greater than 0.</p>
<p id="p0033" num="0033">In another possible implementation, the gradient communications module is specifically configured to adjust, according to a gradient communication policy, the communication sequence of the gradients included in each first gradient set.</p>
<p id="p0034" num="0034">The gradient communication policy is set based on at least one of the following parameters: a communication bandwidth between the deep learning model and the parameter storage space, a value of the gradient corresponding to the parameter matrix of the neuron at each layer of the deep learning model, and a time required by the neuron at each layer of the deep learning model in FP calculation.</p>
<p id="p0035" num="0035">It should be noted that the deep learning model is any one or more of the N deep learning models.</p>
<p id="p0036" num="0036">In another possible implementation, the system further includes a feedback module.</p>
<p id="p0037" num="0037">The feedback module is configured to: obtain the iteration time of the deep learning model, and feed back the obtained iteration time to the gradient communications module.</p>
<p id="p0038" num="0038">It should be understood that the obtained iteration time of the deep learning model may be a sum of a time for BP calculation in a current iteration process and a time for FP calculation in a next iteration process.</p>
<p id="p0039" num="0039">The gradient communications module is further configured to adjust the gradient communication policy based on the iteration time that is of the deep learning model and that is fed back by the feedback module.</p>
<p id="p0040" num="0040">It should be understood that in the distributed model training system that includes at least one model training server and one parameter server, the feedback module is a set of feedback modules in the at least one model training server.</p>
<p id="p0041" num="0041">According to a third aspect, a deep learning model training system is provided. The training system includes at least one computing node, and each computing node includes a memory and at least one processor. The memory is configured to store a program instruction, and when the training system runs, the at least one processor of the at least one computing node executes the program instruction in the memory to perform the method according to any one of the first aspect or the possible implementations of the first aspect.<!-- EPO <DP n="7"> --></p>
<p id="p0042" num="0042">In a possible implementation, the deep learning model training system includes one parameter server and at least one model training server. One model training server may be used as one computing node, and N deep learning modules and a gradient communications module may separately run in the at least one model training server. A gradient update module may run in the parameter server in the training system. A correction module may run in the at least one model training server or the parameter server.</p>
<p id="p0043" num="0043">In a possible implementation, in the deep learning model training system that includes one parameter server and at least one model training server, a feedback module runs in the at least one model training server.</p>
<p id="p0044" num="0044">It should be noted that in the training system that includes one parameter server and at least one model training server, the gradient communications module may be a set of gradient communications modules in the at least one model training server, and the correction module may be a set of correction modules in the at least one model training server. The feedback module may be a set of feedback modules in the at least one model training server.</p>
<p id="p0045" num="0045">In another possible implementation, the deep learning model training system includes one model training server. One model training server includes at least one processor, and one processor may be used as one computing node. N deep learning modules, a gradient communications module, a gradient update module, and a correction module may separately run in the at least one processor.</p>
<p id="p0046" num="0046">In a possible implementation, in the deep learning model training system that includes one model training server, the feedback module runs in the at least one processor of the model training server.</p>
<p id="p0047" num="0047">It should be noted that in the training system that includes one model training server, each of the gradient communications module, the gradient update module, the correction module, and the feedback module may be a set of foregoing modules included in the at least one processor in the model training server.</p>
<p id="p0048" num="0048">According to a fourth aspect, a non-transient readable storage medium is provided, including a program instruction. When the program instruction is run by at least one computing node, the at least one computing node performs the method according to any one of the first aspect and the possible implementations of the first aspect.</p>
<p id="p0049" num="0049">According to a fifth aspect, a computer program product is provided, including a program instruction. When the program instruction is run by at least one computing node, the at least one computing node performs the method according to any one of the first aspect and the possible implementations of the first aspect.</p>
<p id="p0050" num="0050">Based on the implementations provided in the foregoing aspects, this application may be further combined to provide more implementations.<!-- EPO <DP n="8"> --></p>
<heading id="h0004"><b>BRIEF DESCRIPTION OF DRAWINGS</b></heading>
<p id="p0051" num="0051">
<ul id="ul0001" list-style="none" compact="compact">
<li><figref idref="f0001">FIG. 1</figref> is a schematic block diagram of a deep learning model 100 according to an embodiment of this application;</li>
<li><figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> are a schematic structural diagram of a distributed training system 200 of a deep learning model 100 according to an embodiment of this application;</li>
<li><figref idref="f0004">FIG. 3</figref> is a schematic block diagram of communication between each model training server and a parameter server according to an embodiment of this application;</li>
<li><figref idref="f0005">FIG. 4</figref> is a schematic structural diagram of a distributed training system 400 of a deep learning model 100 according to an embodiment of this application;</li>
<li><figref idref="f0006">FIG. 5</figref> is a schematic flowchart of a deep learning model training method according to an embodiment of this application;</li>
<li><figref idref="f0007">FIG. 6</figref> is a schematic architectural diagram of a deep learning model training system according to an embodiment of this application;</li>
<li><figref idref="f0008">FIG. 7</figref> is a schematic flowchart of a method for acceleration training of a deep learning model according to an embodiment of this application;</li>
<li><figref idref="f0008">FIG. 8(a)</figref> is a comparison diagram of iteration time effects of a method for acceleration training of a deep learning model according to an embodiment of this application; and</li>
<li><figref idref="f0008">FIG. 8(b)</figref> is a comparison diagram of iteration time effects of a method for acceleration training of a deep learning model according to an embodiment of this application.</li>
</ul></p>
<heading id="h0005"><b>DESCRIPTION OF EMBODIMENTS</b></heading>
<p id="p0052" num="0052">The following describes technical solutions of this application with reference to accompanying drawings.</p>
<p id="p0053" num="0053">In the AI field, deep learning is a learning technology based on a deep neural network algorithm. A deep learning model includes an input layer, a hidden layer, and an output layer. The deep learning model processes data by using a plurality of nonlinear transformations.</p>
<p id="p0054" num="0054">It should be understood that a neural network is a behavior feature that imitates an animal neural network. This type of network depends on complexity of a system, and processes information by adjusting a relationship between a large quantity of internal nodes that are connected to each other.</p>
<p id="p0055" num="0055">It should be further understood that a deep neural network (the deep learning model) may be understood as a neural network having a plurality of hidden layers, and "a plurality of" herein does not have a special measurement<!-- EPO <DP n="9"> --> standard. Theoretically, a model with a larger quantity of parameters indicates higher complexity and a larger "capacity", and indicates that the model can complete a more complex learning task. A process of training the deep neural network is a process of learning a parameter matrix, and a final objective of the process of training the deep neural network is to obtain a parameter matrix of a neuron at each layer of the trained deep neural network (the parameter matrix of the neurons at each layer includes a weight corresponding to each neuron included in the neurons at the layer).</p>
<p id="p0056" num="0056">With reference to <figref idref="f0001">FIG. 1</figref>, the following describes in detail a possible deep learning model training process corresponding to the embodiments of this application.</p>
<p id="p0057" num="0057"><figref idref="f0001">FIG. 1</figref> is a schematic block diagram of a deep learning model 100 according to an embodiment of this application. The deep learning model 100 may include an input layer 110, a hidden layer 120, and an output layer 130.</p>
<p id="p0058" num="0058">It should be understood that in this embodiment of this application, an example in which the hidden layer 120 includes n (n is greater than 1) layers of neurons is used for description.</p>
<p id="p0059" num="0059">It should be further understood that each of the input layer 110, the output layer 130, and the hidden layer 120 includes one or more neurons. In <figref idref="f0001">FIG. 1</figref>, an example in which the input layer 110 includes two neurons, each of the n layers of the hidden layer 120 includes three neurons, and the output layer 130 includes one neuron is used for description.</p>
<p id="p0060" num="0060">The deep learning model 100 shown in <figref idref="f0001">FIG. 1</figref> may be a fully connected neural network or a convolutional neural network (convolutional neural network, CNN). When all neurons at each layer are connected to all neurons at a next layer (none of weights w of all the neurons at each layer is 0), the deep learning model 100 is a fully connected neural network model. When all neurons at each layer are not connected to all neurons at a next layer (not all weights w of all the neurons at each layer is 0), the deep learning model 100 is a CNN model.</p>
<p id="p0061" num="0061">Referring to <figref idref="f0001">FIG. 1</figref>, the deep learning model 100 may include forward propagation (forward propagation, FP) calculation and back propagation (back propagation, BP) calculation.</p>
<p id="p0062" num="0062">The following describes in detail a process of performing FP calculation in a computing node.</p>
<p id="p0063" num="0063">In the process of performing FP calculation, training data, for example, pixel information of an input image, is obtained, and the training data is used as an input (<i>i</i><sub>1</sub>, <i>i</i><sub>2</sub>) of the input layer 110 of the deep learning model 100. A prediction result may be output from the output layer 130 after the input of the input layer 110 passes through a plurality of neurons at the hidden layer 120. Specifically, a neuron at each layer of the hidden layer 120 corresponds to one parameter matrix. A product of the input of the input layer 110 and a parameter matrix of a neuron at the first layer is used as an input of the neuron at the first layer of the hidden layer 120. An activation function (for example, a sigmoid function) in the neuron at the first layer is performed on the input of the neuron at the first layer of the hidden<!-- EPO <DP n="10"> --> layer 120, to output an output value of the neuron at the first layer. A product of the output value of the neuron at the first layer of the hidden layer 120 and a parameter matrix of a neuron at the second layer is used as an input of the neuron at the second layer of the hidden layer 120. Similarly, by analogy, the prediction result is finally output from the output layer 130.</p>
<p id="p0064" num="0064">In an actual application, weights in these parameter matrices need to be corrected in a large amount of training. Each parameter matrix constituted by weights obtained through training may extract pixel information from a to-be-inferred image input by a user, to help the deep learning model 100 perform correct inference on the to-be-inferred image.</p>
<p id="p0065" num="0065">In a j<sup>th</sup> iteration process of the FP calculation, an input of the first neuron at the first layer is <maths id="math0003" num=""><math display="inline"><msubsup><mi>A</mi><mn>11</mn><mi>j</mi></msubsup><mo>=</mo></math><img id="ib0003" file="imgb0003.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> <maths id="math0004" num=""><math display="inline"><msubsup><mi>w</mi><mn>11</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>w</mi><mn>14</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>2</mn></msub></math><img id="ib0004" file="imgb0004.tif" wi="29" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, and an output of the first neuron at the first layer is <maths id="math0005" num=""><math display="inline"><mi mathvariant="normal">f</mi><mfenced><msubsup><mi>A</mi><mn>11</mn><mi>j</mi></msubsup></mfenced></math><img id="ib0005" file="imgb0005.tif" wi="10" he="7" img-content="math" img-format="tif" inline="yes"/></maths>. An input of the second neuron at the first layer is <maths id="math0006" num=""><math display="inline"><msubsup><mi>A</mi><mn>12</mn><mi>j</mi></msubsup><mo>=</mo><msubsup><mi>w</mi><mn>12</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>w</mi><mn>15</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>2</mn></msub></math><img id="ib0006" file="imgb0006.tif" wi="40" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, and an output of the second neuron at the first layer is <maths id="math0007" num=""><math display="inline"><mi mathvariant="normal">f</mi><mfenced><msubsup><mi>A</mi><mn>12</mn><mi>j</mi></msubsup></mfenced></math><img id="ib0007" file="imgb0007.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths>. An input of the third neuron at the first layer is <maths id="math0008" num=""><math display="inline"><msubsup><mi>A</mi><mn>13</mn><mi>j</mi></msubsup><mo>=</mo><msubsup><mi>w</mi><mn>13</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>w</mi><mn>16</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>2</mn></msub></math><img id="ib0008" file="imgb0008.tif" wi="41" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, and an output of the third neuron at the first layer is <maths id="math0009" num=""><math display="inline"><mi mathvariant="normal">f</mi><mfenced><msubsup><mi>A</mi><mn>13</mn><mi>j</mi></msubsup></mfenced></math><img id="ib0009" file="imgb0009.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, where <maths id="math0010" num=""><math display="inline"><mi mathvariant="normal">f</mi><mfenced><msubsup><mi>A</mi><mn>11</mn><mi>j</mi></msubsup></mfenced></math><img id="ib0010" file="imgb0010.tif" wi="11" he="6" img-content="math" img-format="tif" inline="yes"/></maths> is an activation function whose input is <maths id="math0011" num=""><math display="inline"><msubsup><mi>A</mi><mn>11</mn><mi>j</mi></msubsup></math><img id="ib0011" file="imgb0011.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>.</i></p>
<p id="p0066" num="0066">In the j<sup>th</sup> iteration process, the input of the neuron at the first layer is: <maths id="math0012" num=""><math display="block"><mfenced><mtable><mtr><mtd><mrow><msubsup><mi>w</mi><mn>11</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>w</mi><mn>14</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>2</mn></msub></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>A</mi><mn>12</mn></msub><mo>=</mo><msubsup><mi>w</mi><mn>12</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>w</mi><mn>15</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>2</mn></msub></mrow></mtd></mtr><mtr><mtd><mrow><msubsup><mi>w</mi><mn>13</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>w</mi><mn>16</mn><mi>j</mi></msubsup><mo>×</mo><msub><mi>i</mi><mn>2</mn></msub></mrow></mtd></mtr></mtable></mfenced><mo>=</mo><mfenced><mtable><mtr><mtd><msubsup><mi>w</mi><mn>11</mn><mi>j</mi></msubsup></mtd><mtd><msubsup><mi>w</mi><mn>14</mn><mi>j</mi></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>w</mi><mn>12</mn><mi>j</mi></msubsup></mtd><mtd><msubsup><mi>w</mi><mn>15</mn><mi>j</mi></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>w</mi><mn>13</mn><mi>j</mi></msubsup></mtd><mtd><msubsup><mi>w</mi><mn>16</mn><mi>j</mi></msubsup></mtd></mtr></mtable></mfenced><mo>×</mo><mfenced><mtable><mtr><mtd><msub><mi>i</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>i</mi><mn>2</mn></msub></mtd></mtr></mtable></mfenced></math><img id="ib0012" file="imgb0012.tif" wi="79" he="17" img-content="math" img-format="tif"/></maths></p>
<p id="p0067" num="0067">Therefore, the input of the neuron at the first layer may be represented as <maths id="math0013" num=""><math display="inline"><msubsup><mi>A</mi><mn>1</mn><mi>j</mi></msubsup><mo>=</mo><msubsup><mi>w</mi><mn>1</mn><mi>j</mi></msubsup><mo>×</mo><msubsup><mi>B</mi><mn>0</mn><mi>j</mi></msubsup></math><img id="ib0013" file="imgb0013.tif" wi="22" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, and the output of the neuron at the first layer may be represented as <maths id="math0014" num=""><math display="inline"><msubsup><mi>B</mi><mn>1</mn><mi>j</mi></msubsup><mo>=</mo><mi mathvariant="normal">f</mi><mfenced><msubsup><mi>A</mi><mn>1</mn><mi>j</mi></msubsup></mfenced></math><img id="ib0014" file="imgb0014.tif" wi="17" he="6" img-content="math" img-format="tif" inline="yes"/></maths>. <maths id="math0015" num=""><math display="block"><msubsup><mi>A</mi><mn>1</mn><mi>j</mi></msubsup><mo>=</mo><mfenced><mtable><mtr><mtd><msub><mi>A</mi><mn>11</mn></msub></mtd></mtr><mtr><mtd><msub><mi>A</mi><mn>12</mn></msub></mtd></mtr><mtr><mtd><msub><mi>A</mi><mn>13</mn></msub></mtd></mtr></mtable></mfenced><mo>,</mo><mspace width="1ex"/><msubsup><mi>w</mi><mn>1</mn><mi>j</mi></msubsup><mo>=</mo><mfenced><mtable><mtr><mtd><msubsup><mi>w</mi><mn>11</mn><mi>j</mi></msubsup></mtd><mtd><msubsup><mi>w</mi><mn>14</mn><mi>j</mi></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>w</mi><mn>12</mn><mi>j</mi></msubsup></mtd><mtd><msubsup><mi>w</mi><mn>15</mn><mi>j</mi></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>w</mi><mn>13</mn><mi>j</mi></msubsup></mtd><mtd><msubsup><mi>w</mi><mn>16</mn><mi>j</mi></msubsup></mtd></mtr></mtable></mfenced><mo>,</mo><mspace width="1ex"/><mi>and</mi><mspace width="1ex"/><msubsup><mi>B</mi><mn>0</mn><mi>j</mi></msubsup><mo>=</mo><mfenced><mtable><mtr><mtd><msub><mi>i</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>i</mi><mn>2</mn></msub></mtd></mtr></mtable></mfenced></math><img id="ib0015" file="imgb0015.tif" wi="77" he="19" img-content="math" img-format="tif"/></maths></p>
<p id="p0068" num="0068">j is used to represent a quantity of iteration times, and is usually equal to a quantity of times that the input layer 110 obtains the input (<i>i</i><sub>1</sub>, <i>i</i><sub>2</sub>). <maths id="math0016" num=""><math display="inline"><msubsup><mi>w</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0016" file="imgb0016.tif" wi="6" he="7" img-content="math" img-format="tif" inline="yes"/></maths> is used to represent a parameter matrix of the neuron at the first layer in the j<sup>th</sup> iteration process.</p>
<p id="p0069" num="0069">A product of an output <i>B</i><sub>1</sub> of the neuron at the first layer and the parameter matrix of the neuron at the second layer may be used as the input of the neuron at the second layer. Therefore, in the j<sup>th</sup> iteration process of the FP, the input of the neuron at the second layer may be represented as <maths id="math0017" num=""><math display="inline"><msubsup><mi>A</mi><mn>2</mn><mi>j</mi></msubsup><mo>=</mo><msubsup><mi>w</mi><mn>2</mn><mi>j</mi></msubsup><mo>×</mo><msubsup><mi>B</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0017" file="imgb0017.tif" wi="22" he="7" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> and the output of the neuron at the second layer may be represented as <maths id="math0018" num=""><math display="inline"><msubsup><mi>B</mi><mn>2</mn><mi>j</mi></msubsup><mo>=</mo><mi mathvariant="normal">f</mi><mfenced><msubsup><mi>A</mi><mn>2</mn><mi>j</mi></msubsup></mfenced></math><img id="ib0018" file="imgb0018.tif" wi="17" he="7" img-content="math" img-format="tif" inline="yes"/></maths>.</p>
<p id="p0070" num="0070">Likewise, in the j<sup>th</sup> iteration process of the FP, an input of a neuron at the i<sup>th</sup> layer may be represented as <maths id="math0019" num=""><math display="inline"><msubsup><mi>A</mi><mi>i</mi><mi>j</mi></msubsup><mo>=</mo><msubsup><mi>w</mi><mi>i</mi><mi>j</mi></msubsup><mo>×</mo><msubsup><mi>B</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mi>j</mi></msubsup></math><img id="ib0019" file="imgb0019.tif" wi="24" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, and an output of the neuron at the i<sup>th</sup> layer may be represented as <maths id="math0020" num=""><math display="inline"><msubsup><mi>B</mi><mi>i</mi><mi>j</mi></msubsup><mo>=</mo><mi mathvariant="normal">f</mi><mfenced><msubsup><mi>A</mi><mi>i</mi><mi>j</mi></msubsup></mfenced></math><img id="ib0020" file="imgb0020.tif" wi="17" he="7" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> where 1 ≤ i ≤ n.</p>
<p id="p0071" num="0071">The following describes in detail a process of performing BP calculation in a computing node.</p>
<p id="p0072" num="0072">In a process of training the deep learning model 100, it is expected that a prediction value <i>o</i><sub>1</sub> output<!-- EPO <DP n="11"> --> from the output layer 130 of the deep learning model 100 is as close as possible to prior knowledge (prior knowledge) of the training data. The prior knowledge is also referred to as ground truth (ground truth). Generally, the prior knowledge includes a prediction result corresponding to training data provided by a person. Therefore, a current prediction value can be compared with the prior knowledge. Then, a parameter matrix at each layer of the deep learning model 100 is updated based on a difference between the current predicted value and the prior knowledge (certainly, there is usually an initialization process before a first update, to be specific, the parameter matrix corresponding to the neuron at each layer of the hidden layer 120 of the deep learning model 100 is initialized). In addition, an error BP algorithm is used to correct a weight of the parameter matrix in the deep learning model 100 in the process of training the deep learning model 100, to minimize an error loss of the deep learning model 100.</p>
<p id="p0073" num="0073">Specifically, there may be an error between the prediction value generated in the process of performing FP calculation and the prior knowledge. If the output prediction value is greater than the prior knowledge, the weight in the parameter matrix may be adjusted to make the output prediction value smaller. If the output prediction value is smaller than the prior knowledge, the weight in the parameter matrix may be adjusted to make the output prediction value greater. The BP calculation is an error-dominant reverse motion, and aims to obtain an optimal parameter matrix of the neuron at each layer.</p>
<p id="p0074" num="0074">It should be understood that the training data input by the user may include training data used as an input and the prediction result corresponding to the training data provided by the person.</p>
<p id="p0075" num="0075">In an example, the deep learning model 100 is applied to the image recognition field. The training data input by the deep learning model 100 is pixel information of an image, and the prior knowledge corresponding to the training data is a label "dog" of the image. The training data is input to the input layer 110, and after FP calculation of the deep learning model 100 is performed on the training data, a prediction value output from the output layer 130 is compared with the prior knowledge. For example, if the prediction value output from the output layer 130 is "cat", the parameter matrix at each layer in the deep learning model 100 may be updated based on an error between the prediction value and the prior knowledge "dog".</p>
<p id="p0076" num="0076">In the j<sup>th</sup> iteration process, an error E between the output prediction value <i>o</i><sub>1</sub> and the prior knowledge can be obtained through the BP calculation. In addition, a weight in the parameter matrix of the neuron at each layer in the deep learning model 100 may be corrected based on the error E along a direction of the output layer 130, the hidden layer 120, and the input layer 110. Specifically, correction of the weight may be calculating a gradient <maths id="math0021" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0021" file="imgb0021.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> of the weight in the parameter matrix. The gradient <maths id="math0022" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0022" file="imgb0022.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> may be a derivative of the weight in the parameter matrix by using the error E, where 1 ≤ i ≤ n.</p>
<p id="p0077" num="0077">A (j + 1)<sup>th</sup> iteration process of the deep learning model 100 is similar to the j<sup>th</sup> iteration process of the<!-- EPO <DP n="12"> --> deep learning model 100, and the deep learning model 100 first performs FP calculation, and then performs BP calculation. For example, in an FP calculation process in the (j + 1)<sup>th</sup> iteration, the weight in the parameter matrix is corrected based on the gradient <maths id="math0023" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0023" file="imgb0023.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> obtained through the FP calculation of the j<sup>th</sup> iteration, and an output prediction value is calculated based on the corrected parameter matrix. In a BP calculation process in the (j + 1)<sup>th</sup> iteration, a gradient <maths id="math0024" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0024" file="imgb0024.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> of the weight in the parameter matrix is calculated based on the error E between the output value obtained through the FP calculation in the (j + 1)<sup>th</sup> iteration and the prior knowledge, so that the weight in the parameter matrix can be corrected again based on <maths id="math0025" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0025" file="imgb0025.tif" wi="8" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in a (j + 2)<sup>th</sup> iteration process. The weight in the parameter matrix is continuously corrected in a plurality of iteration processes, so that an output value predicted by the deep learning model 100 is as close as possible to the prior knowledge of the training data.</p>
<p id="p0078" num="0078">Specifically, in the FP calculation in the (j + 1)<sup>th</sup> iteration, when the input and the output of the neuron at the i<sup>th</sup> layer are calculated, a parameter matrix of the neuron at the i<sup>th</sup> layer is changed to <maths id="math0026" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msubsup><mi>w</mi><mi>i</mi><mi>j</mi></msubsup><mo>−</mo><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0026" file="imgb0026.tif" wi="26" he="8" img-content="math" img-format="tif" inline="yes"/></maths>. For a process of calculating an input and an output of the neuron at each layer based on <maths id="math0027" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0027" file="imgb0027.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, refer to the foregoing description of the FP calculation in the j<sup>th</sup> iteration. Details are not described herein again.</p>
<p id="p0079" num="0079">It should be noted that the parameter matrix calculation formula shown above is a possible implementation, or may be another variation of the formula, and falls within the protection scope of the embodiments of this application.</p>
<p id="p0080" num="0080">In this embodiment of this application, the training process (including the FP calculation process and the BP calculation process) of the deep learning model 100 may be completed in a training system including at least one computing node. The at least one computing node may be at least one model training server or at least one processor in one model training server. The following describes a scenario of training the deep learning model 100 with reference to <figref idref="f0002 f0003 f0004 f0005">FIG. 2A to FIG. 4</figref>.</p>
<p id="p0081" num="0081"><figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> are a schematic structural diagram of a distributed training system 200 of a deep learning model 100 according to an embodiment of this application. The distributed training system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> may include a model training server 210, a model training server 220, a model training server 230, a parameter server 240, and a cloud storage 250.</p>
<p id="p0082" num="0082">Generally, precision of the deep learning model increases with an amount of training data. However, increase in the amount of the training data increases computing load. Therefore, a distributed deep learning training technology emerges. Distributed deep learning training aims to increase computing resources by using a plurality of computing nodes, and iterate the trained model by using the plurality of computing nodes, to increase a training speed of the deep learning model.</p>
<p id="p0083" num="0083">Referring to <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>, the distributed training system 200 may include at least one model<!-- EPO <DP n="13"> --> training server, and one model training server may be used as one computing node. For ease of description, three model training servers are used as an example for description in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>. Structures of the model training server 220 and the model training server 230 are similar to a structure of the model training server 210. The following describes the model training server 210 in detail.</p>
<heading id="h0006">(1) Model training server 210:</heading>
<p id="p0084" num="0084">The model training server 210 includes at least one processor, a memory 213, an input/output interface 214, a communications interface 215, and a bus 216.</p>
<p id="p0085" num="0085">The at least one processor may be connected to the memory 213. The memory 213 may be configured to store program code and the training data. The memory 213 may be a storage unit inside the at least one processor, an external storage unit independent of the at least one processor, or a component including a storage unit inside the at least one processor and an external storage unit independent of the at least one processor.</p>
<p id="p0086" num="0086">The memory 213 may be a solid-state drive (solid state drive, SSD), a hard disk drive (hard disk drive, HDD), a read-only memory (read-only memory, ROM), a random access memory (random access memory, RAM), or the like.</p>
<p id="p0087" num="0087">The at least one processor may obtain the program code and the training data from the memory 213, and train the deep learning model 100. As an example instead of a limitation, the at least one processor may perform iterative calculation (for example, the FP calculation and the BP calculation shown in <figref idref="f0001">FIG. 1</figref>) based on the program code and the training data, or may send (push), to the parameter server 240 in the distributed training system 200, a gradient <maths id="math0028" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0028" file="imgb0028.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> that is of a weight in a parameter matrix and that is obtained through the BP calculation.</p>
<p id="p0088" num="0088">Specifically, the at least one processor may include two types of processors. One type of processor includes at least one data processor 211, and the other type of processor includes at least one iterative processor 212. As an example instead of a limitation, the data processor 211 may be a central processing unit (central processing unit, CPU), and the iterative processor 212 may be an embedded neural processing unit (neural-network process units, NPU) or a graphics processing unit (graphics processing unit, GPU).</p>
<p id="p0089" num="0089">A deep learning model 100 runs in the iterative processor 212, and BP calculation is performed on the deep learning model 100, to calculate a gradient <maths id="math0029" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0029" file="imgb0029.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> of a weight in a parameter matrix of a neuron at each layer. The data processor 211 may be configured to send (push) the gradient <maths id="math0030" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0030" file="imgb0030.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> obtained through the BP calculation to the parameter server 240.</p>
<p id="p0090" num="0090">A gradient communications module 2111 may run in the data processor 211, and the deep learning module 100 may run in the iterative processor 212. Optionally, a feedback module 2121 may run in the iterative processor 212. A correction module may run in the data processor 211, or may run in the parameter server 240. For<!-- EPO <DP n="14"> --> example, a correction module 2112 runs in the data processor 211. For another example, a correction module 2412 runs in a data processor 241 of the parameter server 240. For details about the modules running in the data processor 211, refer to descriptions in <figref idref="f0007">FIG. 6</figref>. Details are not described herein.</p>
<p id="p0091" num="0091">Optionally, the model training server 210 may further include the bus 216. The memory 213, the input/output interface 214, and the communications interface 215 may be connected to the at least one processor (for example, the data processor 211 and the iterative processor 212) through the bus 216. The bus 216 may be a peripheral component interconnect (peripheral component interconnect, PCI) bus, an extended industry standard architecture (extended industry standard architecture, EISA) bus, or the like. The bus 216 may be classified into an address bus, a data bus, a control bus, and the like. For ease of representation, only one line is used to represent the bus in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>, but this does not mean that there is only one bus or only one type of bus.</p>
<p id="p0092" num="0092">(2) Parameter server 240:</p>
<p id="p0093" num="0093">The parameter server 240 includes at least one data processor 241, a memory 243, an input/output interface 244, a communications interface 245, and a bus 246.</p>
<p id="p0094" num="0094">Parameter storage space in the memory 243 may store gradients <maths id="math0031" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0031" file="imgb0031.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, of weights, that are separately sent by the model training server 210, the model training server 220, and the model training server 230.</p>
<p id="p0095" num="0095">The at least one data processor 241 may be connected to the memory 243, and the data processor 241 may be, for example, a CPU. The at least one data processor 241 may obtain, from the memory 243, the gradients <maths id="math0032" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0032" file="imgb0032.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> separately sent by the model training server 210, the model training server 220, and the model training server 230, process a plurality of gradients <maths id="math0033" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0033" file="imgb0033.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> pushed by the processor, and store the processed gradients <maths id="math0034" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0034" file="imgb0034.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the memory 243. In an example, the at least one data processor 241 may perform weighted average calculation on the plurality of gradients <maths id="math0035" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0035" file="imgb0035.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> separately pushed by the plurality of model training servers, to obtain <maths id="math0036" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0036" file="imgb0036.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, and store the gradient average value <maths id="math0037" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0037" file="imgb0037.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the memory 243. In a process of obtaining the gradient <maths id="math0038" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0038" file="imgb0038.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> based on the plurality of gradients <maths id="math0039" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0039" file="imgb0039.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, in addition to the weighted average calculation, another algorithm may also be used.</p>
<p id="p0096" num="0096">A gradient update module 2411 may run in the data processor 241. Optionally, a correction module 242 may further run in the data processor 241. For details about the modules running in the data processor 241, refer to descriptions in <figref idref="f0007">FIG. 6</figref>. Details are not described herein.</p>
<p id="p0097" num="0097">It should be noted that, in this embodiment of this application, in a j<sup>th</sup> iteration process, after the data processor 241 in the parameter server 240 calculates the gradient average value <maths id="math0040" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0040" file="imgb0040.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in FP calculation in the (j + 1)<sup>th</sup> iteration, <maths id="math0041" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0041" file="imgb0041.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter matrix in the (j + 1)<sup>th</sup> iteration further needs to be corrected based on the gradient average value <maths id="math0042" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0042" file="imgb0042.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, and <maths id="math0043" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0043" file="imgb0043.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> is stored in the parameter storage space of the memory 243, so that the model training server 210, the model training server 220, and the model training server 230 use <maths id="math0044" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0044" file="imgb0044.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the (j + 1)<sup>th</sup> round of training.<!-- EPO <DP n="15"> --></p>
<p id="p0098" num="0098">In an example, <maths id="math0045" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0045" file="imgb0045.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> may be calculated by the at least one data processor 241 in the parameter server 240, and <maths id="math0046" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0046" file="imgb0046.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> is stored in the memory 243. In the FP calculation in the (j + 1)<sup>th</sup> iteration, the plurality of model training servers may directly obtain (pull) <maths id="math0047" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0047" file="imgb0047.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> from the memory 243. In another example, <maths id="math0048" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0048" file="imgb0048.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> may be calculated by the processor 212 in the model training server 210. In the FP calculation in the (j + 1)<sup>th</sup> iteration, the iterative processor 212 pulls the calculated gradient average value <maths id="math0049" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0049" file="imgb0049.tif" wi="6" he="7" img-content="math" img-format="tif" inline="yes"/></maths> from the memory 243, calculates <maths id="math0050" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0050" file="imgb0050.tif" wi="9" he="8" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter matrix in the (j + 1)<sup>th</sup> iteration based on <maths id="math0051" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0051" file="imgb0051.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> and stores <maths id="math0052" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0052" file="imgb0052.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the memory 243, so that the model training server 210 uses <maths id="math0053" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0053" file="imgb0053.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the (j + 1)<sup>th</sup> round of training.</p>
<p id="p0099" num="0099">Optionally, in some embodiments, the parameter server 240 may further include an iterative processor 242, and a deep learning model 100 may run in the iterative processor 242. In an example, the iterative processor 242 may be an NPU or a GPU.</p>
<p id="p0100" num="0100">It should be noted that the iterative processor 242 may also calculate the gradient average value <maths id="math0054" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0054" file="imgb0054.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths> based on the gradients <maths id="math0055" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0055" file="imgb0055.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, of the weights, that are separately sent by the model training server 210, the model training server 220, and the model training server 230, and store the calculated gradient average value <maths id="math0056" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0056" file="imgb0056.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the memory 243. The iterative processor 242 may further calculate <maths id="math0057" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0057" file="imgb0057.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter matrix in the (j + 1)<sup>th</sup> iteration based on <maths id="math0058" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0058" file="imgb0058.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> and store <maths id="math0059" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0059" file="imgb0059.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the memory 243, so that the model training server 210, the model training server 220, and the model training server 230 use <maths id="math0060" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0060" file="imgb0060.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the (j + 1)<sup>th</sup> round of training.</p>
<heading id="h0007">(3) Cloud storage 250:</heading>
<p id="p0101" num="0101">Optionally, in some embodiments, the system 200 may further include the cloud storage 250. The cloud storage 250 may be used as an external memory, and a user may store the program code and the training data in the external memory. The model training server 210 is used as an example. In a running process, the at least one processor may first store, in the memory 213, the program code and data that are stored in the cloud storage 250, so that the at least one processor may obtain the program code and the training data from the memory 213, and train the deep learning model 100 based on the program code and the training data.</p>
<p id="p0102" num="0102">It should be noted that the data stored in the cloud storage 250 may include the training data, the prior knowledge corresponding to the training data, an initial value of the parameter matrix corresponding to the neuron at each layer of the hidden layer 120 of each deep learning training model 100, and the like.</p>
<p id="p0103" num="0103">With reference to <figref idref="f0004">FIG. 3</figref>, the following further describes in detail a process of communication between the model training servers and the parameter server 240 in the system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>.</p>
<p id="p0104" num="0104">It should be noted that, for ease of description, an internal structural diagram of the plurality of model training servers and the parameter server 240 is not drawn in detail in <figref idref="f0004">FIG. 3</figref>. For details, refer to the descriptions in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>. Details are not described herein again.<!-- EPO <DP n="16"> --></p>
<p id="p0105" num="0105">Referring to <figref idref="f0004">FIG. 3</figref>, training processes of performing the j<sup>th</sup> iteration and the (j + 1)<sup>th</sup> iteration on the deep learning model 100 are used as an example. In the training process of the j<sup>th</sup> iteration, in the BP calculation, the at least one data processor 211 in the model training server 210 may push, to the memory 243 in the parameter server 240, a gradient <maths id="math0061" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>1</mn></mfenced></math><img id="ib0061" file="imgb0061.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to a neuron that is at the i<sup>th</sup> hidden layer and that is calculated by the at least one iterative processor 212. Similarly, in the BP calculation, at least one data processor in the model training server 220 may push a calculated gradient <maths id="math0062" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>2</mn></mfenced></math><img id="ib0062" file="imgb0062.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> to the memory 243 in the parameter server 240, and at least one data processor in the model training server 230 may push a calculated gradient <maths id="math0063" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>3</mn></mfenced></math><img id="ib0063" file="imgb0063.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> to the memory 243 in the parameter server 240.</p>
<p id="p0106" num="0106">The at least one iterative processor 242 in the parameter server 240 may obtain the stored <maths id="math0064" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>1</mn></mfenced></math><img id="ib0064" file="imgb0064.tif" wi="10" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, <maths id="math0065" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>2</mn></mfenced></math><img id="ib0065" file="imgb0065.tif" wi="10" he="8" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> <maths id="math0066" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>3</mn></mfenced></math><img id="ib0066" file="imgb0066.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the memory 243, calculate a gradient average value <maths id="math0067" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0067" file="imgb0067.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> based on <maths id="math0068" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>1</mn></mfenced></math><img id="ib0068" file="imgb0068.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> <maths id="math0069" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>2</mn></mfenced></math><img id="ib0069" file="imgb0069.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> and <maths id="math0070" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>3</mn></mfenced></math><img id="ib0070" file="imgb0070.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> and store <maths id="math0071" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0071" file="imgb0071.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter storage space of the memory 243, so that the model training server 210, the model training server 220, and the model training server 230 use <maths id="math0072" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0072" file="imgb0072.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the (j + 1)<sup>th</sup> round of training. For a specific <maths id="math0073" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0073" file="imgb0073.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths> calculation process, refer to the embodiment corresponding to <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>.</p>
<p id="p0107" num="0107">Therefore, parameters stored in the parameter storage space of the memory 243 include <maths id="math0074" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>1</mn></mfenced></math><img id="ib0074" file="imgb0074.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> <maths id="math0075" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>2</mn></mfenced></math><img id="ib0075" file="imgb0075.tif" wi="10" he="7" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> <maths id="math0076" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>3</mn></mfenced></math><img id="ib0076" file="imgb0076.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, and <maths id="math0077" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0077" file="imgb0077.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths>.</p>
<p id="p0108" num="0108">Optionally, in some embodiments, the at least one iterative processor 242 may further obtain the stored <maths id="math0078" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0078" file="imgb0078.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the memory 243, calculate <maths id="math0079" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0079" file="imgb0079.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter matrix in the (j + 1)<sup>th</sup> iteration based on <maths id="math0080" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0080" file="imgb0080.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, and store <maths id="math0081" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0081" file="imgb0081.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the memory 243, so that the model training server 210 conveniently performs the BP calculation based on <maths id="math0082" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0082" file="imgb0082.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the (j + 1)<sup>th</sup> round of training. Therefore, in some embodiments, the parameter storage space of the memory 243 further stores <maths id="math0083" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0083" file="imgb0083.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths><sub>.</sub></p>
<p id="p0109" num="0109">In a BP calculation process of performing (j + 1)<sup>th</sup> iterative training on the deep learning model 100, the plurality of model training servers may obtain the stored parameters from the parameter server, and calculate a predicted output value by using an input value (the training data) and the parameter matrix <maths id="math0084" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0084" file="imgb0084.tif" wi="9" he="8" img-content="math" img-format="tif" inline="yes"/></maths>. In an example, in the BP calculation, the model training server 210 pulls the stored <maths id="math0085" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0085" file="imgb0085.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the memory 243 in the parameter server 240, calculates the parameter matrix <maths id="math0086" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0086" file="imgb0086.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the neuron at the i<sup>th</sup> hidden layer in the (j + 1)<sup>th</sup> iteration based on <maths id="math0087" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0087" file="imgb0087.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> and calculates the predicted output value by using the input value and the parameter matrix <maths id="math0088" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0088" file="imgb0088.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths>. Similarly, in the BP calculation, the model training server 220 pulls stored <maths id="math0089" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0089" file="imgb0089.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter server 240. In addition, in the BP calculation, the model training server 230 pulls stored <maths id="math0090" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0090" file="imgb0090.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter server 240. In another example, if the memory 243 in the parameter server 240 stores <maths id="math0091" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0091" file="imgb0091.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, in the BP calculation, the model training server 210, the model training server 220, and the model training server 230 may separately pull the stored <maths id="math0092" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0092" file="imgb0092.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter server 242.<!-- EPO <DP n="17"> --></p>
<p id="p0110" num="0110">With reference to <figref idref="f0005">FIG. 4</figref>, the following describes in detail a scenario of training the deep learning model 100 by using an example in which a distributed training system includes one model training server, the model training server includes at least one processor, and the processor may be used as one computing node.</p>
<p id="p0111" num="0111"><figref idref="f0005">FIG. 4</figref> is a schematic structural diagram of a distributed training system 400 of the deep learning model 400 according to an embodiment of this application. As shown in <figref idref="f0005">FIG. 4</figref>, the distributed training system 400 may include a model training server 410.</p>
<p id="p0112" num="0112">The model training server 410 may include at least one processor, a memory 414, an input/output interface 415, a communications interface 416, and a bus 417.</p>
<p id="p0113" num="0113">The at least one processor may be connected to the memory 414. The memory 414 may be configured to store program code and training data. The at least one processor may obtain the program code and the training data from the memory 414, and train the deep learning model 100.</p>
<p id="p0114" num="0114">The at least one processor may include two types of processors. One type of processor includes at least one data processor 411, and the other type of processor includes at least one iterative processor. As an example instead of a limitation, the data processor 411 may be a CPU, and the iterative processor may be an NPU or a GPU.</p>
<p id="p0115" num="0115">It should be understood that the model training server 410 may include at least one iterative processor. For ease of description, an iterative processor 412 and an iterative processor 413 are used as examples for description in <figref idref="f0005">FIG. 4</figref>.</p>
<p id="p0116" num="0116">A deep learning model 100 runs in each of the iterative processor 412 and the iterative processor 413, and in BP calculation performed on the deep learning model 100, each of the iterative processor 412 and the iterative processor 413 may calculate a gradient <maths id="math0093" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0093" file="imgb0093.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> of a weight in a parameter matrix of a neuron at each layer, and store the calculated gradient <maths id="math0094" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0094" file="imgb0094.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the memory 414 through the bus 417.</p>
<p id="p0117" num="0117">Optionally, in some embodiments, a feedback module 4121 may further run in the iterative processor 412, and similarly, a feedback module 4131 may further run in the iterative processor 413. For details about the feedback modules running in the iterative processor 412 and the iterative processor 413, refer to descriptions in <figref idref="f0007">FIG. 6</figref>. Details are not described herein.</p>
<p id="p0118" num="0118">A gradient communications module 4111, a gradient update module 4112, and a correction module 4113 may run in the at least one data processor 411. For details about the modules running in the data processor 411, refer to descriptions in <figref idref="f0007">FIG. 6</figref>. Details are not described herein.</p>
<p id="p0119" num="0119">After obtaining, from the memory 414 through the bus 417, the gradient <maths id="math0095" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0095" file="imgb0095.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> stored in each of the iterative processor 412 and the iterative processor 413, the at least one data processor 411 may further calculate a gradient average value <maths id="math0096" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0096" file="imgb0096.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> based on a plurality of gradients <maths id="math0097" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0097" file="imgb0097.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> calculated by the iterative processor 412 and the<!-- EPO <DP n="18"> --> iterative processor 413, and store the gradient average value <maths id="math0098" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0098" file="imgb0098.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the memory 414 through the bus 417, so that the iterative processor 412 and the iterative processor 413 use <maths id="math0099" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0099" file="imgb0099.tif" wi="6" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the (j + 1)<sup>th</sup> round of training.</p>
<p id="p0120" num="0120">Specifically, in a BP calculation process in the j<sup>th</sup> iteration, the iterative processor 412 may calculate a gradient <maths id="math0100" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>1</mn></mfenced></math><img id="ib0100" file="imgb0100.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to a neuron at each layer in the deep learning model 100, and store the calculated gradient <maths id="math0101" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>1</mn></mfenced></math><img id="ib0101" file="imgb0101.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the neuron at the i<sup>th</sup> hidden layer in parameter storage space of the memory 414 through the bus 417. Likewise, the iterative processor 413 may also store, in the parameter storage space of the memory 414 through the bus 417, a calculated gradient <maths id="math0102" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>2</mn></mfenced></math><img id="ib0102" file="imgb0102.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the neuron at the i<sup>th</sup> hidden layer. The data processor 411 may obtain the stored gradients <maths id="math0103" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>1</mn></mfenced></math><img id="ib0103" file="imgb0103.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> and <maths id="math0104" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>2</mn></mfenced></math><img id="ib0104" file="imgb0104.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter storage space of the memory 414 through the bus 417, calculate, based on the gradients <maths id="math0105" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>1</mn></mfenced></math><img id="ib0105" file="imgb0105.tif" wi="10" he="6" img-content="math" img-format="tif" inline="yes"/></maths> and <maths id="math0106" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup><mfenced><mn>2</mn></mfenced></math><img id="ib0106" file="imgb0106.tif" wi="10" he="7" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> a gradient average value <maths id="math0107" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0107" file="imgb0107.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the neuron at the i<sup>th</sup> hidden layer, and store <maths id="math0108" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0108" file="imgb0108.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter storage space of the memory 414 through the bus 417. In an FP calculation process in the (j + 1)<sup>th</sup> iteration, each of the iterative processor 412 and the iterative processor 413 may obtain the gradient average value <maths id="math0109" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0109" file="imgb0109.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter storage space of the memory 414 through the bus 417, calculate <maths id="math0110" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0110" file="imgb0110.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in a parameter matrix in the (j + 1)<sup>th</sup> iteration based on the gradient average value <maths id="math0111" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0111" file="imgb0111.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, and perform the FP calculation based on the corrected parameter matrix <maths id="math0112" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0112" file="imgb0112.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths><sub>.</sub></p>
<p id="p0121" num="0121">Optionally, in some embodiments, the data processor 411 may further obtain the stored gradient average value <maths id="math0113" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0113" file="imgb0113.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter storage space of the memory 414 through the bus 417, calculate <maths id="math0114" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0114" file="imgb0114.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter matrix in the (j + 1)<sup>th</sup> iteration based on the gradient average value <maths id="math0115" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0115" file="imgb0115.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> and store <maths id="math0116" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0116" file="imgb0116.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter storage space of the memory 414. In this way, in the (j + 1)<sup>th</sup> iteration, each of the iterative processor 412 and the iterative processor 413 may obtain <maths id="math0117" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0117" file="imgb0117.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, through the bus 417, from the parameter storage space of the memory 414, and perform the FP calculation.</p>
<p id="p0122" num="0122">Optionally, in some embodiments, the distributed training system 400 further includes a cloud storage 420. The cloud storage 420 is connected to the model training server 410. The cloud storage 420 may be used as an external memory, and a user may store the program code and the training data in the external memory. In a running process, the at least one processor of the model training server 410 may first store, in the memory 414, the program code and the training data that are stored in the cloud storage 420, so that the at least one processor may obtain the program code and the training data from the memory 414, and perform iterative training on the deep learning model 100 based on the program code and the training data.</p>
<p id="p0123" num="0123">The foregoing <figref idref="f0002 f0003 f0004 f0005">FIG. 2A to FIG. 4</figref> describe in detail the process of training the deep learning model in the distributed training system 200 or the distributed training system 400. In BP calculation in a current iteration, each deep learning model calculates the gradient <maths id="math0118" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0118" file="imgb0118.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> of the neuron at each layer in a direction from a neuron at the n<sup>th</sup> layer to the neuron at the first layer, and pushes the calculated gradient <maths id="math0119" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0119" file="imgb0119.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> of the neuron at each layer to the parameter<!-- EPO <DP n="19"> --> storage space. Generally, in the deep learning model, a larger parameter matrix dimension closer to a neuron at the output layer indicates a larger gradient value corresponding to the parameter matrix and a longer time required for sending the gradient value to the parameter storage space. In FP calculation in a next iteration, the deep learning models sequentially start to obtain the stored parameter average value <maths id="math0120" num=""><math display="inline"><msubsup><mi>G</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0120" file="imgb0120.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> or the parameter matrix <maths id="math0121" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0121" file="imgb0121.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter storage space in a direction from the neuron at the first layer to the neuron at the n<sup>th</sup> layer. Therefore, in the FP calculation in the next iteration, before starting the FP calculation in the next iteration the deep learning model needs to wait for a gradient corresponding to a parameter matrix on the neuron at the first layer to be transmitted to the parameter storage space. In the BP calculation in the current iteration, if the gradient is sequentially sent to the parameter storage space in a sequence ( <maths id="math0122" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0122" file="imgb0122.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> to <maths id="math0123" num=""><math display="inline"><msubsup><mi>g</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0123" file="imgb0123.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths>) of generating the gradient of the neuron at each layer, a time for starting the next iteration by the deep learning model is relatively long, and iterative training efficiency of the deep learning model is relatively low.</p>
<p id="p0124" num="0124">According to a deep learning model training method provided in the embodiments of this application, a sequence of transmitting <maths id="math0124" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0124" file="imgb0124.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> obtained through the BP calculation to the parameter storage space in the current iteration process can be adjusted, to reduce a communication time of each iteration of the deep learning training model, and increase training efficiency of the deep learning model.</p>
<p id="p0125" num="0125">With reference to <figref idref="f0006 f0007 f0008">FIG. 5 to FIG. 7</figref>, the following further describes in detail an acceleration training process of a deep learning model in the embodiments of this application.</p>
<p id="p0126" num="0126"><figref idref="f0006">FIG. 5</figref> is a schematic flowchart of a deep learning model training method according to an embodiment of this application. The method may include steps 510 to 550. The following describes steps 510 to 550 in detail.</p>
<p id="p0127" num="0127">Step 510: N deep learning models respectively generate N first gradient sets in BP calculation in the j<sup>th</sup> iteration.</p>
<p id="p0128" num="0128">In this embodiment of this application, a training system may include N deep learning models, where N is a positive integer greater than 0. A training process of each deep learning model may include a plurality of iterations, and a training process of each iteration may include FB calculation and BP calculation.</p>
<p id="p0129" num="0129">It should be understood that the training system may be the distributed training system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> or the distributed training system 400 shown in <figref idref="f0005">FIG. 4</figref>.</p>
<p id="p0130" num="0130">In the BP calculation of the j<sup>th</sup> iteration, each deep learning model calculates a gradient corresponding to a neuron at each layer in a direction from a neuron at the n<sup>th</sup> layer to a neuron at the first layer, to form the first gradient set. A gradient corresponding to a neuron at the i<sup>th</sup> layer is <maths id="math0125" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0125" file="imgb0125.tif" wi="4" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> and i is a positive integer greater than 0 and less than or equal to n. Any first gradient set may include a gradient <maths id="math0126" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0126" file="imgb0126.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> of a parameter matrix corresponding to a neuron at each of the n layers in the deep learning model, that is, include n gradients <maths id="math0127" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0127" file="imgb0127.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths>.<!-- EPO <DP n="20"> --></p>
<p id="p0131" num="0131">Step 520: Determine a gradient adjustment policy based on training meta information.</p>
<p id="p0132" num="0132">Before step 510 starts, an adjustment submodule in a gradient communications module may determine the gradient adjustment policy based on a parameter included in the training meta information entered by a user, so that a communications submodule in the gradient communications module separately sends, according to the determined gradient adjustment policy, the gradient included in each of the N first gradient sets to parameter storage space.</p>
<p id="p0133" num="0133">The training meta information may include any one of the following parameters: a communication bandwidth between the deep learning model and the parameter storage space, a value of the gradient corresponding to the parameter matrix of the neuron at each layer of the deep learning model, and a time required by the neuron at each layer of the deep learning model in FP calculation.</p>
<p id="p0134" num="0134">It should be understood that the gradient communication policy may be determined based on at least one of the foregoing parameters. The gradient communication policy includes a sequence of transmitting gradients in the first gradient set to a parameter storage area. The following provides detailed descriptions with reference to <figref idref="f0007">FIG. 6</figref> and <figref idref="f0008">FIG. 7</figref>, and details are not described herein.</p>
<p id="p0135" num="0135">It should be noted that step 510 and step 520 are not subject to a sequence. Step 510 may be performed before step 520, or step 520 may be performed before step 510, or step 510 and step 520 may be performed at the same time. This is not specifically limited in this embodiment of this application.</p>
<p id="p0136" num="0136">Step 530: In a process of generating the first gradient sets, adjust a sequence of transmitting <maths id="math0128" num=""><math display="inline"><msubsup><mi>g</mi><mi>a</mi><mi>j</mi></msubsup></math><img id="ib0128" file="imgb0128.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to a neuron at the a<sup>th</sup> layer to the parameter storage space.</p>
<p id="p0137" num="0137">In this embodiment of this application, in the process of generating the first gradient sets in step 510, according to the gradient communication policy determined in step 520,a sequenceof sending a gradient corresponding to a parameter matrix of the neuron at the a<sup>th</sup> layer is adjusted to be before a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the b<sup>th</sup> layerto the parameter storage space, where b is less than or equal to n, a is less than b, and a is a positive integer greater than 0.</p>
<p id="p0138" num="0138">For example, the gradient communication policy indicates to send the gradients to the parameter storage space in a sequence of <maths id="math0129" num=""><math display="inline"><msubsup><mi>g</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mi>j</mi></msubsup></math><img id="ib0129" file="imgb0129.tif" wi="8" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>,</i> <maths id="math0130" num=""><math display="inline"><msubsup><mi>g</mi><mrow><mi>n</mi><mo>−</mo><mn>2</mn></mrow><mi>j</mi></msubsup></math><img id="ib0130" file="imgb0130.tif" wi="8" he="6" img-content="math" img-format="tif" inline="yes"/></maths>, ..., <maths id="math0131" num=""><math display="inline"><msubsup><mi>g</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0131" file="imgb0131.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths>, and <maths id="math0132" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0132" file="imgb0132.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths><i>.</i> In this case, <maths id="math0133" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0133" file="imgb0133.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> may not be transmitted temporarily after <maths id="math0134" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0134" file="imgb0134.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> is generated, <maths id="math0135" num=""><math display="inline"><msubsup><mi>g</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mi>j</mi></msubsup></math><img id="ib0135" file="imgb0135.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> is transmitted after <maths id="math0136" num=""><math display="inline"><msubsup><mi>g</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mi>j</mi></msubsup></math><img id="ib0136" file="imgb0136.tif" wi="8" he="6" img-content="math" img-format="tif" inline="yes"/></maths> is generated, <maths id="math0137" num=""><math display="inline"><msubsup><mi>g</mi><mrow><mi>n</mi><mo>−</mo><mn>2</mn></mrow><mi>j</mi></msubsup></math><img id="ib0137" file="imgb0137.tif" wi="8" he="6" img-content="math" img-format="tif" inline="yes"/></maths> is transmitted after <maths id="math0138" num=""><math display="inline"><msubsup><mi>g</mi><mrow><mi>n</mi><mo>−</mo><mn>2</mn></mrow><mi>j</mi></msubsup></math><img id="ib0138" file="imgb0138.tif" wi="8" he="7" img-content="math" img-format="tif" inline="yes"/></maths> is generated, and so on. After <maths id="math0139" num=""><math display="inline"><msubsup><mi>g</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0139" file="imgb0139.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> is generated and then transmitted to the parameter storage space, the previously generated <maths id="math0140" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0140" file="imgb0140.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths> is sent to the parameter storage space. Therefore, adjustment of a transmission sequence of the gradients included in the first gradient set does not need to be performed after all the gradients in the first gradient set are generated.</p>
<p id="p0139" num="0139">The distributed training system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> is used as an example. In the iterative<!-- EPO <DP n="21"> --> processor 212 in the model training server 210, a deep learning module of the deep learning model 100 may generate the first gradient sets in the BP calculation in the j<sup>th</sup> iteration, and store, in the process of generating the first gradient sets, the calculated gradient corresponding to the parameter matrix of the neuron in the memory 213. The gradient communications module in the data processor 211 is configured to determine the gradient communication policy. The gradient communication policy is used to indicate a communication sequence of the gradients included in each first gradient set. The gradient communications mudole adjusts, according to the determined gradient communication policy, a sequenc of sending the gradient that corresponds to the parameter matrix of the neuron at the a<sup>th</sup> layer, that is included in the first gradient set, and that is stored in the memory 213 to the parameter storage space in the memory 243 in the parameter sever 243 to be before a sequence of sending the gradient corresponding to the parameter matrix of the neuron at the b<sup>th</sup> layer to the parameter storage space.</p>
<p id="p0140" num="0140">The model training server 410 shown in <figref idref="f0005">FIG. 4</figref> is used as an example. In the iterative processor 412 in the data processor 411 in the model training server 410, a deep learning module of the deep learning model 100 may generate the first gradient sets in the BP calculation in the j<sup>th</sup> iteration, and store, in the process of generating the first gradient sets, the calculated gradient corresponding to the parameter matrix of the neuron in a memory 4121. The gradient communications module in the data processor 411 adjusts, according to the determined gradient communication policy, a seuqence of sending the gradient that corresponds to the parameter matrix of the neuron at the a<sup>th</sup> layer, that is included in the first gradient set, and that is stored in the memory 4121 to the parameter storage space of the memory 414 to be before a sequence of seding the gradient corresponding to the parameter matrix of the neuron at the b<sup>th</sup> layer to the parameter storage space.</p>
<p id="p0141" num="0141">It should be understood that, in the system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>, the gradient communications module may be a set of gradient communications modules of the at least one model training server in the system 200.</p>
<p id="p0142" num="0142">It should be further understood that the gradient communications module may include two submodules. One submodule is the adjustment submodule, configured to determine the gradient adjustment policy. The other submodule is the communications submodule, configured to separately send, to the parameter storage space according to the gradient adjustment policy, the gradients included in each of the N first gradient sets.</p>
<p id="p0143" num="0143">Step 540: In a (j + 1)<sup>th</sup> iteration process, each deep learning model obtains a second gradient set from the parameter storage space.</p>
<p id="p0144" num="0144">The system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> is used as an example. The gradient update module 2411 in the data processor 241 in the parameter server 240 obtains the second gradient set based on the N first gradient sets stored in the parameter storage space of the memory 243.<!-- EPO <DP n="22"> --></p>
<p id="p0145" num="0145">The model training server 410 shown in <figref idref="f0005">FIG. 4</figref> is used as an example. The gradient update module 2411 in the data processor 411 in the model training server 410 obtains the second gradient set based on the N first gradient sets stored in the parameter storage space of the memory 414.</p>
<p id="p0146" num="0146">In this embodiment of this application, after the gradients included in each of the N first gradient sets are separately sent to the parameter storage space of the training system according to an adjusted communication sequence of the gradients included in each first gradient set, an average value of gradients corresponding to the parameter matrix of the neuron at each layer of each of the N deep learning models can be calculated.</p>
<p id="p0147" num="0147">As an example, weighted average calculation may be performed on the gradients of the neuron at each layer included in each of the N first gradient sets, so that the average value of the gradients corresponding to the parameter matrix of the neuron at each layer of each of the N deep learning models can be calculated. Average values of gradients corresponding to parameter matrices of neurons at all layers constitute the second gradient set. In other words, the second gradient set includes the average values of the gradients corresponding to the parameter matrices of the neurons at all the layers of the N deep learning models.</p>
<p id="p0148" num="0148">Step 550: Each deep learning model performs the FP calculation based on the second gradient set in the (j + 1)<sup>th</sup> iteration.</p>
<p id="p0149" num="0149">In this embodiment of this application, the model training system may include a correction module, configured to correct a parameter matrix of a neuron at each layer of any one of the deep learning models based on a gradient included in the second gradient set, so that the parameter matrix can be used in the FP calculation in the (i + 1)<sup>th</sup> iteration of the any one of the deep learning models of the training system.</p>
<p id="p0150" num="0150">The system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> is used as an example. The correction module may be in the data processor 241 of the parameter server 240 or in the data processor 211 of the model training server 210.</p>
<p id="p0151" num="0151">Optionally, in some embodiments, the data processor 211 of the model training server 210 includes the correction module 2112. The gradient communications module 2111 of the model training server 210 may obtain the second gradient set from the parameter storage space in the memory 243, and correct the parameter matrix of the neuron at each layer of the deep learning model based on the average value of the gradients of the parameter matrix corresponding to the neuron at each layer that are in the second gradient set. In this way, in BP calculation in a next iteration, an input and an output corresponding to the neuron at each layer are calculated based on a corrected parameter matrix <maths id="math0141" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0141" file="imgb0141.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths>.</p>
<p id="p0152" num="0152">Optionally, in some other embodiments, the data processor 241 in the parameter server 240 includes the correction module 2411. The correction module 2411 may obtain the second gradient set from the parameter storage space in the memory 243, correct the parameter matrix of the neuron at each layer of the deep learning model based<!-- EPO <DP n="23"> --> on the average value of the gradients of the parameter matrix corresponding to the neuron at each layer that are in the second gradient set, and store a corrected set including the parameter matrix <maths id="math0142" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0142" file="imgb0142.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter storage space of the memory 243. In this way, in the BP calculation in the next iteration, the gradient communications module 2111 of the model training server 210 may obtain the corrected set including the parameter matrix <maths id="math0143" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0143" file="imgb0143.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter storage space in the memory 243, and calculate, based on <maths id="math0144" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0144" file="imgb0144.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the set, the input and the output corresponding to the neuron at each layer.</p>
<p id="p0153" num="0153">It should be noted that, in the system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>, the correction module may be a set of correction modules of the at least one model training server in the system 200.</p>
<p id="p0154" num="0154">The model training server 410 shown in <figref idref="f0005">FIG. 4</figref> is used as an example. The data processor 411 in the model training server 410 may include the correction module 4113. The correction module 4113 may obtain the second gradient set from the parameter storage space in the memory 414, correct the parameter matrix of the neuron at each layer of the deep learning model based on the average value of the gradients of the parameter matrix corresponding to the neuron at each layer that are in the second gradient set, and store the corrected set including the parameter matrix <maths id="math0145" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0145" file="imgb0145.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the parameter storage space of the memory 414. In this way, in the BP calculation in the next iteration, the gradient communications module 4111 of the model training server 410 may obtain the corrected set including the parameter matrix <maths id="math0146" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0146" file="imgb0146.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the parameter storage space in the memory 414, and calculate, based on <maths id="math0147" num=""><math display="inline"><msubsup><mi>w</mi><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><img id="ib0147" file="imgb0147.tif" wi="9" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the set, the input and the output corresponding to the neuron at each layer.</p>
<p id="p0155" num="0155">In this embodiment of this application, a sequence of transmitting <maths id="math0148" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0148" file="imgb0148.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> obtained through the BP calculation to the parameter storage space in a current iteration process may be adjusted without affecting training convergence precision of the deep learning model, to reduce a communication time of current iteration, and increase model training efficiency.</p>
<p id="p0156" num="0156">The gradient communication policy in an iteration process of each deep learning model is not specifically limited in this embodiment of this application. The gradient communication policy may be set according to an empirical rule, or may be a gradient communication policy compatible with another manner, for example, an intelligent gradient communication policy based on reinforcement learning. With reference to <figref idref="f0007">FIG. 6</figref>, the following describes in detail a specific implementation of adjusting a communication sequence of the n gradients <maths id="math0149" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0149" file="imgb0149.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> included in the first gradient set.</p>
<p id="p0157" num="0157"><figref idref="f0007">FIG. 6</figref> is a schematic architectural diagram of a deep learning model training system according to an embodiment of this application. The system architecture may include a user side and a cloud platform side.</p>
<p id="p0158" num="0158">As shown in <figref idref="f0007">FIG. 6</figref>, the user side may input at least one of a deep learning model 100, training meta information 660, and training data 670 to the cloud platform side through an interface.<!-- EPO <DP n="24"> --></p>
<p id="p0159" num="0159">The training meta information 610 may include a communication bandwidth between the deep learning model 100 and parameter storage space 640, a value of a gradient corresponding to a parameter matrix of a neuron at each layer of the deep learning model 100, and a time required by the neuron at each layer of the deep learning model 100 in FP calculation. The training data 670 may include training data used as an input and a prediction result corresponding to training data provided by a person.</p>
<p id="p0160" num="0160">It should be noted that the deep learning model 100 may be sent by the user side to the cloud platform side through the interface, or may be a model stored on the cloud platform side. This is not specifically limited in this application.</p>
<p id="p0161" num="0161">The cloud platform side may include a gradient communications module 620, a local memory 630, the parameter storage space 640, and a deep learning model 100.</p>
<p id="p0162" num="0162">Optionally, in some embodiments, the cloud platform side may further include a cloud storage 610. The cloud storage 610 may store the deep learning model 100, the training meta information 660, and the training data 670 that are sent by the user side.</p>
<p id="p0163" num="0163">Optionally, in some embodiments, the cloud platform side may further include a feedback module 650.</p>
<p id="p0164" num="0164">Referring to <figref idref="f0007">FIG. 6</figref>, the gradient communications module 620 on the cloud platform side may include an adjustment submodule 621 and a communications submodule 622. The adjustment submodule 621 may be configured to perform the method in step 520, and the communications submodule 622 may be configured to perform the method in step 530. The feedback module 650 on the cloud platform side may be configured to perform the method in step 540.</p>
<p id="p0165" num="0165">It should be understood that the platform side may correspond to the distributed training system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> or the distributed training system 400 shown in <figref idref="f0005">FIG. 4</figref>.</p>
<p id="p0166" num="0166">The following uses the distributed training system 200 shown in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref> as an example to describe in detail an iteration process of the deep learning model 100 on the cloud platform side shown in <figref idref="f0007">FIG. 6</figref>.</p>
<p id="p0167" num="0167">It should be noted that, for <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>, the gradient communications module 620 shown in <figref idref="f0007">FIG. 6</figref> corresponds to a set of the gradient communications module 2111 in the model training server 210 and gradient communications modules running in the model training server 220 and the model training server 230 in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>. The feedback module 650 corresponds to a set of the feedback module 2121 in the model training server 210 and feedback modules running in the model training server 220 and the model training server 230 in <figref idref="f0002">FIG. 2A</figref> and <figref idref="f0003">FIG. 2B</figref>. For <figref idref="f0005">FIG. 4</figref>, the feedback module 650 corresponds to a set of the feedback module 4121 in the iterative processor 412 and the feedback module 4131 in the iterative processor 413 in <figref idref="f0005">FIG. 4</figref>.</p>
<p id="p0168" num="0168">The adjustment submodule 621 may determine a gradient communication policy based on the training<!-- EPO <DP n="25"> --> meta information 660. For details, refer to the descriptions in step 520. Details are not described herein again.</p>
<p id="p0169" num="0169">When the deep learning model 100 starts training, a deep learning module of the deep learning model 100 obtains the training data 670 from the cloud storage 610, and starts iterative training of the model based on the training data 670. In BP calculation in a current iteration, the communications submodule 622 may be configured to perform the method in step 530. The communications submodule 622 may send, according to the gradient communication policy determined by the adjustment submodule 621, gradients stored in the local memory 630 to the parameter storage area 640 in an adjusted sequence. When starting FP calculation in a next iteration, the deep learning model 100 may obtain stored data from the parameter storage area 640, and start the FP calculation based on the data. For details, refer to the descriptions in FIG. 530, and details are not described herein again.</p>
<p id="p0170" num="0170">It should be understood that the local memory 630 is a set of the memory 213 in the model training server 210 and memories in the model training server 220 and the model training server 230 in the distributed training system 200.</p>
<p id="p0171" num="0171">It should be further understood that the parameter storage area 640 corresponds to the memory 243 in the parameter server 240 in the distributed training system 200.</p>
<p id="p0172" num="0172">The feedback module 650 may obtain an iteration time of the deep learning model 100, and the iteration time may include a time for the BP calculation in the current iteration of the deep learning model 100 and a time for the FP calculation in the next iteration of the deep learning model 100. For example, the feedback module 650 may obtain a time for BP calculation in the L<sup>th</sup> iteration of the deep learning model 100 and a time for FP calculation in the (L + 1)<sup>th</sup> iteration of the deep learning model 100, where L is a positive integer greater than j. The feedback module 650 may feed back the iteration time of the deep learning model 100 to the adjustment submodule 621 in the gradient communications module 620. After receiving the iteration time that is of the deep learning model 100 and that is fed back by the feedback module 650, the adjustment submodule 621 may adjust the determined gradient communication policy, so that a subsequent iterative training speed is faster. For details, refer to the descriptions in FIG. 540, and details are not described herein again.</p>
<p id="p0173" num="0173">The following further explains and describes the deep learning training model training method with reference to <figref idref="f0007">FIG. 6</figref> by using an example in which a facial recognition model is ResNet50 and a computing engine is TensorFlow.</p>
<p id="p0174" num="0174"><figref idref="f0008">FIG. 7</figref> is a schematic flowchart of a deep learning model training method according to an embodiment of this application. The method shown in <figref idref="f0008">FIG. 7</figref> may include steps 710 to 730. The following separately describes steps 710 to 730 in detail.</p>
<p id="p0175" num="0175">It should be noted that the examples in <figref idref="f0008">FIG. 7</figref> are merely intended to help a person skilled in the art<!-- EPO <DP n="26"> --> understand this embodiment of this application, instead of limiting this embodiment of this application to a specific value or a specific scenario shown in the examples. A person skilled in the art can apparently make various equivalent modifications or changes according to the examples shown in <figref idref="f0008">FIG. 7</figref>, and such modifications or changes also fall within the scope of the embodiments of this application.</p>
<p id="p0176" num="0176">Step 710: Initialize a gradient communication policy of a deep learning model.</p>
<p id="p0177" num="0177">The facial recognition model ResNet50 (a quantity of categories is 10,000) is used as an example. A quantity of parameters in a corresponding parameter matrix of a neuron at the last layer (a fully connected (fully connected, FC) layer) of the ResNet50 face model is about 78 MB, which accounts for about half of a size of the entire model. A larger calculated gradient corresponding to the neuron at this layer indicates a longer communication time required for sending the gradient to parameter storage space.</p>
<p id="p0178" num="0178">Therefore, during initialization, it is assumed that the ResNet50 face model has n layers of neurons. A sequence of sending a corresponding gradient <maths id="math0150" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0150" file="imgb0150.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths> on the neuron at the FC layer, namely, the last layer, of the model is adjusted to after a sequence of sending a gradient <maths id="math0151" num=""><math display="inline"><msubsup><mi>g</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0151" file="imgb0151.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to a parameter matrix of a neuron at the first layer to the parameter storage space, the gradient <maths id="math0152" num=""><math display="inline"><msubsup><mi>g</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0152" file="imgb0152.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the parameter matrix of the neuron at the first layer is transmitted to the parameter storage space. In other words, after the gradient <maths id="math0153" num=""><math display="inline"><msubsup><mi>g</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0153" file="imgb0153.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the parameter matrix of the neuron at the first layer is transmitted to the parameter storage space, FB calculation in a next iteration may be started, so that the gradient <maths id="math0154" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0154" file="imgb0154.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the parameter matrix of the neuron at the last layer is transmitted to the parameter storage space within a time period of the FB calculation in the next iteration.</p>
<p id="p0179" num="0179"><figref idref="f0008">FIG. 8(a)</figref> represents at least one deep learning model 100. Gradients <maths id="math0155" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0155" file="imgb0155.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> to <maths id="math0156" num=""><math display="inline"><msubsup><mi>g</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0156" file="imgb0156.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> included in a first gradient set obtained through BP calculation in the j<sup>th</sup> iteration are sequentially transmitted to the parameter storage space in a sequence from a neuron at the n<sup>th</sup> layer to the neuron at the first layer. t<sub>n</sub> represents a communication time required for transmitting the gradient <maths id="math0157" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0157" file="imgb0157.tif" wi="5" he="7" img-content="math" img-format="tif" inline="yes"/></maths> that corresponds to the neuron at the n<sup>th</sup> layer and that is obtained through the BP calculation to the parameter storage space, t<sub>n-1</sub> represents a communication time required for transmitting a gradient <maths id="math0158" num=""><math display="inline"><msubsup><mi>g</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mi>j</mi></msubsup></math><img id="ib0158" file="imgb0158.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> that corresponds to the neuron at the n<sup>th</sup> layer and that is obtained through the BP calculation to the parameter storage space, ..., and t<sub>1</sub> represents a communication time required for transmitting the gradient <maths id="math0159" num=""><math display="inline"><msubsup><mi>g</mi><mn>1</mn><mi>j</mi></msubsup></math><img id="ib0159" file="imgb0159.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the neuron at the first layer to the parameter storage space. A time for triggering the deep learning model to perform the FP calculation in the next iteration (the (j + 1)<sup>th</sup> iteration) is a time for transmitting, to the parameter storage space, the gradient corresponding to the neuron at the first layer in the BP calculation in the j<sup>th</sup> iteration. Referring to <figref idref="f0008">FIG. 8(a)</figref>, a time period from a time for obtaining <maths id="math0160" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0160" file="imgb0160.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> through the BP calculation in the j<sup>th</sup> iteration to a time for triggering the deep learning model to perform the FP calculation in the (j + 1)<sup>th</sup> iteration is not less than T<sub>1</sub>, where T<sub>1</sub> = t<sub>n</sub> + t<sub>n-1</sub> +... + t<sub>2</sub> + t<sub>1</sub>. To be specific, after a transmission time T<sub>1</sub>, the deep learning model can<!-- EPO <DP n="27"> --> perform the FP calculation in the next iteration.</p>
<p id="p0180" num="0180"><figref idref="f0008">FIG. 8(b)</figref> represents at least one deep learning model 100. According to a solution for acceleration training of a deep learning model provided in the embodiments of this application, in a process of obtaining corresponding gradients <maths id="math0161" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0161" file="imgb0161.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> from the neuron at the n<sup>th</sup> layer to the neuron at the first layer though the BP calculation, according to a communication scheduling policy, the sequence of sending the gradient <maths id="math0162" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0162" file="imgb0162.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the parameter matrix of the neuron at the n<sup>th</sup> layer is adjusted to be after a sequence of sending the gradient corresponding to the parameter matrix of the neuron at the first layer to the parameter storage space. A time for triggering the deep learning model to perform the FP calculation in the next iteration is a time for transmitting, to the parameter storage space, the gradient corresponding to the neuron at the first layer in the BP calculation in the j<sup>th</sup> iteration. Referring to <figref idref="f0008">FIG. 8(b)</figref>, a time period from a time for obtaining <maths id="math0163" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0163" file="imgb0163.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> through the BP calculation in the j<sup>th</sup> iteration to a time for triggering the deep learning model to perform the FP calculation in the (j + 1)<sup>th</sup> iteration is not less than T<sub>2</sub>, where T<sub>2</sub> = a time required for calculating <maths id="math0164" num=""><math display="inline"><msubsup><mi>g</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><mi>j</mi></msubsup></math><img id="ib0164" file="imgb0164.tif" wi="8" he="6" img-content="math" img-format="tif" inline="yes"/></maths> in the BP calculation in the j<sup>th</sup> iteration + t<sub>n-1</sub> + ... + t<sub>2</sub> + t<sub>1</sub>. To be specific, after the transmission time T<sub>2</sub>, the deep learning model can perform the FP calculation in the next iteration. After the time t<sub>1</sub>, the gradient <maths id="math0165" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0165" file="imgb0165.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> corresponding to the neuron at the n<sup>th</sup> layer is transmitted to the parameter storage space.</p>
<p id="p0181" num="0181">Generally, because performance of an iterative processor is improved day by day, a time required for obtaining a gradient through BP calculation is less than a time required for transmitting the calculated gradient to the parameter storage space. If the time required for obtaining the gradient through BP calculation is greater than the time required for transmitting the calculated gradient to the parameter storage space, a case in which the time period from the time for obtaining <maths id="math0166" num=""><math display="inline"><msubsup><mi>g</mi><mi>n</mi><mi>j</mi></msubsup></math><img id="ib0166" file="imgb0166.tif" wi="6" he="6" img-content="math" img-format="tif" inline="yes"/></maths> through the BP calculation in the j<sup>th</sup> iteration to the time for triggering the deep learning model to perform the FP calculation in the (j + 1)<sup>th</sup> iteration is greater than T<sub>1</sub> or T<sub>2</sub> is caused.</p>
<p id="p0182" num="0182">The time T<sub>2</sub> for triggering the deep learning model to perform the FP calculation in the next iteration in <figref idref="f0008">FIG. 8(b)</figref> is less than the time T<sub>1</sub> for triggering the deep learning model to perform the FP calculation in the next iteration in <figref idref="f0008">FIG. 8(a)</figref>. Therefore, according to the deep learning model training method provided in the embodiments of this application, a sequence of transmitting <maths id="math0167" num=""><math display="inline"><msubsup><mi>g</mi><mi>i</mi><mi>j</mi></msubsup></math><img id="ib0167" file="imgb0167.tif" wi="5" he="6" img-content="math" img-format="tif" inline="yes"/></maths> obtained through the BP calculation to the parameter storage space in a current iteration process can be adjusted, to reduce a communication time of a current iteration, and increase model training efficiency.</p>
<p id="p0183" num="0183">Step 720: Perform iterative training on the deep learning model.</p>
<p id="p0184" num="0184">The initialized gradient communication policy is written into the adjustment submodule 621 in the gradient communications module 620 in <figref idref="f0007">FIG. 6</figref>. The communications submodule 622 sends, to the parameter storage space 640 according to the gradient communication policy determined by the adjustment submodule 621, the gradient in the first gradient set stored in the local memory 630.<!-- EPO <DP n="28"> --></p>
<p id="p0185" num="0185">The feedback module 650 may obtain an iteration time of the deep learning model 100 from a deep learning module of the deep learning model 100. The iteration time may include a time for the BP calculation in the current iteration of the deep learning model 100 and the time for the FP calculation in the next iteration of the deep learning model 100. In addition, the iteration time of the deep learning model 100 may be fed back to the adjustment submodule 621 in the gradient communications module 620.</p>
<p id="p0186" num="0186">Step 730: Optimize the gradient communication policy in the deep learning model.</p>
<p id="p0187" num="0187">After receiving the iteration time that is of the deep learning model 100 and that is fed back by the feedback module 650, the adjustment submodule 621 may adjust the determined gradient communication policy, so that a subsequent iterative training speed is faster. After a plurality of times of iteration and adjustment of the gradient communication policy, an optimal gradient communication policy is found, and then the optimal gradient communication policy is used to perform the iterative training of the deep learning model 100 in a subsequent training step.</p>
<p id="p0188" num="0188">In this embodiment of this application, a training speed of the deep learning model is increased only by adjusting a communication sequence of gradients without affecting training convergence precision of the deep learning model. A current deep learning model basically depends on a BP algorithm (including a back propagation through time (back propagation through time, BPTT) algorithm), most open-source deep learning engines (such as the TensorFlow) are also implemented based on the BP algorithm. Therefore, the method for acceleration training of a deep learning model provided in the embodiments of this application has extensive applications.</p>
<p id="p0189" num="0189">All or some of the foregoing embodiments may be implemented by using software, hardware, firmware, or any combination thereof. When software is used to implement the embodiments, the foregoing embodiments may be implemented entirely or partially in a form of a computer program product. The computer program product includes one or more computer instructions. When the computer program instructions are loaded and executed on a computer, the procedure or functions according to the embodiments of this application are entirely or partially generated. The computer may be a general-purpose computer, a special-purpose computer, a computer network, or another programmable apparatus. The computer instructions may be stored in a computer-readable storage medium or may be transmitted from a computer-readable storage medium to another computer-readable storage medium. For example, the computer instructions may be transmitted from a website, computer, server, or data center to another website, computer, server, or data center in a wired (for example, a coaxial cable, an optical fiber, or a digital subscriber line (DSL)) or wireless (for example, infrared, radio, or microwave) manner. The computer-readable storage medium may be any usable medium accessible by a computer, or a data storage device, for example, a server or a data center, integrating one or more usable media. The usable medium may be a magnetic medium (for example, a floppy disk, a<!-- EPO <DP n="29"> --> hard disk, or a magnetic tape), an optical medium (for example, a DVD), or a semiconductor medium. The semiconductor medium may be a solid-state drive (solid state drive, SSD).</p>
<p id="p0190" num="0190">An embodiment of this application further provides a computer program product. The computer program product includes a program instruction, and when the program instruction is run on a computer, the computer is enabled to perform the methods in the foregoing aspects.</p>
<p id="p0191" num="0191">An embodiment of this application further provides a computer-readable medium. The computer-readable medium stores a program instruction, and when the program instruction is run on a computer, the computer is enabled to perform the methods in the foregoing aspects.</p>
<p id="p0192" num="0192">Aspects or features of this application may be implemented as a method, an apparatus or a product that uses standard programming and/or engineering technologies. The term "product" used in this application covers a computer program that can be accessed from any computer readable component, carrier or medium. For example, the computer-readable medium may include but is not limited to: a magnetic storage component (for example, a hard disk, a floppy disk or a magnetic tape), an optical disc (for example, a compact disc (compact disc, CD)), a digital versatile disc (digital versatile disc, DVD), a smart card and a flash memory component (for example, an erasable programmable read-only memory (erasable programmable read-only memory, EPROM), a card, a stick, or a key drive). In addition, various storage media described in this specification may indicate one or more devices and/or other machine-readable media that are configured to store information. The term "machine-readable media" may include but is not limited to a radio channel and various other media that can store, contain, and/or carry an instruction and/or data.</p>
<p id="p0193" num="0193">A person of ordinary skill in the art may be aware that, in combination with the examples described in the embodiments disclosed in this specification, units and algorithm steps may be implemented by electronic hardware or a combination of computer software and electronic hardware. Whether the functions are performed by hardware or software depends on particular applications and design constraints of the technical solutions. A person skilled in the art may use different methods to implement the described functions for each particular application, but it should not be considered that the implementation goes beyond the scope of this application.</p>
<p id="p0194" num="0194">It may be clearly understood by a person skilled in the art that, for the purpose of convenient and brief description, for a detailed working process of the foregoing system, apparatus, and unit, refer to a corresponding process in the foregoing method embodiments, and details are not described herein again.</p>
<p id="p0195" num="0195">In the several embodiments provided in this application, it should be understood that the disclosed system, apparatus, and method may be implemented in other manners. For example, the described apparatus embodiment is merely an example. For example, division into the units is merely logical function division and may be other division<!-- EPO <DP n="30"> --> in an actual implementation. For example, a plurality of units or components may be combined or integrated into another system, or some features may be ignored or not performed. In addition, the displayed or discussed mutual couplings or direct couplings or communication connections may be implemented through some interfaces. The indirect couplings or communication connections between the apparatuses or units may be implemented in electronic, mechanical, or other forms.</p>
<p id="p0196" num="0196">The units described as separate parts may or may not be physically separate, and parts displayed as units may or may not be physical units, may be located in one position, or may be distributed on a plurality of network units. Some or all of the units may be selected based on actual requirements to achieve the objectives of the solutions of the embodiments.</p>
<p id="p0197" num="0197">In addition, functional units in the embodiments of this application may be integrated into one processing unit, or each of the units may exist alone physically, or two or more units are integrated into one unit.</p>
<p id="p0198" num="0198">When the functions are implemented in the form of a software functional unit and sold or used as an independent product, the functions may be stored in a computer-readable storage medium. Based on such an understanding, the technical solutions of this application essentially, or the part contributing to the prior art, or some of the technical solutions may be implemented in a form of a software product. The computer software product is stored in a storage medium, and includes several instructions for indicating a computer device (which may be a personal computer, a server, or a network device) to perform all or some of the steps of the methods described in the embodiments of this application. The foregoing storage medium includes: any medium that can store program code, for example, a USB flash drive, a removable hard disk, a read-only memory (read-only memory, ROM), a random access memory (random access memory, RAM), a magnetic disk, or an optical disc.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="31"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A deep learning model training method, wherein a training system to which the method is applied comprises N deep learning models, each deep learning model comprises n layers of neurons, a training process of each deep learning model comprises a plurality of iterations, and each iteration comprises forward propagation FP calculation and back propagation BP calculation, N is a positive integer greater than 1, n is a positive integer greater than 1, and the method comprises:
<claim-text>generating N first gradient sets in BP calculation in the j<sup>th</sup> iteration of the N deep learning models, wherein each first gradient set comprises a gradient corresponding to a parameter matrix of a neuron at each layer of one deep learning model, and j is a positive integer greater than 0;</claim-text>
<claim-text>adjusting a communication sequence of the gradients comprised in each first gradient set;</claim-text>
<claim-text>separately sending, according to an adjusted communication sequence of the gradients comprised in each first gradient set, the gradients comprised in each of the N first gradient sets to parameter storage space of the training system;</claim-text>
<claim-text>obtaining a second gradient set based on the N first gradient sets stored in the parameter storage space; and</claim-text>
<claim-text>correcting a parameter matrix of a neuron at each layer of each deep learning model based on a gradient comprised in the second gradient set, to perform FP calculation in the (j + 1)<sup>th</sup> iteration on each deep learning model.</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The method according to claim 1, wherein the adjusting a communication sequence of the gradients comprised in each first gradient set comprises:<br/>
adjusting a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the a<sup>th</sup> layer to the parameter storage space to be before a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the b<sup>th</sup> layer to the parameter storage space, wherein b is less than or equal to n, a is less than b, and a is a positive integer greater than 0.</claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The method according to claim 1 or 2, wherein the adjusting a communication sequence of the gradients comprised in each first gradient set comprises:
<claim-text>adjusting, according to a gradient communication policy, the communication sequence of the gradients comprised in each first gradient set, wherein</claim-text>
<claim-text>the gradient communication policy is set based on at least one of the following parameters: a communication bandwidth between the deep learning model and the parameter storage space, a value of the gradient corresponding to the parameter matrix of the neuron at each layer of the deep learning model, and a time required by the neuron at<!-- EPO <DP n="32"> --> each layer of the deep learning model in FP calculation.</claim-text></claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>The method according to any one of claims 1 to 3, wherein the adjusting a communication sequence of the gradients comprised in each first gradient set comprises:
<claim-text>adjusting, according to the gradient communication policy, the communication sequence of the gradients comprised in each first gradient set; and</claim-text>
<claim-text>the method further comprises:
<claim-text>obtaining an iteration time of the deep learning model, wherein the iteration time comprises a time for BP calculation in the L<sup>th</sup> iteration of the deep learning model and a time for FP calculation in the (L + 1)<sup>th</sup> iteration of the deep learning model, and L is a positive integer greater than j; and</claim-text>
<claim-text>adjusting the gradient communication policy based on the iteration time.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>A deep learning model training system, wherein the training system comprises N deep learning models, a gradient communications module, a gradient update module, a correction module, and parameter storage space, each of the deep learning models comprises n layers of neurons, a training process of each of the deep learning models comprises a plurality of iterations, each iteration comprises forward propagation FP calculation and back propagation BP calculation, N is a positive integer greater than 1, and n is a positive integer greater than 1;
<claim-text>each deep learning module of the N deep learning models is configured to generate a first gradient set in BP calculation in the j<sup>th</sup> iteration, and each first gradient set comprises a gradient corresponding to a parameter matrix of a neuron at each layer of each deep learning model, and j is a positive integer greater than 0;</claim-text>
<claim-text>the gradient communications module is configured to: adjust a communication sequence of the gradients comprised in each first gradient set, and separately send, to the parameter storage space according to an adjusted communication sequence of the gradients comprised in each first gradient set, the gradients comprised in each of the N first gradient sets;</claim-text>
<claim-text>the gradient update module is configured to obtain a second gradient set based on the N first gradient sets stored in the parameter storage space; and</claim-text>
<claim-text>the correction module is configured to correct the parameter matrix of the neuron at each layer of each deep learning model based on a gradient comprised in the second gradient set, to perform FP calculation in the (j + 1)<sup>th</sup> iteration of each deep learning model.</claim-text></claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>The system according to claim 5, wherein the gradient communications module is configured to:<br/>
adjust a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the a<sup>th</sup> layer to the parameter storage space to be before a sequence of sending a gradient corresponding to a parameter matrix of a neuron at the b<sup>th</sup> layer to the parameter storage space, wherein b is less than or equal to n, a is less than b, and a is a positive<!-- EPO <DP n="33"> --> integer greater than 0.</claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>The system according to claim 5 or 6, wherein the gradient communications module is configured to:
<claim-text>adjust, according to a gradient communication policy, the communication sequence of the gradients comprised in each first gradient set; and</claim-text>
<claim-text>the gradient communication policy is set based on at least one of the following parameters: a communication bandwidth between the deep learning model and the parameter storage space, a value of the gradient corresponding to the parameter matrix of the neuron at each layer of the deep learning model, and a time required by the neuron at each layer of the deep learning model in FP calculation.</claim-text></claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>The system according to any one of claims 5 to 7, wherein the training system further comprises a feedback module, wherein
<claim-text>the feedback module is configured to feed back an obtained iteration time of the deep learning model to the gradient communications module, the iteration time comprises a time for BP calculation in the L<sup>th</sup> iteration of the deep learning model and a time for FP calculation in the (L + 1)<sup>th</sup> iteration of the deep learning model, and L is a positive integer greater than j; and</claim-text>
<claim-text>the gradient communications module is further configured to adjust the gradient communication policy based on the iteration time.</claim-text></claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>A deep learning model training system, wherein the training system comprises at least one computing node, each computing node comprises a memory and at least one processor, the memory is configured to store a program instruction, and the at least one processor of the at least one computing node executes the program instruction in the memory to perform the operation steps of the method according to any one of claims 1 to 4.</claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>A non-transient readable storage medium, comprising a program instruction, wherein when the program instruction is run by at least one computing node, the at least one computing node performs the method according to any one of claims 1 to 4.</claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>A computer program product, comprising a program instruction, wherein when the program instruction is run by at least one computing node, the at least one computing node performs the method according to any one of claims 1 to 4.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="34"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="114" he="227" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> -->
<figure id="f0002" num="2A"><img id="if0002" file="imgf0002.tif" wi="164" he="192" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> -->
<figure id="f0003" num="2B"><img id="if0003" file="imgf0003.tif" wi="165" he="140" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> -->
<figure id="f0004" num="3"><img id="if0004" file="imgf0004.tif" wi="165" he="126" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> -->
<figure id="f0005" num="4"><img id="if0005" file="imgf0005.tif" wi="154" he="228" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> -->
<figure id="f0006" num="5"><img id="if0006" file="imgf0006.tif" wi="127" he="136" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> -->
<figure id="f0007" num="6"><img id="if0007" file="imgf0007.tif" wi="129" he="232" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="41"> -->
<figure id="f0008" num="7,8(a),8(b)"><img id="if0008" file="imgf0008.tif" wi="148" he="227" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="160" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="160" he="233" type="tif"/></search-report-data>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="CN201910041235" dnum-type="L"><document-id><country>CN</country><doc-number>201910041235</doc-number><date>20190116</date></document-id></patcit><crossref idref="pcit0001">[0001]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
