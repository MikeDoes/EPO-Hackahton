<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP18188576B1" file="EP18188576NWB1.xml" lang="en" country="EP" doc-number="3609189" kind="B1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSK..HRIS..MTNORS..SM..................</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  2100000/0</B007EP></eptags></B000><B100><B110>3609189</B110><B120><B121>EUROPEAN PATENT SPECIFICATION</B121></B120><B130>B1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>18188576.5</B210><B220><date>20180810</date></B220><B240><B241><date>20200806</date></B241><B242><date>20201001</date></B242></B240><B250>en</B250><B251EP>en</B251EP><B260>en</B260></B200><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20200212</date><bnum>202007</bnum></B430><B450><date>20211006</date><bnum>202140</bnum></B450><B452EP><date>20210428</date></B452EP></B400><B500><B510EP><classification-ipcr sequence="1"><text>H04N  21/43        20110101AFI20181107BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>H04N  21/488       20110101ALI20181107BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>H04N  21/84        20110101ALI20181107BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>H04N  21/434       20110101ALI20181107BHEP        </text></classification-ipcr><classification-ipcr sequence="5"><text>H04N  21/4425      20110101ALI20181107BHEP        </text></classification-ipcr><classification-ipcr sequence="6"><text>H04N  21/2187      20110101ALI20181107BHEP        </text></classification-ipcr><classification-ipcr sequence="7"><text>H04N  21/44        20110101ALI20181107BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>H04N  21/4425      20130101 LI20181105BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>H04N  21/2187      20130101 LI20181105BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>H04N  21/44008     20130101 LI20181105BHEP        </text></classification-cpc><classification-cpc sequence="4"><text>H04N  21/4307      20130101 LI20181105BHEP        </text></classification-cpc><classification-cpc sequence="5"><text>H04N  21/84        20130101 LI20181105BHEP        </text></classification-cpc><classification-cpc sequence="6"><text>H04N  21/4884      20130101 FI20181105BHEP        </text></classification-cpc><classification-cpc sequence="7"><text>H04N  21/4348      20130101 LI20181105BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>TESTEN DES RENDERING VON BILDSCHIRMOBJEKTEN</B542><B541>en</B541><B542>TESTING RENDERING OF SCREEN OBJECTS</B542><B541>fr</B541><B542>TESTS DU RENDU D'OBJETS D'ÉCRAN</B542></B540><B560><B561><text>KR-A- 20170 047 547</text></B561><B562><text>Alberto Sabater: "Automatic Subtitle Synchronization - Machine Learnings", , 14 September 2017 (2017-09-14), XP055520600, Retrieved from the Internet: URL:https://machinelearnings.co/automatic- subtitle-synchronization-e188a9275617 [retrieved on 2018-10-31]</text></B562></B560></B500><B700><B720><B721><snm>GORE, Douglas</snm><adr><str>Nagra Media UK Ltd
Machen, The Pavillions Llantarnam Business Park</str><city>Cwmbran, Gwent NP44 3UW</city><ctry>GB</ctry></adr></B721><B721><snm>ZOU, Ping</snm><adr><str>Nagra Media UK Ltd
Machen, The Pavillions Llantarnam Business Park</str><city>Cwmbran, Gwent NP44 3UW</city><ctry>GB</ctry></adr></B721></B720><B730><B731><snm>Nagravision SA</snm><iid>101726417</iid><irf>33261</irf><adr><str>22-24, route de Genève</str><city>1033 Cheseaux-sur-Lausanne</city><ctry>CH</ctry></adr></B731></B730><B740><B741><snm>Schwegman Lundberg Woessner Limited</snm><iid>101730205</iid><adr><str>Hillington Park Innovation Centre 
1 Ainslie Road</str><city>Glasgow G52 4RU</city><ctry>GB</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840></B800></SDOBI>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001">FIELD</heading>
<p id="p0001" num="0001">The present disclosure relates to methods and devices for testing rendering of one or more screen objects in a video stream</p>
<heading id="h0002">BACKGROUND</heading>
<p id="p0002" num="0002">Automated testing is part of software development life cycles, which serves to continually ensure that software solutions are maintained in a functionally correct state. However, in areas like digital television (DTV) related development, an existing issue is that testing the correctness of the software or an instruction set is very difficult when the output is visual. Solutions like binary comparison of a visual output against a reference image are very brittle, time consuming to implement, and not suited for dynamic contents, e.g. live video streams. Particularly, existing DTV playback visual quality checks, e.g. subtitle rendering, is being done manually, or by a simple fixed image comparison. The existing processes for checking the quality of subtitle rendering are inefficient, hard to repeat, error prone and not feasible to apply to dynamic contents that are rendered for playback. Furthermore, testing of visual output of a video stream of a live event is difficult to implement and almost impossible to guarantee accuracy when done using existing image comparison techniques such as OCR or manual testing methods. It is also very difficult to continually and consistently test a live visual output using known testing techniques. Accordingly, there is a need for an efficient, automated, scalable and accurate process for testing the rendering, i.e. output playback of dynamic content, i.e. a video stream comprising a plurality of video frames. <patcit id="pcit0001" dnum="KR20170047547"><text>KR20170047547</text></patcit> discloses a method of automatically adjusting the synchronization of subtitles in video. An article by <nplcit id="ncit0001" npl-type="s" url="https://machinelearnings.co/automatic-subtitle-synchronization-e188a9275617"><text>A. Sabater entitled "Automatic Subtitle Synchronization - Machine Learnings (14 Sept 2017, https://machinelearnings.co/automatic-subtitle-synchronization-e188a9275617</text></nplcit>) discloses the use of a trained neural network to detect speech in videos and the use of that to synchronize the video subtitles.</p>
<heading id="h0003">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p0003" num="0003">Aspects and embodiments of this disclosure are now described by way of example for the purpose of illustration and with reference to the accompanying drawings, in which:
<ul id="ul0001" list-style="none">
<li><figref idref="f0001">Figure 1</figref> is a schematic depicting a method of obtaining training data to train a data model to test rendering of one or more video frames, according to a first aspect of the disclosure.<!-- EPO <DP n="2"> --></li>
<li><figref idref="f0002">Figure 1a</figref> is an example of a data model for use with the aspects and embodiments of the present disclosure.</li>
<li><figref idref="f0003">Figure 2</figref> is a schematic depicting a second aspect of testing video data that is rendered, where the testing is automatically carried out using a trained data model.</li>
<li><figref idref="f0004">Figure 3</figref> shows an example of original or source metadata of video frames prior to rendering, and an example of extracted metadata corresponding to the video frames after rendering, relating to the second aspect.</li>
<li><figref idref="f0005">Figure 4</figref> is a schematic depicting a method of providing a feedback loop to a media device relating to the outcome of the testing of <figref idref="f0004">figure 3</figref>.</li>
<li><figref idref="f0006">Figure 5</figref> is an example implementation of a media device for use in accordance with aspects of the present disclosure.</li>
</ul></p>
<heading id="h0004">DETAILED DESCRIPTION</heading>
<p id="p0004" num="0004">In overview, methods and devices in accordance with the disclosure relate to obtaining metadata from an output video stream, which is then used to test if the output video stream was correctly rendered. Data models are trained to generate metadata from a video/visual output, i.e. a render of a video data of a media device or application, such as a set top box, to capture more information from the visual output for testing, diagnostics, as well as data for an automated quality check. The term rendering is understood to include playback at or using a media device. In some implementations, the media device may include an application or processing module for a player capable of preparing and providing a video output on a display, such as a visual display unit, graphical interface unit associated with the media device. The rendering also includes processing the video stream at media device for projection on a screen that is not directly associated with the media device, i.e. when the media device is a projector.</p>
<p id="p0005" num="0005">In a first aspect a method of obtaining training data to train a data model is disclosed. In some implementations, the data model is trained for testing a rendering of one or more video frames at or using a media device. The method of the first aspect comprises obtaining as training data for a data model a set of rendered video frames, the method comprising rendering at a media device, a plurality of video frames of a source video<!-- EPO <DP n="3"> --> stream, each frame comprising one or more primary screen objects. The primary screen objects include dynamic video content comprising images and optical characters that make up a visual scene represented in a video frame. The source video stream further comprises source or original metadata for describing a further screen object to be rendered in at least one of the plurality of video frames, where the further screen object is configured to be superimposed on the one or more primary screen objects of a given frame during rendering. The method comprises rendering each frame of the plurality of frames at least once with the further screen object and at least once without the further screen object. The media device includes a setting allowing or not allowing the further screen object to be rendered. The primary screen objects are components that make up a dynamic image in a video frame, for example in-video images of individuals or buildings, vehicles etc. that make up a scene represented in the video frame. Further screen objects in a video frame include a timed text screen object, which for example represents subtitles, otherwise known as closed captions, for the video frame. Thus, the plurality of video frames is rendered at least once with the subtitles turned on, and at least once with the subtitled turned off. Rendering relates to preparing the video frames for video/visual output at or using a media player. This includes preparing the video frames for playback and/or display on a screen or graphical user interface associated with the media device. The method further includes creating a training data set including a set of rendered frames, each frame in the training data set corresponding to one of the plurality of video frames rendered with and without the further screen object, and a corresponding data item for each frame indicating the presence or absence of a further screen object in a given frame</p>
<p id="p0006" num="0006">The source metadata includes information to indicate whether a given frame includes a further screen object. In some implementations this may be a time stamp associated with the given video frame.<!-- EPO <DP n="4"> --></p>
<p id="p0007" num="0007">The data item for a given frame is a descriptive label. The label states if there is a further screen object, i.e. subtitle present or not. In some implementations the label may also state the location of the subtitle, dimensions, the text is etc. In other implementations, the data item may be a flag to state if a subtitle is present or not.<!-- EPO <DP n="5"> --></p>
<p id="p0008" num="0008">Thus, the training data to be provided to a data model is a set of frames or images, along with a corresponding data item or a flag or label for each.</p>
<p id="p0009" num="0009">In some embodiments, the data item obtained may be descriptive data that is provided or applied by an observer, for example by crowd sourcing or other means of data gathering, or provided by an existing database of associated labels, or any other known means. Crowd sourcing for the data items may typically be done by humans analysing the rendered frames and describing the contents of the frame. In some implementations many different kinds of video frames from video streams are analysed so that a number of various kinds of videos are provided.</p>
<p id="p0010" num="0010">Therefore, the training data is generated by providing videos with and without source metadata for further screen objects such as subtitles and rendering each video. For each video frame, a descriptive label applied by an observer is also provided.</p>
<p id="p0011" num="0011">The above described method proposes two rendered video outputs of video frames that include the same primary screen objects. The difference in the renderings being that a further screen object is overlaid on the primary screen objects of each video frame when rendered with a further screen object setting turned on. The rendered outputs are read and analysed by an artificial neural network to distinguish further screen objects from primary screen objects that may have one or more features that are the same or appear similar to the further screen object. Primary screen objects in a video frame include in-video text labels, e.g. a shop brands or text printed on a moving vehicle in the video background, and the further screen object are timed text.</p>
<p id="p0012" num="0012">As the in-video content is the same in both rendered visual outputs, the method of the first aspect can advantageously distinguish text in the in-video labels from timed text to facilitate an automatic and accurate detection of a timed text object in a rendered visual output.</p>
<p id="p0013" num="0013">In some embodiments, the original metadata for the plurality video frames of the input video stream comprises one or more properties associated with each primary and/or further screen object in each video frame. For example, the metadata may include a position of the screen object in a given video frame, or a time-stamp associated with the instance that the screen object is rendered, or dimensions such as the height and width of<!-- EPO <DP n="6"> --> the screen object, or a duration of time for which the screen object is rendered etc. This advantageously ensures that the machine/computer readable metadata includes enough information defining how and when each screen object is rendered, thereby making detection of and/or validation of a render of the further screen object easier and accurate in a data model or processor.</p>
<p id="p0014" num="0014">A media device for testing the rendering of one or more video frames is disclosed. The media device may be a set top box or the like comprising a receiver for obtaining a video stream that comprises a plurality of video frames, a player comprising one or more processors for preparing and/or rendering the obtained video stream and a display communicatively coupled with the player for displaying the rendered video stream as a visual output. The media device is configured to implement the method of the first aspect set out above.</p>
<p id="p0015" num="0015">In a second aspect of the present disclosure, a method of training a data model for detecting screen objects in one or more video frames rendered at or using a media device is disclosed. In an example implementation, the data model comprises a data structure that may be provided in a memory area of a computing device. The data structure may in some implementations be based on an artificial neural network configured to receive, analyse, characterise and store data such as video frames i.e. pixels and images contents, or metadata associated with a plurality of screen objects from a plurality of video frames. The method comprises initialising a data model for obtaining and storing data associated with a plurality of video frames and obtaining as a first input, a plurality of rendered video frames. The rendered frames of the first input may be video frames rendered at the media player of the first aspect. As a second input, data items corresponding to each of the rendered frames of the first input is provided. In some implementations, the data items may be descriptive labels, each for a given rendered frame. For example, as with the first aspect the data items may be obtained by crowd sourcing by human observers or from a database. The corresponding data items for each frame is indicative of the presence or absence of a further screen object in the corresponding frame. The second aspect of training the data model is provided by applying each rendered frame to an input of the data model and reducing an error between an output of the data model for a given frame and the corresponding data item for the given frame.<!-- EPO <DP n="7"> --></p>
<p id="p0016" num="0016">Thus, once we have the training data set, for instance, as set out in the first aspect, including a set of rendered frames, and corresponding data items for each frame labelled with descriptive labels, this set ca be used to train an ANN, such as CNN like Caffe, which is a known existing image based neural network.</p>
<p id="p0017" num="0017">In some implementations, the method of training the data model is by applying each frame to an input layer of the CNN. In the simplest case, it may be considered that for example the CNN has a neuron for each pixel, for generating the output. The output is the extracted metadata from the rendered frames in the first input generated by the data model. In some implementations, this extracted metadata will include details on the further screen object rendered. Next, the method of training the data model includes adjusting connection weights of the neurons within the data model to minimise the error or difference between the recorded output metadata and the data items, i.e. descriptive metadata in the labels in the second input.</p>
<p id="p0018" num="0018">As noted above, in the simplest case this can be an indication if a further screen object such as a subtitle is present or not. The output of the data model may a flag or setting or metadata that indicates the presence or absence of a further screen object. This output is referred to as extracted metadata, as it related to computer/machine readable data relating to one or more screen objects that are rendered in a video output. In some embodiments, the exacted metadata maybe in a binary output form, i.e. a flag or setting, where "1" or similar indicates the presence of a further screen object such as a subtitle, and a "0" or similar indicates the absence of a further screen object.</p>
<p id="p0019" num="0019">The data model is an artificial neural network (ANN), such as a convolutional neural network (CNN), that receives and analyses images i.e. the screen objects on in a video frame, and outputs a flag or a result indicating the presence of a further screen object such as a subtitle. This is extracted metadata explaining what the further screen object is, i.e. a subtitle superimposed on other primary screen objects, or an extension in a frame on which there is zero primary screen object information, etc. The extracted metadata is therefore the output of the data model.</p>
<p id="p0020" num="0020">In some implementations, a classification and or clustering algorithm may be used to train the data model for identifying one or more specific characteristics of the further screen objects in the video frames of the first input.<!-- EPO <DP n="8"> --></p>
<p id="p0021" num="0021">Advantageously, training the data model with a training data set as set out in the first aspect, that includes properties of the primary screen objects in the input video frame with and without the further screen objects of the same video frames before rendering enables the data model to accurately detect such properties subsequent or other video streams when generating the extracted metadata for such subsequent or other streams.</p>
<p id="p0022" num="0022">The method of training the data model comprises detecting the presence of one or more specific or defined characteristics in the video frames of the first input. Advantageously, this enables accurate identification of rendered screen objects that include the one or more specific characteristics to better differentiate from rendered screen objects that do not include them. In some implementations, the method comprises providing a binary output identifier to indicate the presence or absence of a further screen object in the rendered video frame, based on the outcome of detecting one or more specific characteristics.</p>
<p id="p0023" num="0023">In a third aspect of the present disclosure, a method of testing video data being rendered at or using a media device is disclosed. The method of testing according to the third aspect comprises receiving at a media device a plurality of video frames to be rendered, each frame comprising one or more primary screen objects and at least one further screen object. The method of testing comprises rendering the received frames at or using the media device wherein the at least one further screen object is superimposed on the one or more primary screen objects of a given frame during rendering. The plurality of frames is rendered in accordance with source or original metadata, including machine readable instructions for a media player or the like to render primary and/or further screen objects from a source of the plurality of video frames.</p>
<p id="p0024" num="0024">The rendered frames are then provided to a data model, said data model being trained to identify a further screen object rendered in the video frame. Metadata is then extracted from the rendered video frames by the data model. As mentioned in relation to the second aspect, the extracted metadata comprises one or more properties associated with at least a further screen object rendered for a given video frame. The data model is one that is trained in accordance with the second aspect set out above. The data model is trained with training data as set out above in relation to the first aspect.<!-- EPO <DP n="9"> --></p>
<p id="p0025" num="0025">The original metadata is provided to the data model from a source of the video stream that comprises the video frames provided to the media device to be rendered. The original metadata that is provided includes metadata relating to only the further screen object for each video frame, and the original metadata relate to the primary screen objects and the further screen objects. The method of testing according to the third aspect further comprises detecting the presence or absence of a further screen object by the trained data model, wherein the output of the data model is extracted metadata indicating such presence or absence for each frame. The method of testing further includes validating a detected further screen object using the original or source metadata relating to at least the further screen object before rendering. In some implementations, the validation is carried out if the source metadata indicates that a subtitle should be generated for the rendered frame and the output or extracted metadata from the data model indicates that a subtitle has been rendered, or if the source metadata indicates that a subtitle should not be generated for the rendered frame and the output or extracted indicates that a subtitle has not been rendered</p>
<p id="p0026" num="0026">Advantageously, the method according to the third aspect enables the use of computer readable data, such as metadata from an output video stream, to test if the output video stream was correctly rendered. This enables testing of an output rendering to be done automatically, rather than by manually checking if the dynamic media components for each frame are correctly rendered. As metadata is generated by a data model to identify several types of screen objects and detect the presence one or more further screen object, this process is more accurate and effective when compared to a manual or a fixed image comparison process. As the testing process is automatic, i.e. carried out by computing devices implementing the data model that can characterise, classify and/or analyse the further screen objects in a rendered video frame, the process of testing a render can be repeated as often as needed for testing dynamic contents of a video stream that is rendered for playback. Furthermore, the method can be used to accurately test a live stream of video data in real time from metadata extracted from the render of the live video frames.</p>
<p id="p0027" num="0027">In some embodiments, the method of testing comprises identifying and comparing one or more specific characteristics of a detected further screen object from a rendered video frame to one or more corresponding or related specific characteristics of the original<!-- EPO <DP n="10"> --> metadata pertaining to the detected further screen object. This is for identifying the presence or absence of a variance in each specific characteristic for the detected further screen object.</p>
<p id="p0028" num="0028">Advantageously, the automatic detecting of a variance in specific characteristics enables an identification of whether the result of the automatic testing is positive or negative. The presence of a variance indicates that further screen object has not been rendered correctly. In some implementations, the further screen object is a timed text instruction and the specific characteristic is a time stamp relative to one or more primary screen objects in a given frame. Therefore, a variance indicates that the timed text is out of sync for the given video frame.</p>
<p id="p0029" num="0029">In some implementations an offset is calculated based on a detected variance. The offset advantageously indicates the extent to which the further screen object is incorrectly rendered. In some embodiments, the offset is provided as feedback to the media device for subsequent adjustments. This advantageously enables the media device to adjust a subsequent rendering of the plurality of video frames based on the offset, so that the subsequent rendering no longer includes the variance. This enables application of a correction factor to automatically ensure that the same error(s) do not reoccur when rendering the video stream.</p>
<p id="p0030" num="0030">Some specific components and embodiments are now described by way of illustration with reference to the accompanying drawings, in which like reference numerals refer to like features.</p>
<p id="p0031" num="0031">With reference to <figref idref="f0001">Figure 1</figref>, a schematic implementation of a method of obtaining training data for training a data model is shown, according to a first aspect.</p>
<p id="p0032" num="0032">A video stream 102 comprising a plurality of video frames is shown, along with source or original metadata 104 associated the video stream 102. In some implementations of the first aspect, the video stream is considered to be a test video stream. The original metadata is a machine-readable representation of one or more dynamic components of the video frames in the video stream 102. The original metadata 104 provided at a source of the video stream 102 is for specifying properties dictating the output or render of the video stream 102. In some implementations, this original metadata 104 may at least include metadata for generating a subtitle for one or more video frames in the video<!-- EPO <DP n="11"> --> stream 102. The video stream 102 in <figref idref="f0001">Figure 1</figref> may be considered as a test video stream, rendered samples of which are to be provided for training a data model 114.</p>
<p id="p0033" num="0033">Each video frame in the video stream 102 comprises a plurality of primary screen objects, i.e. in-video images and dynamic optical characters. A plurality of such video frames in the video stream 102 also have a further screen object that is to be superimposed on the primary screen objects, during rendering. The further screen object is timed text, i.e. a subtitle, for a given frame.</p>
<p id="p0034" num="0034">In step 106, the video stream 102 is provided to a media device 108 in two input stages. In the first input stage, the media device 108 is provided with the video stream 102 without any machine-readable instructions for defining the subtitles. In a second input stage the media device is provided with the same video stream 102, but this time along with machine-readable instructions that defines the subtitles to be rendered or generated for each video frame. The machine-readable instructions provided to the media device 108 are considered to be source or original metadata 104. The original metadata is obtained from a source of the media stream 102 to the media device 108. The media device 108 is configured to render the video stream 102 with the subtitles added for each frame received in the second input stage separately, or after rendering the same video stream 102 without the subtitles of the first input stage.</p>
<p id="p0035" num="0035">In step 110a the media 108 renders video stream 102 without any subtitle instructions, i.e. no subtitle instructions are present in the original metadata 104, to create a first sample 112a comprising a set of rendered video frames. This can be considered in implementation as a playback of the video stream with subtitles turned off. Therefore, no subtitle instructions are processed for this render and the video frames with in-video dynamic content, i.e. primary screen objects, are rendered to create the first sample of video frames 112a.<!-- EPO <DP n="12"> --></p>
<p id="p0036" num="0036">In step 110b the media device 108 renders video stream 102 with the subtitle instructions to create a second sample 112b comprising a set of rendered video frames with their associated subtitles. For example, in implementation the video stream 102 can be considered as being rendered by a media device with the subtitle turned on to create a second sample 112b. In an example implementation, one video frame in the second sample is rendered per second by the media device 108.</p>
<p id="p0037" num="0037">Training data to be provided to a train a data model includes the first sample 112a and the second sample 112b. In other words, the training data set includes the test video frames rendered once with the further screen object and once without. In addition to the pairs of rendered frames, the training data set also includes a data item associated with each of the rendered test video frames. The data item includes descriptive labels for at least the further screen objects of the video frames. The descriptive label may include information that is somewhat similar to the source metadata, i.e. it provides information on how, where and when a certain screen object is rendered for the video frame. As mentioned above, the descriptive labels may be obtained from crowdsourcing, where one or more human observers assign labels to describe the screen object of a certain video frame. Accordingly, the training data set includes video frames rendered with and without a further screen object, as well as a data item as explained above for each frame. This advantageously enables, configures or trains the data model 114to detect the presence of one or more given screen objects in a render of any video stream based on the properties of the rendered frame and/or validate if the render of the detected screen object has been performed correctly.</p>
<p id="p0038" num="0038">The data model 114 is an artificial neural network (ANN), such as a convolutional neural network (CNN). For example, Caffe™ is example of a CNN computational model may be used for detecting objects from a dynamic visual output or image.</p>
<p id="p0039" num="0039">ANNs, otherwise known as connectionist systems, are computing systems vaguely inspired by biological neural networks. It is known that such networks "learn" tasks by considering data samples, generally without task-specific pre-programming and without any prior knowledge about a task, and instead, evolve their own set of relevant characteristics from learning material or samples that they process. ANNs can be<!-- EPO <DP n="13"> --> hardware-based (where neurons are represented by physical components) or software-based (computer models) and can use a variety of topologies, learning algorithms or functions.</p>
<p id="p0040" num="0040">To initialise such a data model 114, which is an ANN, to be able to detect one or more screen objects from a rendered output- one or more functions may be defined for the data model 114. For example, the defined functions can be one or more of: a function that returns a structure describing labels for list of images, and/or a function that returns a region of interest (ROI) structure that describes bounding box annotations, for subtitles for instance; and/or a function that provides a test evaluation, i.e. comparison function. The text evaluation function in some embodiments can be a binary function, where a 0 indicates no subtitles (which will be the case for the first sample 112a) and 1 indicates that a subtitle exists (which will be the case for second sample 112b).</p>
<p id="p0041" num="0041">An example implementation of an ANN in its simplest form that can be used as the data model 114 for aspects and embodiments of the present invention is seen in <figref idref="f0002">Figure 1a</figref>. ANNs usually have at least three layers that are interconnected. The first layer, which is an input layer consists of input neurons. Those neurons are configured to send data on to the second layer, referred to as a hidden layer which implements one or more functions on data from the input layer, and which in turn is configured to send the output neurons to the third, or output layer. There may be a plurality of hidden layers in the ANN, but for simplicity a model with one such layer is shown.</p>
<p id="p0042" num="0042">The second or hidden layer in a neural network implements one or more functions. For example, the function or functions may each compute a linear transformation of the output from a previous layer or compute logical functions. For instance, considering an implementation where an input vector can be represented as x, the second or hidden layer functions as h, and the third or output layer as y, then the ANN may be understood as implementing a function f(x) using the first layer and the second layer that maps from x to h, and another function g(x) using the second layer and the third layer that maps from h to y.</p>
<p id="p0043" num="0043">Thus, the hidden layer is activated by f(x) and the output of the network is g(f(x)). In some implementations of the present described embodiments, it may be understood that f(x) can represent subtitle or further object detection, and g(x) can represent validation of the<!-- EPO <DP n="14"> --> render of the detected subtitle or further object at f(x), for instance. Subtitle detection and validation is further explained below in relation to <figref idref="f0003">Figure 2</figref> below.</p>
<p id="p0044" num="0044">The data model 114 detects the presence of absence of a subtitle and provides extracted metadata indicating this in an output, so that a further validation can take place to establish if the model produces the same output as the source metadata for each frame. For instance, the data model output can be represented as</p>
<heading id="h0005">Extracted Metadata =&gt; subtitle present? (Yes/no = 0/1)</heading>
<p id="p0045" num="0045">The training data of the first aspect, for example, is applied to the input and the weights of the neurons are adjusted so that the output 0 or 1 (or &lt;0.5 or &gt;0.5,) depending on whether a subtitle was in the training frame or not.</p>
<p id="p0046" num="0046">Finally, further to validation, the data model may be represented as: {extracted metadata; source metadata} =&gt; validated (yes or no).</p>
<p id="p0047" num="0047">With reference to <figref idref="f0003">Figure 2</figref>, a method of automatically testing a rendering of a video stream 202 is described, in accordance with a third aspect of the present disclosure. For the purposes of illustration, the video stream 202 is a live or real-time stream of an event such as a game or concert taking place, or a news channel broadcast live from a television studio. Like the video stream 102 in <figref idref="f0001">Figure 1</figref>, The live video stream 202 comprises a plurality of video frames, each frame having a plurality of primary screen objects and at least one further screen object, which is timed text, i.e. a subtitle. Original or source metadata 204 for at least the subtitle is provided at the source of the video stream 202. The original metadata 204 comprises, for example, a subtitle render instruction that includes properties defining how and when the subtitle is supposed to be displayed for a given video frame in the video stream 202, when the video stream 202 is rendered at a media device. An example of the original metadata 204 for a subtitle is seen for example in <figref idref="f0004">Figure 3</figref> and will be explained in further detailed below.</p>
<p id="p0048" num="0048">In step 206 the live video stream 202 is provided to a media device 208. The media device 208 is configured to render the input video frames from the video stream 202 for<!-- EPO <DP n="15"> --> playback and display with in-video primary screen objects, along with the superimposed or overlaid further screen object, i.e. the subtitle, that is rendered using the instructions in the original metadata 204. Upon rendering in step 210, rendered video frames 212 are provided as a visual output. The media device 208 may be arranged in step 210 to render the video frames 212 by sampling one video frame of the input live video stream 202 per second.</p>
<p id="p0049" num="0049">In step 214, the rendered frames are provided to a further screen object detection module Given that the further screen object is a subtitle in the illustrated embodiments, <figref idref="f0003">Figure 2</figref> shows a subtitle detection unit 216. The subtitle detection unit 216 is included in or associated with a data model 114 as described above in the second aspect, that has been trained based on training data illustrated and explained in relation to <figref idref="f0001">Figure 1</figref>, for accurately detecting and testing subtitles in a rendered video frame by generating metadata, i.e. this is extracted metadata obtained from a render of a visual output. Thus, in some implementations, as with the data model 114 in the second aspect, a binary output of '1' is produced to indicate if a subtitle is detected. Otherwise, a binary output of '0' is produced.</p>
<p id="p0050" num="0050">In some examples, when rendered video frame 212 is provided to the data model 114, which includes or is associated with the subtitle detection unit 216, the output in some embodiments may include the following, for each video frame, referred to as extracted metadata from the data model 114.
<ul id="ul0002" list-style="none" compact="compact">
<li>Timestamp ...</li>
<li>Is subtitle input provided (Y/N or 0/1)?</li>
<li>(and optional dimensions such as)</li>
<li>subtitle-top: ...</li>
<li>subtitle-left: ...</li>
<li>subtitle-width: ...</li>
<li>subtitle-height: ...</li>
</ul></p>
<p id="p0051" num="0051">The data model 114 detects the presence of a subtitle in the rendered frame 212. This is possible as the data model 114 has been trained using the training data set as described in relation to the first aspect including a pairs of video frames rendered with and without a further screen object, and a data item with a descriptive label for at least the further screen objects (see the first aspect in <figref idref="f0001">figure 1</figref>).<!-- EPO <DP n="16"> --></p>
<p id="p0052" num="0052">In step 218, if a subtitle is detected in a rendered video frame by the subtitle detection unit 216, then the extracted metadata output from the subtitle detection unit 216 of a detected subtitle 220, or in some implementations the extracted metadata of the video frame including the detected subtitle 220, is provided to a subtitle validation unit 224. The subtitle validation unit 224 may in some embodiments be implemented using or associated with the trained data module 114 mentioned earlier in relation to <figref idref="f0001">Figure 1</figref>.</p>
<p id="p0053" num="0053">In step 222, original subtitle metadata 204 in the live video stream 202 corresponding to the rendered video frame(s) with detected subtitle metadata 220 from the subtitle detection unit 216 is then provided as a further input to the subtitle validation unit 224.</p>
<p id="p0054" num="0054">Thus, the subtitle validation unit 224 is provided with extracted metadata 220 of the detected subtitle, as well as the corresponding original metadata 204, i.e. source subtitle instruction for the same input subtitle prior to rendering. Based on a comparative analysis of both source 204 and detected metadata 220, the subtitle validation unit 224 is configured to provide an output result 226 to indicate whether the detected subtitle has been rendered correctly by the media device 208</p>
<p id="p0055" num="0055">An example implementation indicating the original metadata 204 and the extracted metadata for the detected subtitle 220 is shown in <figref idref="f0004">Figure 3</figref>, to illustrate how the data model 114 implementing the subtitle validating unit 224 can be used to test if one or more subtitles were correctly rendered. The original subtitle metadata 204 in <figref idref="f0004">Figure 3</figref> indicates an instruction to render a subtitle ('First subtitle') beginning at the 2<sup>nd</sup> second and finishing at the 5<sup>th</sup> second. Also, the metadata 204 includes an instruction to render another one ('Second subtitle') beginning at the 10<sup>th</sup> second and finishing at the 15<sup>th</sup> second.</p>
<p id="p0056" num="0056">The extracted subtitle metadata for the detected subtitle 220 or the video frame including the detected subtitle 220 indicates that a subtitle is rendered between the 2<sup>nd</sup> second and the 5<sup>th</sup> second. So, for the First subtitle in this illustration, the validation result will be a pass, i.e. output result 226 will indicate that the subtitle was rendered correctly and as defined by the original metadata 204.</p>
<p id="p0057" num="0057">The extracted subtitle metadata for the detected subtitle 220 also indicates that another subtitle is rendered between the 13<sup>th</sup> second and the 17<sup>th</sup> second during the render of the video stream 202. In this case the validation results 226 will be a fail in that, the Second<!-- EPO <DP n="17"> --> subtitle starts to be rendered 2 seconds later than instructed in the original metadata 204, and the Second subtitle lasts 2 seconds longer than the instruction in the original metadata 204. The validation result 226 thus also in some embodiments, indicates that the detected rendered subtitle 220 is 2 seconds out of sync during rendering by the media device 208.</p>
<p id="p0058" num="0058"><figref idref="f0005">Figure 4</figref> illustrates an example embodiment for adjusting a rendering using the result of the validation unit 226 obtained in <figref idref="f0003">Figure 2</figref>. <figref idref="f0005">Figure 4</figref> includes all the steps and computations explained above for <figref idref="f0003">Figure 2</figref>. In addition, in step 228 of <figref idref="f0005">Figure 4</figref>, an offset to indicate a variance of an incorrect or a failed subtitle test is obtained. This can be the 2 second out of sync result that is seen in <figref idref="f0004">Figure 3</figref>, for example. The offset is provided as a feedback to the media device 208. One or more processors or computing devices in or associated with the media device 208 may be configured to apply this as a correction factor. The correction factor may in some implementations be provided to ensure that a future rendering or playback of the detected subtitle 220 or subtitles subsequently rendered in the video stream is no longer out of sync by 2 seconds.</p>
<p id="p0059" num="0059">Accordingly, from <figref idref="f0001 f0002 f0003 f0004 f0005">Figures 1 to 4</figref>, techniques for automatically testing rendering of visual outputs based on metadata extracted from a render of a live or real time stream of video data have been explained and illustrated. Advantageously the illustrated embodiments in <figref idref="f0001 f0002 f0003 f0004 f0005">Figures 1 to 4</figref> enable automatic testing of visual output of a video stream of a live event and provides an efficient, automated, scalable and accurate process for testing the rendering of dynamic content.</p>
<p id="p0060" num="0060"><figref idref="f0006">Figure 5</figref> illustrates a block diagram of one implementation of a computing device 500 within which a set of instructions, for causing the computing device to perform any one or more of the methodologies discussed herein, may be executed. The computing device 500 may be the media device or a device implementing the data model in the present disclosure. Similarly, the computing device 500 may also related to an example implementation of the source of the video stream. In alternative implementations, the computing device 500 may be connected (e.g., networked) to other machines in a Local Area Network (LAN), an intranet, an extranet, or the Internet. The computing device may operate in the capacity of a server or a client machine in a client-server network environment, or as a peer machine in a peer-to-peer (or distributed) network environment. The computing device may be a personal computer (PC), a tablet computer, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a web appliance, a server,<!-- EPO <DP n="18"> --> a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while only a single computing device is illustrated, the term "computing device" shall also be taken to include any collection of machines (e.g., computers) that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.</p>
<p id="p0061" num="0061">The example computing device 500 includes a processing device 502, a main memory 504 (e.g., read-only memory (ROM), flash memory, dynamic random-access memory (DRAM) such as synchronous DRAM (SDRAM) or Rambus DRAM (RDRAM), etc.), a static memory 506 (e.g., flash memory, static random-access memory (SRAM), etc.), and a secondary memory (e.g., a data storage device 518), which communicate with each other via a bus 530.</p>
<p id="p0062" num="0062">Processing device 502 represents one or more general-purpose processors such as a microprocessor, central processing unit, or the like. More particularly, the processing device 502 may be a complex instruction set computing (CISC) microprocessor, reduced instruction set computing (RISC) microprocessor, very long instruction word (VLIW) microprocessor, processor implementing other instruction sets, or processors implementing a combination of instruction sets. Processing device 502 may also be one or more special-purpose processing devices such as an application specific integrated circuit (ASIC), a field programmable gate array (FPGA), a digital signal processor (DSP), network processor, or the like. Processing device 502 is configured to execute the processing logic (instructions 522) for performing the operations and steps discussed herein.</p>
<p id="p0063" num="0063">The computing device 500 may further include a network interface device 508. The computing device 500 also may include a video display unit 510 (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)), an alphanumeric input device 512 (e.g., a keyboard or touchscreen), a cursor control device 514 (e.g., a mouse or touchscreen), and an audio device 516 (e.g., a speaker).</p>
<p id="p0064" num="0064">The data storage device 518 may include one or more machine-readable storage media (or more specifically one or more non-transitory computer-readable storage media) 528 on which is stored one or more sets of instructions 522 embodying any one or more of the methodologies or functions described herein. The instructions 522 may also reside,<!-- EPO <DP n="19"> --> completely or at least partially, within the main memory 504 and/or within the processing device 502 during execution thereof by the computer system 500, the main memory 504 and the processing device 502 also constituting computer-readable storage media.</p>
<p id="p0065" num="0065">The various methods described above may be implemented by a computer program. The computer program may include computer code arranged to instruct a computer to perform the functions of one or more of the various methods described above. The computer program and/or the code for performing such methods may be provided to an apparatus, such as a computer, on one or more computer readable media or, more generally, a computer program product. The computer readable media may be transitory or non-transitory. The one or more computer readable media could be, for example, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, or a propagation medium for data transmission, for example for downloading the code over the Internet. Alternatively, the one or more computer readable media could take the form of one or more physical computer readable media such as semiconductor or solid-state memory, magnetic tape, a removable computer diskette, a random-access memory (RAM), a read-only memory (ROM), a rigid magnetic disc, and an optical disk, such as a CD-ROM, CD-R/W or DVD.</p>
<p id="p0066" num="0066">In an implementation, the modules, components and other features described herein can be implemented as discrete components or integrated in the functionality of hardware components such as ASICS, FPGAs, DSPs or similar devices.</p>
<p id="p0067" num="0067">A "hardware component" is a tangible (e.g., non-transitory) physical component (e.g., a set of one or more processors) capable of performing certain operations and may be configured or arranged in a certain physical manner. A hardware component may include dedicated circuitry or logic that is permanently configured to perform certain operations. A hardware component may be or include a special-purpose processor, such as a field programmable gate array (FPGA) or an ASIC. A hardware component may also include programmable logic or circuitry that is temporarily configured by software to perform certain operations.</p>
<p id="p0068" num="0068">Accordingly, the phrase "hardware component" should be understood to encompass a tangible entity that may be physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein.<!-- EPO <DP n="20"> --></p>
<p id="p0069" num="0069">In addition, the modules and components can be implemented as firmware or functional circuitry within hardware devices. Further, the modules and components can be implemented in any combination of hardware devices and software components, or only in software (e.g., code stored or otherwise embodied in a machine-readable medium or in a transmission medium).</p>
<p id="p0070" num="0070">Unless specifically stated otherwise, as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as "receiving", "determining", "obtaining", "sending," "implementing," , "detecting", "extracting", "establishing" , "providing" , "rendering" , " identifying" or the like, refer to the actions and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices.</p>
<p id="p0071" num="0071">It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other implementations will be apparent to those of skill in the art upon reading and understanding the above description. Although the present disclosure has been described with reference to specific example implementations, it will be recognized that the disclosure is not limited to the implementations described but can be practiced with modification and alteration within the scope of the appended claims. Accordingly, the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense. The scope of the disclosure should, therefore, be determined with reference to the appended claims.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="21"> -->
<claim id="c-en-01-0001" num="0001">
<claim-text>A method of creating training data for training an artificial neural network for detecting screen objects in one or more video frames rendered at or using a media device (108, 208), the method comprising:
<claim-text>rendering at a media device (108, 208), a plurality of video frames of a source video stream (102, 202), each frame comprising one or more primary screen objects, the source video stream (102, 202) further comprising original metadata (104, 204) for describing a further screen object to be rendered in at least one of the plurality of video frames, wherein the further screen object is configured to be superimposed on the one or more primary screen objects of a given frame during rendering;</claim-text>
<claim-text>rendering each frame of the plurality of frames at least once with the further screen object and at least once without the further screen object; and</claim-text>
<claim-text>creating a training data set including a set of rendered video frames (112, 212), each frame in the training data set corresponding to one of the plurality of video frames rendered with and without the further screen object, and further including a corresponding data item for each frame indicating the presence or absence of a further screen object in each frame;</claim-text>
<claim-text>wherein the one or more primary screen objects comprise dynamic images and/or optical characters and the further screen object is a timed text screen object.</claim-text></claim-text></claim>
<claim id="c-en-01-0002" num="0002">
<claim-text>The method as claimed in claim 1, further including obtaining the corresponding data item for each frame, wherein each data item includes a descriptive label for agiven frame, wherein said obtaining includes analysing the given frame by an observer to identify the presence or absence of a further screen object in the frame.</claim-text></claim>
<claim id="c-en-01-0003" num="0003">
<claim-text>The method as claimed in any one of the preceding claims, wherein the original metadata comprises one or more properties associated with each primary and/or further screen object in each video frame.</claim-text></claim>
<claim id="c-en-01-0004" num="0004">
<claim-text>The method as claimed in claim 3, wherein the one or more properties comprise at least one of:
<claim-text>a position of the screen object in a video frame;</claim-text>
<claim-text>a time-stamp associated with the instance that the screen object is rendered;</claim-text>
<claim-text>dimensions of the screen object when rendered relative to a display associated with the media device; and<!-- EPO <DP n="22"> --></claim-text>
<claim-text>a duration of time for which the screen object is rendered.</claim-text></claim-text></claim>
<claim id="c-en-01-0005" num="0005">
<claim-text>A media device (108, 208) for obtaining training data for testing rendering of one or more video frames, the media device comprising:
<claim-text>a receiver for obtaining a video stream (102, 202) that comprises a plurality of dynamic video frames;</claim-text>
<claim-text>a player comprising one or more processors for rendering the obtained video stream (102, 202); and</claim-text>
<claim-text>a display communicatively coupled with the player for displaying the rendered video stream as a visual output,</claim-text>
<claim-text>wherein the media device (108, 208) is configured to implement the method steps of any one of claims 1 to 4.</claim-text></claim-text></claim>
<claim id="c-en-01-0006" num="0006">
<claim-text>A computer implemented method of training an artificial neural network for detecting screen objects in one or more video frames rendered at or using a media device, the method implemented using one or more processors that are associated with a memory, the method comprising:
<claim-text>initialising an artificial neural network in the memory for obtaining and/or storing data associated with a plurality of video frames;</claim-text>
<claim-text>obtaining as a first input training data including a plurality of rendered frames of a video stream rendered at or using the media player;</claim-text>
<claim-text>obtaining as a second input training data including data items associated with each of the rendered frames of the first input, each data item being indicative of the presence or absence of a further screen object in a given frame;</claim-text>
<claim-text>obtaining as an output of the artificial neural network, extracted metadata indicating the presence or absence of a further screen object in a given frame;</claim-text>
<claim-text>applying one or more functions to reduce an error between the output extracted metadata of the artificial neural network and the corresponding data item for the given frame;</claim-text>
<claim-text>wherein the first and second inputs are provided by the method of obtaining training data according to any one of claims 1 to 4.</claim-text></claim-text></claim>
<claim id="c-en-01-0007" num="0007">
<claim-text>The method as claimed in claim 6, wherein training the artificial neural network further comprises:
<claim-text>detecting the presence of the one or more specific characteristics in the rendered<!-- EPO <DP n="23"> --> video frames; and</claim-text>
<claim-text>providing a binary output identifier to indicate the presence or absence of a further screen object in the rendered video frames, based on the outcome of the detecting step.</claim-text></claim-text></claim>
<claim id="c-en-01-0008" num="0008">
<claim-text>A method of testing video data being rendered at or using a media device (108, 208), the method comprising:
<claim-text>receiving at a media device (108, 208) a plurality of video frames to be rendered in accordance with original metadata, each frame comprising one or more primary screen objects and at least one further screen object;</claim-text>
<claim-text>rendering the received frames at or using the media device (108, 208) wherein the at least one further screen object is superimposed on the one or more primary screen objects of a given frame during rendering, the received plurality of frames being rendered in accordance with original metadata;</claim-text>
<claim-text>providing the rendered frames to an artificial neural network, said artificial neural network being trained in accordance with the method as claimed in claim 6 or claim 7;</claim-text>
<claim-text>obtaining extracted metadata as an output of the artificial neural network, the extracted metadata detecting the presence or absence of a further screen object in a given rendered frame;</claim-text>
<claim-text>providing the original metadata associated with the plurality of video frames to the artificial neural network; and</claim-text>
<claim-text>validating the rendering of a detected further screen object based on the original metadata relating to the given video frame.</claim-text></claim-text></claim>
<claim id="c-en-01-0009" num="0009">
<claim-text>The method as claimed in claim 8, wherein the step of validating further comprises:
<claim-text>identifying a specific characteristic of the detected further screen object;</claim-text>
<claim-text>comparing the identified characteristic to a related specific characteristic in the original metadata; and</claim-text>
<claim-text>detecting the presence of a variance in the identified specific characteristic based on the outcome of the comparing step.</claim-text></claim-text></claim>
<claim id="c-en-01-0010" num="0010">
<claim-text>The method as claimed in 9, further comprising:
<claim-text>responsive to a variance being detected, calculating an offset based on the detected variance;</claim-text>
<claim-text>providing the offset as feedback to a processor associated with the media device<!-- EPO <DP n="24"> --> (108, 208); and</claim-text>
<claim-text>adjusting a subsequent rendering of the video frames based on the offset.</claim-text></claim-text></claim>
<claim id="c-en-01-0011" num="0011">
<claim-text>A system for testing video data being rendered at or using a media device (108, 208), the system including the media device (108, 208) and at least one processor associated with a memory for storing a plurality of instructions, wherein the at least one processor is configured to execute the plurality of instructions to implement the method as claimed in any one claims 8 to 10.</claim-text></claim>
</claims>
<claims id="claims02" lang="de"><!-- EPO <DP n="25"> -->
<claim id="c-de-01-0001" num="0001">
<claim-text>Verfahren zum Erzeugen von Trainingsdaten zum Trainieren eines künstlichen neuronalen Netzwerks, um Bildschirmobjekte in einem oder mehreren Videoframes, die auf oder unter Verwendung einer Medienvorrichtung (108, 208) gerendert werden, zu erkennen, wobei das Verfahren umfasst:
<claim-text>Rendern einer Vielzahl von Videoframes eines Quellen-Videostreams (102, 202) auf einer Medienvorrichtung (108, 208), wobei jeder Frame ein oder mehrere primäre Bildschirmobjekte umfasst, wobei der Quellen-Videostream (102, 202) weiter ursprüngliche Metadaten (104, 204) umfasst, um ein weiteres Bildschirmobjekt, das in mindestens einem aus der Vielzahl von Videoframes gerendert werden soll, zu beschreiben, wobei das weitere Bildschirmobjekt so ausgelegt ist, dass es beim Rendern dem einen oder den mehreren primären Bildschirmobjekten eines gegebenen Frames überlagert wird;</claim-text>
<claim-text>Rendern jedes Frames aus der Vielzahl von Frames mindestens einmal mit dem weiteren Bildschirmobjekt, und mindestens einmal ohne dem weiteren Bildschirmobjekt; und</claim-text>
<claim-text>Erzeugen eines Trainingsdatensatzes, der einen Satz gerenderter Videoframes (112, 212) einschließt, wobei jeder Frame in dem Trainingsdatensatz einem aus der Vielzahl von mit und ohne dem weiteren Bildschirmobjekt gerenderten Videoframes entspricht, und der weiter ein entsprechendes Datenelement für jeden Frame einschließt, das das Vorhandensein oder Fehlen eines weiteren Bildschirmobjekts in jedem Frame angibt;</claim-text>
<claim-text>wobei das eine oder die mehreren primären Bildschirmobjekte dynamische Bilder und/oder optische Zeichen umfassen, und das weitere Bildschirmobjekt ein Timed Text-Bildschirmobjekt ist.</claim-text></claim-text></claim>
<claim id="c-de-01-0002" num="0002">
<claim-text>Verfahren nach Anspruch 1, das weiter das Erhalten des entsprechenden Datenelements für jeden Frame einschließt, wobei jedes Datenelement eine beschreibende Bezeichnung für einen gegebenen Frame einschließt, wobei das Erhalten das Analysieren des gegebenen Frames durch einen Beobachter einschließt, um das Vorhandensein oder Fehlen eines weiteren Bildschirmobjekts im Frame zu identifizieren.<!-- EPO <DP n="26"> --></claim-text></claim>
<claim id="c-de-01-0003" num="0003">
<claim-text>Verfahren nach einem der vorstehenden Ansprüche, wobei die ursprünglichen Metadaten eine oder mehrere Eigenschaften umfassen, die mit jedem primären und/oder weiteren Bildschirmobjekt in jedem Videoframe verknüpft sind.</claim-text></claim>
<claim id="c-de-01-0004" num="0004">
<claim-text>Verfahren nach Anspruch 3, wobei die eine oder die mehreren Eigenschaften mindestens eines umfassen aus:
<claim-text>einer Position des Bildschirmobjekts in einem Videoframe;</claim-text>
<claim-text>einem Zeitstempel, der mit dem Zeitpunkt, zu dem das Bildschirmobjekt gerendert wird, verknüpft ist;</claim-text>
<claim-text>Abmessungen des Bildschirmobjekts, wenn es gerendert wird, in Bezug auf eine Anzeige, die mit der Medienvorrichtung verknüpft ist; und</claim-text>
<claim-text>einer Zeitdauer, während der das Bildschirmobjekt gerendert wird.</claim-text></claim-text></claim>
<claim id="c-de-01-0005" num="0005">
<claim-text>Medienvorrichtung (108, 208) zum Erhalten von Trainingsdaten, um das Rendern eines oder mehrerer Videoframes zu testen, wobei die Medienvorrichtung umfasst:
<claim-text>einen Empfänger, um einen Videostream (102, 202) zu erhalten, der eine Vielzahl von dynamischen Videoframes umfasst;</claim-text>
<claim-text>ein Abspielgerät, das einen oder mehrere Prozessoren umfasst, um den erhaltenen Videostream (102, 202) zu rendern; und</claim-text>
<claim-text>eine Anzeige, die kommunikationsmäßig mit dem Abspielgerät gekoppelt ist, um den gerenderten Videostream als eine visuelle Ausgabe anzuzeigen,</claim-text>
<claim-text>wobei die Medienvorrichtung (108, 208) so ausgelegt ist, dass sie die Verfahrensschritte nach einem der Ansprüche 1 bis 4 implementiert.</claim-text></claim-text></claim>
<claim id="c-de-01-0006" num="0006">
<claim-text>Computerimplementiertes Verfahren zum Trainieren eines künstlichen neuronalen Netzwerks, um Bildschirmobjekte in einem oder mehreren Videoframes, die auf oder unter Verwendung einer Medienvorrichtung gerendert werden, zu erkennen, wobei das Verfahren unter Verwendung eines oder mehrerer Prozessoren, die mit einem Speicher verknüpft sind, implementiert wird, wobei das Verfahren umfasst:
<claim-text>Initialisieren eines künstlichen neuronalen Netzwerks im Speicher, um Daten, die mit einer Vielzahl von Videoframes verknüpft sind, zu erhalten und/oder zu speichern;<!-- EPO <DP n="27"> --></claim-text>
<claim-text>Erhalten von Trainingsdaten, die eine Vielzahl von gerenderten Frames eines Videostreams, der auf oder unter Verwendung des Medienabspielgeräts gerendert wurde, einschließen, als eine erste Eingabe;</claim-text>
<claim-text>Erhalten von Trainingsdaten, die Datenelemente, welche mit jedem der gerenderten Frames der ersten Eingabe verknüpft sind, einschließen, als eine zweite Eingabe, wobei jedes Datenelement das Vorhandensein oder Fehlen eines weiteren Bildschirmobjekts in einem gegebenen Frame angibt;</claim-text>
<claim-text>Erhalten von extrahierten Metadaten, die das Vorhandensein oder Fehlen eines weiteren Bildschirmobjekts in einem gegebenen Frame angeben, als eine Ausgabe des künstlichen neuronalen Netzwerks;</claim-text>
<claim-text>Anwenden einer oder mehrerer Funktionen, um einen Fehler zwischen den ausgegebenen extrahierten Metadaten des künstlichen neuronalen Netzwerks und dem entsprechenden Datenelement für den gegebenen Frame zu reduzieren;</claim-text>
<claim-text>wobei die erste und die zweite Eingabe über das Verfahren zum Erhalten von Trainingsdaten nach einem der Ansprüche 1 bis 4 bereitgestellt werden.</claim-text></claim-text></claim>
<claim id="c-de-01-0007" num="0007">
<claim-text>Verfahren nach Anspruch 6, wobei das Trainieren des künstlichen neuronalen Netzwerks weiter umfasst:
<claim-text>Erkennen des Vorhandenseins des einen oder der mehreren spezifischen Merkmale in den gerenderten Videoframes; und</claim-text>
<claim-text>Bereitstellen, auf Basis des Ergebnisses des Erkennungsschritts, eines binären Ausgabe-Identifikators, um das Vorhandensein oder Fehlen eines weiteren Bildschirmobjekts in den gerenderten Videoframes anzugeben.</claim-text></claim-text></claim>
<claim id="c-de-01-0008" num="0008">
<claim-text>Verfahren zum Testen von Videodaten, die auf oder unter Verwendung einer Medienvorrichtung (108, 208) gerendert werden, wobei das Verfahren umfasst:
<claim-text>Empfangen einer Vielzahl von Videoframes, die in Übereinstimmung mit ursprünglichen Metadaten gerendert werden sollen, an einer Medienvorrichtung (108, 208), wobei jeder Frame ein oder mehrere primäre Bildschirmobjekte und mindestens ein weiteres Bildschirmobjekt umfasst;</claim-text>
<claim-text>Rendern der empfangenen Frames auf oder unter Verwendung der Medienvorrichtung (108, 208), wobei das mindestens eine weitere Bildschirmobjekt beim Rendern dem einen oder den mehreren primären Bildschirmobjekten eines gegebenen<!-- EPO <DP n="28"> --> Frames überlagert wird, wobei die empfangene Vielzahl von Frames in Übereinstimmung mit ursprünglichen Metadaten gerendert wird;</claim-text>
<claim-text>Bereitstellen der gerenderten Frames für ein künstliches neuronales Netzwerk, wobei das künstliche neuronale Netzwerk in Übereinstimmung mit dem Verfahren nach Anspruch 6 oder Anspruch 7 trainiert ist;</claim-text>
<claim-text>Erhalten von extrahierten Metadaten als eine Ausgabe des künstlichen neuronalen Netzwerks, wobei die extrahierten Metadaten das Vorhandensein oder Fehlen eines weiteren Bildschirmobjekts in einem gegebenen gerenderten Frame erkennen;</claim-text>
<claim-text>Bereitstellen der ursprünglichen Metadaten, die mit der Vielzahl von Videoframes verknüpft sind, für das künstliche neuronale Netzwerk; und</claim-text>
<claim-text>Validieren des Renderings eines erkannten weiteren Bildschirmobjekts auf Basis der ursprünglichen Metadaten, die sich auf den gegebenen Videoframe beziehen.</claim-text></claim-text></claim>
<claim id="c-de-01-0009" num="0009">
<claim-text>Verfahren nach Anspruch 8, wobei der Schritt des Validierens weiter umfasst:
<claim-text>Identifizieren eines spezifischen Merkmals des erkannten weiteren Bildschirmobjekts;</claim-text>
<claim-text>Vergleichen des identifizierten Merkmals mit einem zugehörigen spezifischen Merkmal in den ursprünglichen Metadaten; und</claim-text>
<claim-text>Erkennen des Vorhandenseins einer Abweichung des identifizierten spezifischen Merkmals auf Basis des Ergebnisses des Vergleichsschritts.</claim-text></claim-text></claim>
<claim id="c-de-01-0010" num="0010">
<claim-text>Verfahren nach 9, weiter umfassend:
<claim-text>in Reaktion darauf, dass eine Abweichung erkannt wird, Berechnen eines Versatzes auf Basis der erkannten Abweichung;</claim-text>
<claim-text>Bereitstellen des Versatzes als Rückmeldung für einen Prozessor, der mit der Medienvorrichtung (108, 208) verknüpft ist; und</claim-text>
<claim-text>Anpassen eines anschließenden Renderings der Videoframes auf Basis des Versatzes.</claim-text></claim-text></claim>
<claim id="c-de-01-0011" num="0011">
<claim-text>System zum Testen von Videodaten, die auf oder unter Verwendung einer Medienvorrichtung (108, 208) gerendert werden, wobei das System die Medienvorrichtung (108, 208) und mindestens einen Prozessor einschließt, der mit einem Speicher verknüpft ist, um eine Vielzahl von Anweisungen zu speichern, wobei der<!-- EPO <DP n="29"> --> mindestens eine Prozessor so ausgelegt ist, dass er die Vielzahl von Anweisungen ausführt, um das Verfahren nach einem der Ansprüche 8 bis 10 zu implementieren.</claim-text></claim>
</claims>
<claims id="claims03" lang="fr"><!-- EPO <DP n="30"> -->
<claim id="c-fr-01-0001" num="0001">
<claim-text>Procédé de création de données d'apprentissage pour apprendre à un réseau neuronal artificiel la détection d'objets d'écran dans une ou plusieurs trames vidéo rendues au niveau ou en utilisant un dispositif multimédia (108, 208), le procédé comprenant :
<claim-text>le rendu au niveau d'un dispositif multimédia (108, 208), d'une pluralité de trames vidéo d'un flux vidéo source (102, 202), chaque trame comprenant un ou plusieurs objets d'écran primaires, le flux vidéo source (102, 202) comprenant en outre des métadonnées d'origine (104, 204) pour décrire un objet d'écran supplémentaire à rendre dans au moins l'une de la pluralité de trames vidéo, dans lequel l'objet d'écran supplémentaire est configuré pour être superposé sur les un ou plusieurs objets d'écran primaires d'une trame donnée pendant le rendu ;</claim-text>
<claim-text>le rendu de chaque trame de la pluralité de trames au moins une fois avec l'objet d'écran supplémentaire et au moins une fois sans l'objet d'écran supplémentaire ; et</claim-text>
<claim-text>la création d'un ensemble de données d'apprentissage incluant un ensemble de trames vidéo rendues (112, 212), chaque trame dans l'ensemble de données d'apprentissage correspondant à l'une de la pluralité de trames vidéo rendues avec ou sans l'objet d'écran supplémentaire, et incluant en outre un élément de données correspondant pour chaque trame indiquant la présence ou l'absence d'un objet d'écran supplémentaire dans chaque trame ;</claim-text>
<claim-text>dans lequel les un ou plusieurs objets d'écran primaires comprennent des images dynamiques et/ou des caractères optiques et l'objet d'écran supplémentaire est un objet d'écran textuel temporisé.</claim-text></claim-text></claim>
<claim id="c-fr-01-0002" num="0002">
<claim-text>Procédé selon la revendication 1, incluant en outre l'obtention de l'élément de données correspondant pour chaque trame, dans lequel chaque élément de données inclut une étiquette descriptive pour une trame donnée, dans lequel ladite obtention inclut l'analyse de la trame donnée par un observateur pour identifier la présence ou l'absence d'un objet d'écran supplémentaire dans la trame.<!-- EPO <DP n="31"> --></claim-text></claim>
<claim id="c-fr-01-0003" num="0003">
<claim-text>Procédé selon l'une quelconque des revendications précédentes, dans lequel les métadonnées d'origine comprennent une ou plusieurs propriétés associées à chaque objet d'écran primaire et/ou supplémentaire dans chaque trame vidéo.</claim-text></claim>
<claim id="c-fr-01-0004" num="0004">
<claim-text>Procédé selon la revendication 3, dans lequel les une ou plusieurs propriétés comprennent au moins l'un parmi :
<claim-text>une position de l'objet d'écran dans une trame vidéo ;</claim-text>
<claim-text>un horodatage associé au cas où l'objet d'écran est rendu ;</claim-text>
<claim-text>des dimensions de l'objet d'écran lorsqu'il est rendu par rapport à un affichage associé au dispositif multimédia ; et</claim-text>
<claim-text>une durée pendant laquelle l'objet d'écran est rendu.</claim-text></claim-text></claim>
<claim id="c-fr-01-0005" num="0005">
<claim-text>Dispositif multimédia (108, 208) d'obtention de données d'apprentissage pour tester le rendu d'une ou plusieurs trames vidéo, le dispositif multimédia comprenant :
<claim-text>un récepteur pour obtenir un flux vidéo (102, 202) qui comprend une pluralité de trames vidéo dynamiques ;</claim-text>
<claim-text>un lecteur comprenant un ou plusieurs processeurs pour rendre le flux vidéo obtenu (102, 202) ; et</claim-text>
<claim-text>un affichage couplé en communication avec le lecteur pour afficher le flux vidéo rendu en tant que sortie visuelle,</claim-text>
<claim-text>dans lequel le dispositif multimédia (108, 208) est configuré pour mettre en œuvre les étapes du procédé selon l'une quelconque des revendications 1 à 4.</claim-text></claim-text></claim>
<claim id="c-fr-01-0006" num="0006">
<claim-text>Procédé mis en œuvre par ordinateur pour apprendre à un réseau neuronal artificiel la détection d'objets d'écran dans une ou plusieurs trames vidéo rendues au niveau ou en utilisant un dispositif multimédia, le procédé mis en œuvre utilisant un ou plusieurs processeurs qui sont associés à une mémoire, le procédé comprenant :
<claim-text>l'initialisation d'un réseau neuronal artificiel dans la mémoire pour obtenir et/ou stocker des données associées à une pluralité de trames vidéo ;</claim-text>
<claim-text>l'obtention, en tant que première entrée, des données d'apprentissage incluant une pluralité de trames rendues d'un flux vidéo rendu au niveau ou en utilisant le dispositif multimédia ;<!-- EPO <DP n="32"> --></claim-text>
<claim-text>l'obtention, en tant que seconde entrée, des données d'apprentissage incluant des éléments de données associés à chacune des trames rendues de la première entrée, chaque élément de données indiquant la présence ou l'absence d'un objet d'écran supplémentaire dans une trame donnée ;</claim-text>
<claim-text>l'obtention, en tant que sortie du réseau neuronal artificiel, de métadonnées extraites indiquant la présence ou l'absence d'un objet d'écran supplémentaire dans une trame donnée ;</claim-text>
<claim-text>l'application d'une ou plusieurs fonctions pour réduire une erreur entre les métadonnées extraites de sortie du réseau neuronal artificiel et les éléments de données correspondants pour la trame donnée ;</claim-text>
<claim-text>dans lequel les première et seconde entrées sont fournies dans le procédé d'obtention de données d'apprentissage selon l'une quelconque des revendications 1 à 4.</claim-text></claim-text></claim>
<claim id="c-fr-01-0007" num="0007">
<claim-text>Procédé selon la revendication 6, dans lequel l'apprentissage du réseau neuronal artificiel comprend en outre :
<claim-text>la détection de la présence des une ou plusieurs caractéristiques spécifiques dans les trames vidéo rendues ; et</claim-text>
<claim-text>la fourniture d'un identifiant de sortie binaire pour indiquer la présence ou l'absence d'un objet d'écran supplémentaire dans les trames vidéo rendues, sur la base du résultat de l'étape de détection.</claim-text></claim-text></claim>
<claim id="c-fr-01-0008" num="0008">
<claim-text>Procédé de test de données vidéo rendues au niveau ou en utilisant un dispositif multimédia (108, 208), le procédé comprenant :
<claim-text>la réception au niveau d'un dispositif multimédia (108, 208) d'une pluralité de trames vidéo à rendre en fonction de métadonnées d'origine, chaque trame comprenant un ou plusieurs objets d'écran primaires et au moins un objet d'écran supplémentaire ;</claim-text>
<claim-text>le rendu des trames reçues au niveau ou en utilisant le dispositif multimédia (108, 208) dans lequel l'au moins un objet d'écran supplémentaire est superposé sur les un ou plusieurs objets d'écran primaires d'une trame donnée pendant le rendu, la pluralité reçue de trames étant rendues en fonction de métadonnées d'origine ;</claim-text>
<claim-text>la fourniture des trames rendues à un réseau neuronal artificiel, ledit réseau neuronal artificiel suivant un apprentissage en fonction du procédé selon la revendication 6 ou la revendication 7 ;<!-- EPO <DP n="33"> --></claim-text>
<claim-text>l'obtention de métadonnées extraites en tant que sortie du réseau neuronal artificiel, les métadonnées extraites détectant la présence ou l'absence d'un objet d'écran supplémentaire dans une trame rendue donnée ;</claim-text>
<claim-text>la fourniture des métadonnées d'origine associées à la pluralité de trames vidéo au réseau neuronal artificiel ; et</claim-text>
<claim-text>la validation du rendu d'un objet d'écran supplémentaire détecté sur la base des métadonnées d'origine concernant la trame vidéo donnée.</claim-text></claim-text></claim>
<claim id="c-fr-01-0009" num="0009">
<claim-text>Procédé selon la revendication 8, dans lequel l'étape de validation comprend en outre :
<claim-text>l'identification d'une caractéristique spécifique de l'objet d'écran supplémentaire détecté ;</claim-text>
<claim-text>la comparaison de la caractéristique identifiée à une caractéristique spécifique liée dans les métadonnées d'origine ; et</claim-text>
<claim-text>la détection de la présence d'une variation de la caractéristique spécifique identifiée sur la base du résultat de l'étape de comparaison.</claim-text></claim-text></claim>
<claim id="c-fr-01-0010" num="0010">
<claim-text>Procédé selon 9, comprenant en outre :
<claim-text>en réponse à une variation détectée, le calcul d'un écart sur la base de la variation détectée ;</claim-text>
<claim-text>la fourniture de l'écart en tant que retour à un processeur associé au dispositif multimédia (108, 208) ; et</claim-text>
<claim-text>l'ajustement d'un rendu suivant des trames vidéo sur la base de l'écart.</claim-text></claim-text></claim>
<claim id="c-fr-01-0011" num="0011">
<claim-text>Système de test de données vidéo rendues au niveau ou en utilisant un dispositif multimédia (108, 208), le système incluant le dispositif multimédia (108, 208) et au moins un processeur associé à une mémoire pour stocker une pluralité d'instructions, dans lequel l'au moins un processeur est configuré pour exécuter la pluralité d'instructions pour mettre en œuvre le procédé selon l'une quelconque des revendications 8 à 10.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="34"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="115" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> -->
<figure id="f0002" num="1a"><img id="if0002" file="imgf0002.tif" wi="117" he="108" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> -->
<figure id="f0003" num="2"><img id="if0003" file="imgf0003.tif" wi="146" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> -->
<figure id="f0004" num="3"><img id="if0004" file="imgf0004.tif" wi="146" he="222" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> -->
<figure id="f0005" num="4"><img id="if0005" file="imgf0005.tif" wi="139" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> -->
<figure id="f0006" num="5"><img id="if0006" file="imgf0006.tif" wi="148" he="233" img-content="drawing" img-format="tif"/></figure>
</drawings>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="KR20170047547"><document-id><country>KR</country><doc-number>20170047547</doc-number></document-id></patcit><crossref idref="pcit0001">[0002]</crossref></li>
</ul></p>
<heading id="ref-h0003"><b>Non-patent literature cited in the description</b></heading>
<p id="ref-p0003" num="">
<ul id="ref-ul0002" list-style="bullet">
<li><nplcit id="ref-ncit0001" npl-type="s" url="https://machinelearnings.co/automatic-subtitle-synchronization-e188a9275617"><article><author><name>A. SABATER</name></author><atl/><serial><sertitle>Automatic Subtitle Synchronization - Machine Learnings</sertitle><pubdate><sdate>20170914</sdate><edate/></pubdate></serial></article></nplcit><crossref idref="ncit0001">[0002]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
