<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP20213548A1" file="EP20213548NWA1.xml" lang="en" country="EP" doc-number="3890327" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3890327</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>20213548.9</B210><B220><date>20201211</date></B220><B240><B241><date>20210118</date></B241></B240><B250>en</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>202063002604 P</B310><B320><date>20200331</date></B320><B330><ctry>US</ctry></B330><B310>202016998579</B310><B320><date>20200820</date></B320><B330><ctry>US</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>H04N  21/2187      20110101AFI20210505BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>H04N  21/239       20110101ALI20210505BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>H04N  21/258       20110101ALI20210505BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>H04N  21/2743      20110101ALI20210505BHEP        </text></classification-ipcr><classification-ipcr sequence="5"><text>H04N  21/4223      20110101ALI20210505BHEP        </text></classification-ipcr><classification-ipcr sequence="6"><text>H04N  21/472       20110101ALI20210505BHEP        </text></classification-ipcr><classification-ipcr sequence="7"><text>H04N  21/81        20110101ALI20210505BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>H04N  21/8133      20130101 LI20210503BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>H04N  21/47205     20130101 LI20210503BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>H04N  21/2743      20130101 LI20210503BHEP        </text></classification-cpc><classification-cpc sequence="4"><text>H04N  21/2396      20130101 LI20210503BHEP        </text></classification-cpc><classification-cpc sequence="5"><text>H04N  21/2187      20130101 FI20210503BHEP        </text></classification-cpc><classification-cpc sequence="6"><text>H04N  21/4223      20130101 LI20210503BHEP        </text></classification-cpc><classification-cpc sequence="7"><text>H04N  21/25875     20130101 LI20210503BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>VERFAHREN UND VORRICHTUNG ZUM AUFNEHMEN UND STREAMEN VON INHALTEN</B542><B541>en</B541><B542>METHOD AND DEVICE FOR CONTENT RECORDING AND STREAMING</B542><B541>fr</B541><B542>PROCÉDÉ ET DISPOSITIF POUR L'ENREGISTREMENT ET LA DIFFUSION DE CONTENU</B542></B540><B590><B598>1</B598></B590></B500><B700><B710><B711><snm>Northwest Instrument Inc.</snm><iid>101839850</iid><irf>N 1024-EP /KT</irf><adr><str>69 Kings Street</str><city>Dover, NJ 07801</city><ctry>US</ctry></adr></B711></B710><B720><B721><snm>XING, David</snm><adr><str>69 Kings Street</str><city>Dover, NJ 07801</city><ctry>US</ctry></adr></B721></B720><B740><B741><snm>Otten, Roth, Dobler &amp; Partner mbB Patentanwälte</snm><iid>100810373</iid><adr><str>Großtobeler Straße 39</str><city>88276 Berg / Ravensburg</city><ctry>DE</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">A content recording and sharing method is applied to a computing device. The method includes: obtaining a video in real time, the video capturing a physical scene for a time duration; generating supplemental contents synchronized with the video; receiving user information from a first user terminal device; determining first selected supplemental content according to the user information and a first selection rule, and sending the first selected supplemental content to the first user terminal device; determining second selected supplemental content according to a second selection rule; and sharing the video and the second selected supplemental content with one or more second user terminal devices.
<img id="iaf01" file="imgaf001.tif" wi="154" he="89" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001"><b>CROSS-REFERENCE TO RELATED APPLICATION</b></heading>
<p id="p0001" num="0001">This application claims the priority of U. S. Provisional Patent Application No. <patcit id="pcit0001" dnum="US63002604B"><text>63/002,604</text></patcit>, titled "Method and Device for Content Recording and Streaming", filed on March 31, 2020, the entire contents of which are incorporated herein by reference.</p>
<heading id="h0002"><b>TECHNICAL FIELD</b></heading>
<p id="p0002" num="0002">The present disclosure generally relates to audio and video processing technology and augmented reality (AR) technology, and in particular, to systems, methods, and devices for recording and streaming content.</p>
<heading id="h0003"><b>BACKGROUND</b></heading>
<p id="p0003" num="0003">Video and audio recording and streaming of live scenes are becoming a key attraction for users of computing devices. For instance, more and more users are enjoying the benefits of creating videos at live events, such as sports events and live performances, and sharing the recorded video with an audience either in real time or after the event.</p>
<p id="p0004" num="0004">Currently, a participant or an event personnel may use a mobile device, such as a smart phone or a tablet, or a digital video (DV) recorder to record live videos. However, these devices in general only record real scenes in the physical environment. Due to development of wireless technology, live event organizers may deliver additional contents to the on-site audiences, such<!-- EPO <DP n="2"> --> as real-time prompt information or interactive information between performers and audiences. In particular, the development of augmented reality (AR) technology may allow stage performers to add real-time special effects or interactive information in a digital format. When the on-site audiences want to record live scenes for sharing, they may either record real-scene videos of the live event, or they may record video frames capturing all the information, including the additional contents, blended together in the video frames. They cannot selectively remove certain contents that they do not wish to display during sharing or stored playback. This limitation may negatively affect user experience of such a recording systems. In addition, for the event organizers, they may not wish to share certain information to outside viewers who are not a participant or attendee of the event. For example, they may wish to exclude certain on-site prompt or interactive information from videos shared to public audiences. The current recording systems do not provide selective recording of real-scenes and supplemental contents, and do not allow streaming systems to selectively broadcast or display the recorded contents. The present disclosure in part aims to address the limitations in the existing systems.</p>
<heading id="h0004"><b>SUMMARY</b></heading>
<p id="p0005" num="0005">In one aspect, the present disclosure provides a content recording and sharing method. The method may be applied to a computing device, and includes: obtaining a video in real time, the video capturing a physical scene for a time duration; generating supplemental contents synchronized with the video; receiving user information from a first user terminal device; determining first selected supplemental content according to the user information and a first selection rule, and sending the first selected supplemental content to the first user terminal device; determining second selected supplemental content according to a second selection rule;<!-- EPO <DP n="3"> --> and sharing the video and the second selected supplemental content with one or more second user terminal devices.</p>
<p id="p0006" num="0006">In certain embodiments, the method further includes controlling a video camera to capture the video.</p>
<p id="p0007" num="0007">In certain embodiments, the user information indicates that the first user terminal device is associated with an on-site participant of an event associated with the physical scene in the time duration; and the first selected supplemental content includes content that is not included in the second supplemental content.</p>
<p id="p0008" num="0008">In certain embodiments, the user information includes one or more of: a user ID; user credential data; user location data; owner data of the user terminal device; and user preference data.</p>
<p id="p0009" num="0009">In certain embodiments, the supplemental contents include one or more of: information related to a content of the video; information related to a target object in the video; a special effect; one or more user feedback; a notification from a physical environment; and navigation information.</p>
<p id="p0010" num="0010">In certain embodiments, the supplemental contents include one or more of a text, an image, an audio signal, an image, and an animation.</p>
<p id="p0011" num="0011">In certain embodiments, the method further includes: after receiving the user information from the user terminal device, determining a user category according to the user information, and determining the first selected supplemental content according to the user category.<!-- EPO <DP n="4"> --></p>
<p id="p0012" num="0012">In certain embodiments, the method further includes: after receiving the user information from the user terminal device, determining a user location according to the user information, and determining the first selected supplemental content according to the user location.</p>
<p id="p0013" num="0013">In certain embodiments, the user information includes a current location associated with the user terminal device; and the first selected supplemental content includes navigation information guiding a user from the current location to a designated seating location for the user.</p>
<p id="p0014" num="0014">In another aspect, the present disclosure provides another content recording and sharing method. The method may be applied to a user terminal device, and includes: displaying a video in an interface of the user terminal device in real time, the video capturing a physical scene for a time duration; receiving supplemental contents from a server in real time; displaying the supplemental contents superimposed on the video in the interface; storing the video and the supplemental contents; determining selected supplemental content for sharing according to a selection rule; and uploading the video and the selected supplemental content to a video sharing platform.</p>
<p id="p0015" num="0015">In certain embodiments, displaying the video in the interface is initiated in response to receiving a first user instruction; storing the video and the supplemental contents is initiated in response to receiving a second user instruction; and uploading the video and the selected supplemental content is initiated in response to receiving a third user instruction.</p>
<p id="p0016" num="0016">In certain embodiments, the selected supplemental content is a subset of the supplemental contents.<!-- EPO <DP n="5"> --></p>
<p id="p0017" num="0017">In certain embodiments, displaying the supplemental contents superimposed on the video in the interface includes displaying the supplemental contents according to a first user configuration.</p>
<p id="p0018" num="0018">In certain embodiments, storing the video and the supplemental contents includes storing the video and the supplemental contents in synchronization according to a second user configuration.</p>
<p id="p0019" num="0019">In certain embodiments, uploading the video and the selected supplemental content includes uploading the video and the selected supplemental content according to a third user configuration.</p>
<p id="p0020" num="0020">In certain embodiments, the method further includes: before receiving the supplemental contents from the server, sending user information to the server.</p>
<p id="p0021" num="0021">In another aspect of the present disclosure, a device for performing content recording and sharing is provided. The device includes: a non-transitory computer-readable storage medium storing a plurality of computer-executable instructions; and a processor, coupled with the non-transitory computer-readable storage medium and, when executing the computer-executable instructions, configured to: obtain a video in real time, the video capturing a physical scene for a time duration; generate supplemental contents synchronized with the video; receive user information from a first user terminal device; determine first selected supplemental content according to the user information and a first selection rule, and sending the first selected supplemental content to the first user terminal device; determine second selected supplemental content according to a second selection rule; and share the video and the second selected supplemental content with one or more second user terminal devices.<!-- EPO <DP n="6"> --></p>
<p id="p0022" num="0022">In certain embodiments, the processor is further configured to: after receiving the user information from the user terminal device, determine a user category according to the user information, and determine the first selected supplemental content according to the user category.</p>
<p id="p0023" num="0023">In certain embodiments, the processor is further configured to: after receiving the user information from the user terminal device, determine a user location according to the user information, and determine the first selected supplemental content according to the user location.</p>
<p id="p0024" num="0024">In certain embodiments, the processor is further configured to upload the video superimposed with the second selected supplemental content in real time to a video hosting platform to cause the video hosting platform live broadcasting of the video superimposed with the second selected supplemental content..</p>
<heading id="h0005"><b>BRIEF DESCRIPTION OF THE DRAWINGS</b></heading>
<p id="p0025" num="0025">In order to more clearly illustrate the technical solutions in the embodiments of the present disclosure, the drawings used in the description of the embodiments will be briefly described below. It is obvious that the drawings in the following description are only some embodiments of the present disclosure. Other drawings may be obtained by those of ordinary skill in the art based on these drawings.
<ul id="ul0001" list-style="none">
<li><figref idref="f0001">FIG. 1</figref> illustrates an application scenario of the content streaming and recording system according to certain embodiments;</li>
<li><figref idref="f0002">FIG. 2</figref> illustrates a content recording and sharing method according to certain embodiments;<!-- EPO <DP n="7"> --></li>
<li><figref idref="f0003">FIG. 3</figref> illustrates a content recording and sharing method according to certain other embodiments; and</li>
<li><figref idref="f0004">FIG. 4</figref> illustrates a device configuration according to certain embodiments of the present disclosure.</li>
</ul></p>
<heading id="h0006"><b>DETAILED DESCRIPTION</b></heading>
<p id="p0026" num="0026">The technical solutions according to the embodiments of the present disclosure are described in the following with reference to the accompanying drawings. The described embodiments are only part of the embodiments of the present disclosure, but not all the embodiments. All other embodiments obtained by a person of ordinary skill in the art based on the embodiments of the present disclosure without creative efforts are within the scope of the present disclosure.</p>
<p id="p0027" num="0027">The present disclosure provides a method, a device, and a system for synchronous recording and playback of real scenes and supplemental contents in real time, as well as selectively outputting desired content during recording, playback, and sharing according to targeted audience or system or user configuration. For example, the method, device, and system of the present disclosure may provide the following functions:
<ol id="ol0001" ol-style="">
<li>1. performing on-site real-time recording of real scenes in a physical environment;</li>
<li>2. superimposing supplemental contents on the videos of real-scene recording, where the supplemental contents may contents such as interactive information, special effects, feedback from online and off-line users including comments, thumbs-up, digital gifts, and so on, and the contents may be in the forms of text, images, videos, audios, animations, and other AR contents;<!-- EPO <DP n="8"> --></li>
<li>3. storing, exporting, and replaying the video contents, where during playback, supplemental contents may be selectively superimposed on the real-scene videos;</li>
<li>4. allowing a user to choose a time or a place for recording and playback, and providing different manners for display according to different scenes;</li>
<li>5. providing language support functions such as real-time voice recognition, voice-to-text transcription, voice translation, and so on; and</li>
<li>6. providing supplementary indications and instructions, such as giving directions, providing route navigation, and so on.</li>
</ol></p>
<p id="p0028" num="0028">Further, according to certain embodiments of the present disclosure, a display terminal for real scene display and playback may be a smart device such as a smart phone, AR glasses, a tablet computer, or a TV or a screen projector, and so on. The system, device, and method can be applied to a wide variety of application scenarios involving live events, such as concerts, lectures, competitions, symposiums, seminars, face-to-face interviews and so on.</p>
<p id="p0029" num="0029">In one aspect of the present disclosure, a content streaming and recording system is provided. <figref idref="f0001">FIG. 1</figref> shows an application scenario of the content streaming and recording system 100 according to certain embodiments. As shown in <figref idref="f0001">FIG. 1</figref>, the content streaming and recording system 100 may include a camera 110, a control interface 120, a wireless communication unit 140, a public network 150, a data server 160, a processing server 170, a user terminal 180, and a display interface 190. The camera 110 may be configured to record videos of a real scene 130 in a physical environment. The real scene 130 may also be termed as a physical scene and it is a scene taking place in the physical environment instead of a computer-generated or a camera-recorded<!-- EPO <DP n="9"> --> scene. The display interface 190 may be configured to display scenes recorded by the camera 110. The control interface 120 may be configured to receive user inputs. In some embodiments, the control interface 120 may include a physical control panel. In certain other embodiments, the control interface may include a software interface displayed on a screen. The control interface 120 and the display interface 190 may be associated with one or more processors that execute command to operate the control interface 120 and the display interface 190. The control interface 120 and the display interface 190 may communicate with the user terminal through the public network 150. In certain embodiments, the control interface 120 and the display interface 190 may communicate with the data server 160 and the processing server 170 through the public network 150. The data server 160 and the processing server 170 may process the recorded real-scene video and supplemental contents to create a customized AR display. In certain embodiments, the user terminal may employ augmented reality (AR) technology, and display real-time special effects or interactive information in as AR contents.</p>
<p id="p0030" num="0030">The control interface 190 may control whether the display interface 190 displays the recorded real-scene videos in real time. It may further control whether to superimpose supplemental contents on the real-scene videos on the display interface 190 in real time. The supplemental contents may include supplemental information of a scene in the real-scene videos (e. g., subtitles). The supplemental contents may also include supplemental information of a target object in the real-scene videos, such as text, images, or animations providing additional information about the target object. Further, the supplemental contents may include notification information from a surrounding environment, such as notification from a theater where the live event takes place. Supplemental information customized to specific audience may also be<!-- EPO <DP n="10"> --> included, such as multilingual annotations, and feedback from off-site viewers (such as discussions of friends outside the theater about shared videos) may also be included.</p>
<p id="p0031" num="0031">The control interface may also control whether to upload and share the recorded live videos and supplemental contents in real time through the wireless communication component 140. The processing server 170 may push supplemental information that matches the current content and the subscribed information that matches a user to the public network 150 through the server 160.</p>
<p id="p0032" num="0032">The processing server 170 may receive the recorded real-scene videos as well as the supplemental contents displayed on the on-site display interface with the real-scene videos uploaded and shared by the on-site users in real time from the network 150 through the data server 160. The processing server 170 may further perform secondary processing on the received video data and supplemental contents, and upload the processed contents to the public network 150 through the data server 160.</p>
<p id="p0033" num="0033">In certain embodiments, the control interface may control uploading and sharing the recorded live videos and supplemental contents in real time as the contents are being recorded. An off-site viewer may receive real-time contents uploaded and shared by on-site users from on the public network 150 through the user terminal 8, enabling content sharing and discussion. The off-site viewer may also receive real-time contents from the processing server 170 via the public network 150 through the user terminal 8, enabling superimpose display of contents such as commercial promotion while ensuring content privacy and security. In certain other embodiments, the control interface may control storing the recorded live videos and<!-- EPO <DP n="11"> --> supplemental contents in a memory device, and control uploading and sharing the recorded live videos and supplemental contents after the contents have been recorded.</p>
<p id="p0034" num="0034">In certain embodiments, the camera 110, the control interface 120, and the display interface 190 may be included in an electronic device, such as a smart device. A user at the location of a live event may be a member of the on-site audiences. The user may use the smart device to record the event and at the same time use the control interface on the smart device to configure the recording, playback, and sharing the recorded videos. In one example, the user may use the control interface to choose which supplemental contents to superimpose the playback video while recording. In another example, the user may choose which supplemental contents to upload with the recorded real-scene data to a video hosting platform.</p>
<p id="p0035" num="0035">In certain embodiments, the camera 110 may be separated from the display interface 190 and the control interface 120. For example, the camera 110 may be a DV camcorder operated by a photographer. Live video recorded by the DV camcorder may be transmitted to the display interface 190 in real time for playback. The control interface 120 may be operated by an event organizer to generate supplemental contents and to specify which supplemental contents can be shared according to a configuration. The configuration may include different audience categories, for example, on-site audience, VIP audience, and off-site audience. The configuration may further include an index of the supplemental contents allowed to be shared with a corresponding group. For example, the event organizer may allow a specific supplemental content to be delivered to the on-site audiences but not allow it to be uploaded to a public video hosting platforms or be shared to outside audiences. Through the control interface 120, the event organizer may further selectively deliver the supplemental contents to a specific audience based on audience information such as audience category, audience location, audience request, and so<!-- EPO <DP n="12"> --> on. Certain audience information may be acquired through wirelessly communicating with an application installed on a user mobile device. For example, the control interface 120 may communicate with the user mobile device and receive a location of an event attendee from the user mobile device. Once the location of the event attendee is determined, the control interface 120 may select supplemental contents to deliver to the user mobile device, such as AR superimpose on the recorded videos to provide direction and navigation information, location-specific special effects, and so on.</p>
<p id="p0036" num="0036">In an aspect of the present disclosure, a content recording and sharing method is provided. <figref idref="f0002">FIG. 2</figref> illustrates the content recording and sharing method 200 according to certain embodiments. The method may be implemented by a user terminal device. In certain embodiments, the user terminal device may be a mobile device used by a user who is an on-site participant or an on-site audience of a live event. In certain embodiments, the user terminal may employ augmented reality (AR) technology, and configured to display real-time special effects or interactive information in as AR contents. The user terminal device may have a display interface to display contents, an interactive interface for receiving user instructions, a communication unit to receive and send data via a network, and processor that executes computer program instructions to control operations of the display interface, the input interface, and communication unit. In certain embodiments, the user terminal device may further include a camera configured to record real-scene videos and a memory to store data. As shown in <figref idref="f0002">FIG. 2</figref>, the content recording and sharing method may include the following steps.</p>
<p id="p0037" num="0037">Step S202 is to initiate a display interface to display real-scene video in real time in response to receiving a first user instruction. The real-scene video is a video capturing a physical scene taking place in a physical environment. By contrast, the real-scene video is not a<!-- EPO <DP n="13"> --> computer-generated display or a video recording of a computer-generated or recorded display. In certain embodiments, the user terminal device may receive a user instruction through the interactive interface to start displaying real-scene videos on the display interface. In one example, an on-site participant or audience may use the camera of the user terminal device to capture real scenes at an event location. The captured real scene may be displayed on the display interface of the user terminal device. In another example, the user terminal device may receive real-scene video data from another camera at the event location, and display the video since on the display interface in real time.</p>
<p id="p0038" num="0038">Step S204 is to obtain the supplemental contents in real time. In certain embodiments, the supplemental contents may be received from a server in real time. For example, the event organizer may create supplemental contents synchronized with the real-scene videos to better inform the audience or to enhance their experience. The user terminal device may receive these supplemental contents. The supplemental contents may include supplemental information of a scene in the real-scene videos (e. g., subtitles). The supplemental contents may also include supplemental information of a target object in the real-scene videos, such as text, images, or animations providing additional information about the target object. Further, the supplemental contents may include notification information from a surrounding environment, such as notification from a theater or other types of venue where the live event takes place. For example, a theater where the event is taking place may send the seating information to the audience in real time, and may notify the audience the starting time, the finishing time, and/or the progress of the performance. In other examples, the venue may use the supplemental contents to timely broadcast emergency information to the on-site audience, including, for example, occurrence or forecast of a natural disaster warnings. Further, in other examples, the supplemental contents<!-- EPO <DP n="14"> --> may be used to notify off-site information to the audience in real time, such as traffic and weather conditions, or to provide assistance information such as real-time taxi service information. Supplemental information customized to specific audience may also be included, such as multilingual annotations. Feedback from on-site and off-site viewers (such as discussions of friends outside the theater about shared videos) may also be included in the supplemental information. In certain embodiments, the user terminal device may provide a user ID and credential data to the server, so that the server may authorize the user terminal device to receive certain supplemental contents. In certain embodiments, the user terminal device may provide additional information to the server, such as location data, owner data, user subscription and preference, and so on, so that the server may tailor supplemental contents provided to that specific user terminal device. In certain embodiments, the user may configure one or more of the owner data, user subscription data, and user preference data on the user terminal device. In certain embodiments, the sever may determine a user category according to one or more of the user ID, user credential data, location data, owner data, user subscription data, and user preference data, and determine which supplemental contents to be delivered to the specific user terminal device. For example, one or more of the user ID, user credential data, and user location data, and/or owner data may indicate that the user is a ticketed on-site participant, and the server may accordingly authorize the user to receive certain supplemental contents that are not available to a user not being an on-site participant. In another example, one or more of the user ID, user credential data, and user location data, and/or owner data may indicate that the user is a VIP on-site participant, and the server may accordingly authorize the user to receive additional supplemental contents. In another example, the server may use the location data to determine certain supplemental contents containing a visual effect specifically designed for a viewing angle<!-- EPO <DP n="15"> --> corresponding to the user location, and deliver the supplemental contents containing the special visual effect to the user terminal device, so the special visual effect may be displayed superimposing the real-scene video on the user terminal device. In yet another example, the server may use the location data and one or more of the user ID and/or credential data to determine certain supplemental contents containing navigation information for guiding the user from the current location to a designated seating location of the user, and deliver the supplemental contents containing the navigation information to the user terminal device to help guiding the user to find the designated seating location.</p>
<p id="p0039" num="0039">Step S206 is to display the supplemental contents superimposed on the real-scene video in the display interface according to a first configuration. In certain embodiments, the user terminal device may superimpose received supplemental contents to the real-scene video. The user terminal device may selectively display certain supplemental information, and display the selected supplemental information in a certain manner, according to a first configuration. In certain embodiments, the user terminal device may store configuration information, such as selection of a language, or whether to display a certain category of supplemental information. Certain configuration information may be entered by the user through a user interface. Certain other configuration information may be configured at the time when a software application corresponding to the display interface is installed on the user terminal device, or when the display interface is initiated. The configuration information may include the first configuration. In certain embodiments, the user may change the first configuration according to user need or preference.</p>
<p id="p0040" num="0040">Step S208 is to initiate storing real-scene video and the supplemental contents in response to a second user instruction. In certain embodiments, the user may choose to store the<!-- EPO <DP n="16"> --> real-scene video as well as the supplemental contents for later playback or sharing. In certain embodiments, the real-scene video and the supplemental contents may be stored in a same file with time information, so that they can be synchronized at playback. In certain other embodiments, the real-scene video and the supplemental contents may be stored in separate files, each having time information or synchronization information. In certain embodiments, the real-scene video and the supplemental contents may be stored according to a second user configuration in addition to the user instruction. The second user configuration may also be stored on the user terminal device and may be changed by the user. For example, the second user configuration may be used to specify video storage format, resolution, compression, as well as types of supplemental information to be stored. In certain embodiments, the second user configuration may be configured or edited by the event organizer, and the user terminal device may receive the configuration information from the server in real time. For example, the event organizer may allow a specific supplemental content to be delivered to the on-site audiences for viewing but not allow it to be stored to the user terminal device.</p>
<p id="p0041" num="0041">Step S210 is to determine selected supplemental contents for sharing according to a selection rule. The method in the present disclosure provides flexibility of sharing only selected supplemental contents to a specific group of audiences. For example, off-site viewers may not access certain supplemental contents due to the event organizer's preference or data security and privacy. In certain embodiments, an on-site user may choose to share the event video with outside viewers, and a subset of the supplemental contents the on-site user receives may be selected to share with the outside viewers along with the real-scene videos. The selection may be made according to a selection rule. In certain embodiments, the selection rule may be pre-configured, for example, as pre-configured parameters of a software application running on the<!-- EPO <DP n="17"> --> user terminal device. In certain embodiments, the selection rule may also be made or edited in real time by the event organizer, and the user terminal device may receive the selection rule from the server in real time. For example, the event organizer may allow a specific supplemental content to be delivered to the on-site audiences but not allow it to be uploaded to a public video hosting platforms or be shared to outside viewers. In certain other embodiments, the on-site user may edit certain aspect of the selection rule using the interactive interface of the user terminal device. For example, the on-site user may choose to share or not to share a specific supplemental content with a specific group of off-site viewers.</p>
<p id="p0042" num="0042">S212 is to share the real-scene video and selected supplemental contents with additional users according to a third configuration in response to a third user instruction. In certain embodiments, the sharing may be achieved by uploading the real-scene video and selected supplemental contents to a video hosting platform. The real-scene video and selected supplemental contents may be uploaded from the on-site user terminal device to a video hosting platform or a data server in order for other viewers to access them. The process may be conducted according to a third user configuration. In certain embodiments, the selected supplemental contents and the real-scene videos may be processed by the user terminal device to generate superimposed video contents for the sharing. In certain other embodiments, the selected supplemental contents and the real-scene videos may be first sent with their time or synchronization information to a processing server to generate the superimposed video contents.</p>
<p id="p0043" num="0043">In certain embodiments, the real-scene video and selected supplemental contents may be uploaded in real time as the contents are being recorded. Thus, an off-site viewer may receive real-time contents uploaded and shared by on-site users from a public network, enabling real time content sharing and discussion. In certain other embodiments, the real-scene video and<!-- EPO <DP n="18"> --> selected supplemental contents may be uploaded at a later time after the contents have been recorded. Additional post processing and editing may be performed on the stored contents before sharing.</p>
<p id="p0044" num="0044"><figref idref="f0003">FIG. 3</figref> illustrates another content recording and sharing method according to certain other embodiments. The method may be implemented by a computing device. In certain embodiments, the computing device may be a server used by an event personnel to provide content and media management for the event. The server may have a display interface to display contents, an interactive interface for receiving user instructions, a communication unit to receive and send data via a network, and processor that executes computer program instructions to control operations of the display interface, the input interface, and communication unit. In certain embodiments, the computing device may communicate with a camera configured to record real-scene videos and a memory to store data. As shown in <figref idref="f0003">FIG. 3</figref>, the content recording and sharing method 300 may include the following steps.</p>
<p id="p0045" num="0045">Step S302 is to receive real-scene videos in real time. In certain embodiments, the computing device may receive video data from a camera, for example, from a DV camcorder that records real-scene videos at an event location. In certain embodiments, the computing device may further control the camera to take real-scene videos while receiving video data from the camera.</p>
<p id="p0046" num="0046">Step S304 is to generate supplemental contents synchronized with the real-scene videos. In certain embodiments, the event organizer or personnel may create supplemental contents synchronized with the real-scene videos to better inform the audience or to enhance audience experience. The supplemental contents may include supplemental information of a scene in the<!-- EPO <DP n="19"> --> real-scene videos (e. g., subtitles). The supplemental contents may also include supplemental information of the physical scene or a target object in the real-scene videos, such as text, audio signal, images, or animations providing additional information about the physical scene or the target object. Further, the supplemental contents may include notification information from a surrounding environment, such as notification from a theater or other types of venue where the live event takes place.</p>
<p id="p0047" num="0047">Step S306 is to receive user information from an on-site user terminal device. The method provided by the present disclosure may offer the event organizer the flexibility to deliver supplemental contents customized to specific audience. Thus, in certain embodiments, the computing device may receive user information from a user terminal device. In certain embodiments, the user terminal device may provide user ID and credential data to the computing device, so that the computing device may authorize the user terminal device to receive certain supplemental contents. In certain embodiments, the user terminal device may provide additional information to the computing device, such as location data, owner data, user subscription and preference, and so on, so that the computing device may tailor supplemental contents provided to that specific user terminal device.</p>
<p id="p0048" num="0048">Step S308 is to send first selected supplemental contents to the on-site user terminal device according to the user information and/or a first selection rule. In certain embodiments, the computing device may select supplemental contents according to the user information and/or a first selection rule, and send selected supplemental contents to the on-site user terminal, so that an on-site user may view the targeted supplemental contents superimposed on the real-scene video. In certain embodiments, the computing device may determine a user category according to one or more of the user ID, user credential data, location data, owner data, user subscription<!-- EPO <DP n="20"> --> data, and user reference data, and determine which supplemental contents to be delivered to the specific user terminal device according to the user category. For example, the computing device may determine whether the user terminal device is associated with a user who is a ticketed on-site participant based on one or more of the user ID, user credential data, user location data, and/or owner data, and the computing device may accordingly authorize the user to receive certain supplemental contents that are not available to a user not being an on-site participant. In another example, the computing device may determine whether the user is a VIP participant based on one or more of the user ID, user credential data, and user location data, and/or owner data, and the computing device may accordingly authorize the user to receive additional supplemental contents targeted to VIP participants. In another example, the computing device may use the location data to determine certain supplemental contents containing a visual effect specifically designed for a viewing angle corresponding to the user location, and deliver the supplemental contents containing the special visual effect to the user terminal device, so the special visual effect may be displayed superimposing the real-scene video on the user terminal device. In yet another example, the computing device may use the location data and one or more of the user ID and/or credential data to determine certain supplemental contents containing navigation information for guiding the user from the current location to a designated seating location of the user, and deliver the supplemental contents containing the navigation information to the user terminal device to help guiding the user to find the designated seating location.</p>
<p id="p0049" num="0049">Step S310 is to determine second selected supplemental contents according to a second selection rule. The method in the present disclosure provides flexibility of sharing only selected supplemental contents to a specific group of audiences. For example, the event organizer may allow a specific supplemental content to be delivered to the on-site audiences but not allow it to<!-- EPO <DP n="21"> --> be uploaded to a public video hosting platforms or be shared to outside audiences. For example, off-site audiences may not access certain supplemental contents due to the event organizer's preference or data security and privacy. In certain embodiments, the computing device may configure a selection rule to specify which supplemental contents are to be shared with off-site viewers.</p>
<p id="p0050" num="0050">Step S312 is to share real-scene videos and the second selected supplemental contents with off-site viewers. In certain embodiments, the computing device may share the real-scene videos and the second selected supplemental contents with off-site viewers who are not direct participants of the event. In certain embodiments, the computing device may locally process the real-scene videos and the second selected supplemental contents to generate a superimposed video, and upload the superimposed video to a video hosting platform to share with off-site viewers. In certain other embodiments, to save computational resources, the computing device may send the real-scene videos and the second selected supplemental contents to a processing server to cause the processing server to superimpose the real-scene videos with the second selected supplemental contents to share with off-site viewers.</p>
<p id="p0051" num="0051">In certain embodiments, the real-scene video and selected supplemental contents may be sent to a processing server in real time as the contents are being recorded. Thus, an off-site viewer may receive real-time contents shared by the processing server from a public network, enabling real time content sharing and discussion. In certain other embodiments, the real-scene video and selected supplemental contents may be sent to the processing server at a later time after the contents have been recorded. Additional post processing and editing may be performed on the stored contents before sharing.<!-- EPO <DP n="22"> --></p>
<p id="p0052" num="0052"><figref idref="f0004">FIG. 4</figref> illustrates a device configuration for the user terminal device for implementing the method of <figref idref="f0002">FIG. 2</figref> or the computing device for implementing the method of <figref idref="f0003">FIG. 3</figref> according to certain embodiments. As shown in <figref idref="f0004">FIG. 4</figref>, the device 400 may be a computing device including a processor 402 and a storage medium 404. According to certain embodiments, the device 400 may further include a display 406, a communication module 408, and additional peripheral devices 412. Certain devices may be omitted, and other devices may be included.</p>
<p id="p0053" num="0053">Processor 402 may include any appropriate processor(s). In certain embodiments, processor 402 may include multiple cores for multi-thread or parallel processing. Processor 402 may execute sequences of computer program instructions to perform various processes, such as a neural network processing program. Storage medium 404 may be a non-transitory computer-readable storage medium, and may include memory modules, such as ROM, RAM, flash memory modules, and erasable and rewritable memory, and mass storages, such as CD-ROM, U-disk, and hard disk, etc. Storage medium 404 may store computer programs for implementing various processes, when executed by processor 402. The communication module 408 may include network devices for establishing connections through a network. Display 406 may include any appropriate type of computer display device or electronic device display (e.g., CRT or LCD based devices, touch screens). Peripherals 412 may include additional I/O devices, such as a keyboard, a mouse, and so on. The processor 402 may be configured to execute instructions stored on the storage medium 404 and perform various operations related to the content recording and sharing method as detailed in <figref idref="f0002">FIG. 2</figref> or <figref idref="f0003">FIG. 3</figref>.</p>
<p id="p0054" num="0054">The method and devices provided by the present disclosure may improve interaction efficiency of on-site users. For example, an on-site user may record the live event or performances while watching the event or performances and interacting with off-site users. The<!-- EPO <DP n="23"> --> method provided by the present disclosure enables the user to obtain supplemental contents according to customization of the user, thus improving user experience. Further, the method and devices provided by the present disclosure may offer improved commercial integration by presenting advertisement or notification in a more targeted and efficient manner.</p>
<p id="p0055" num="0055">The method and apparatus provided by the present disclosure according to the embodiments are described in detail above. The principles and implementation manners provided by the present disclosure are described herein by using specific examples. The description of the above embodiments is only used to help understand the method provided by the present disclosure. At the same time, a person skilled in the art will make changes the specific embodiments and the application scope according to the idea provided by the present disclosure. In summary, the contents of the present specification should not be construed as limiting the present disclosure.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="24"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A content recording and sharing method, applied to a computing device, comprising:
<claim-text>obtaining a video in real time, the video capturing a physical scene for a time duration;</claim-text>
<claim-text>generating supplemental contents synchronized with the video;</claim-text>
<claim-text>receiving user information from a first user terminal device; and</claim-text>
<claim-text>determining first selected supplemental content according to the user information and a first selection rule, and sending the first selected supplemental content to the first user terminal device.</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The method according to claim 1, further including:
<claim-text>determining second selected supplemental content according to a second selection rule; and</claim-text>
<claim-text>sharing the video and the second selected supplemental content with one or more second user terminal devices.</claim-text></claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The method according to claim 2, wherein:
<claim-text>the user information indicates that the first user terminal device is associated with an on-site participant of an event associated with the physical scene in the time duration; and</claim-text>
<claim-text>the first selected supplemental content includes content that is not included in the second supplemental content.</claim-text></claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>The method according to claim 4, wherein the user information includes one or more of:<!-- EPO <DP n="25"> -->
<claim-text>a user ID;</claim-text>
<claim-text>user credential data;</claim-text>
<claim-text>user location data;</claim-text>
<claim-text>owner data of the user terminal device; and</claim-text>
<claim-text>user preference data.</claim-text></claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>The method according to claim 2, wherein the supplemental contents include one or more of:
<claim-text>information related to a content of the video;</claim-text>
<claim-text>information related to a target object in the video;</claim-text>
<claim-text>a special effect;</claim-text>
<claim-text>one or more user feedback;</claim-text>
<claim-text>a notification from a physical environment; and</claim-text>
<claim-text>navigation information.</claim-text></claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>The method according to claim 2, further comprising:<br/>
after receiving the user information from the user terminal device, determining a user category according to the user information, and determining the first selected supplemental content according to the user category or the user location.</claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>The method according to claim 2, wherein:
<claim-text>the user information includes a current location associated with the user terminal device; and<!-- EPO <DP n="26"> --></claim-text>
<claim-text>the first selected supplemental content includes navigation information guiding a user from the current location to a designated seating location for the user.</claim-text></claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>A content recording and sharing method, applied to a user terminal device, comprising:
<claim-text>displaying a video in an interface of the user terminal device in real time, the video capturing a physical scene for a time duration;</claim-text>
<claim-text>obtaining supplemental contents in real time;</claim-text>
<claim-text>displaying the supplemental contents superimposed on the video in the interface;</claim-text>
<claim-text>determining selected supplemental content for sharing according to a selection rule; and</claim-text>
<claim-text>sharing the video and the selected supplemental contents with additional users.</claim-text></claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>The content recording and sharing method according to claim 8, wherein:
<claim-text>obtaining supplemental contents in real time includes receiving the supplemental contents from a server in real time; and</claim-text>
<claim-text>sharing the video and the selected supplemental content with additional users includes uploading the video and the selected supplemental contents to a video sharing platform.</claim-text></claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>The content recording and sharing method according to claim 8, wherein:
<claim-text>displaying the video in the interface is initiated in response to receiving a first user instruction;</claim-text>
<claim-text>storing the video and the supplemental contents is initiated in response to receiving a second user instruction; and<!-- EPO <DP n="27"> --></claim-text>
<claim-text>uploading the video and the selected supplemental content is initiated in response to receiving a third user instruction.</claim-text></claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>The content recording and sharing method according to claim 8, wherein:<br/>
displaying the supplemental contents superimposed on the video in the interface includes displaying the supplemental contents according to a first user configuration.</claim-text></claim>
<claim id="c-en-0012" num="0012">
<claim-text>The content recording and sharing method according to claim 8, wherein:<br/>
storing the video and the supplemental contents includes storing the video and the supplemental contents in synchronization according to a second user configuration.</claim-text></claim>
<claim id="c-en-0013" num="0013">
<claim-text>The content recording and sharing method according to claim 8, wherein:<br/>
sharing the video and the selected supplemental contents includes sharing the video and the selected supplemental content saccording to a third user configuration.</claim-text></claim>
<claim id="c-en-0014" num="0014">
<claim-text>A device for performing content recording and sharing, comprising:
<claim-text>a non-transitory computer-readable storage medium storing a plurality of computer-executable instructions; and</claim-text>
<claim-text>a processor, coupled with the non-transitory computer-readable storage medium and, when executing the computer-executable instructions, configured to:
<claim-text>obtain a video in real time, the video capturing a physical scene for a time duration;</claim-text>
<claim-text>generate supplemental contents synchronized with the video;<!-- EPO <DP n="28"> --></claim-text>
<claim-text>receive user information from a first user terminal device;</claim-text>
<claim-text>determine first selected supplemental content according to the user information and a first selection rule, and sending the first selected supplemental content to the first user terminal device;</claim-text>
<claim-text>determine second selected supplemental content according to a second selection rule; and</claim-text>
<claim-text>share the video and the second selected supplemental content with one or more second user terminal devices.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0015" num="0015">
<claim-text>The device according to claim 14, wherein the processor is further configured to:<br/>
after receiving the user information from the user terminal device, determine a user category according to the user information, and determine the first selected supplemental content according to the user category.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="29"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="155" he="88" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="30"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="130" he="163" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="31"> -->
<figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="130" he="157" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="32"> -->
<figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="137" he="94" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="157" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="155" he="233" type="tif"/></search-report-data><search-report-data date-produced="20210503" id="srepxml" lang="en" srep-office="EP" srep-type="ep-sr" status="n"><!--
 The search report data in XML is provided for the users' convenience only. It might differ from the search report of the PDF document, which contains the officially published data. The EPO disclaims any liability for incorrect or incomplete data in the XML for search reports.
 -->

<srep-info><file-reference-id>N 1024-EP /KT</file-reference-id><application-reference><document-id><country>EP</country><doc-number>20213548.9</doc-number></document-id></application-reference><applicant-name><name>Northwest Instrument Inc.</name></applicant-name><srep-established srep-established="yes"/><srep-invention-title title-approval="yes"/><srep-abstract abs-approval="yes"/><srep-figure-to-publish figinfo="by-applicant"><figure-to-publish><fig-number>1</fig-number></figure-to-publish></srep-figure-to-publish><srep-info-admin><srep-office><addressbook><text>MN</text></addressbook></srep-office><date-search-report-mailed><date>20210511</date></date-search-report-mailed></srep-info-admin></srep-info><srep-for-pub><srep-fields-searched><minimum-documentation><classifications-ipcr><classification-ipcr><text>H04N</text></classification-ipcr></classifications-ipcr></minimum-documentation></srep-fields-searched><srep-citations><citation id="sr-cit0001"><patcit dnum="US2018374267A1" id="sr-pcit0001" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2018374267&amp;CY=ep"><document-id><country>US</country><doc-number>2018374267</doc-number><kind>A1</kind><name>YURKIN FERNANDO JOSE [US]</name><date>20181227</date></document-id></patcit><category>X</category><rel-claims>1-6,8-15</rel-claims><category>Y</category><rel-claims>7</rel-claims><rel-passage><passage>* figures 3, 4, 6 *</passage></rel-passage><rel-passage><passage>* paragraph [0009] - paragraph [0013] *</passage><passage>* paragraph [0032] - paragraph [0033] *</passage><passage>* paragraph [0046] - paragraph [0052] *</passage><passage>* paragraph [0108] - paragraph [0122] *</passage><passage>* claims 1, 8 *</passage></rel-passage></citation><citation id="sr-cit0002"><patcit dnum="GB2505978A" id="sr-pcit0002" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=GB2505978&amp;CY=ep"><document-id><country>GB</country><doc-number>2505978</doc-number><kind>A</kind><name>MARTIR TUPAC [GB]</name><date>20140319</date></document-id></patcit><category>Y</category><rel-claims>7</rel-claims><category>A</category><rel-claims>1,3,4,8,14</rel-claims><rel-passage><passage>* figure 1 *</passage></rel-passage><rel-passage><passage>* page 1, line 21 - page 2, line 7 *</passage><passage>* page 3, line 29 - page 4, line 7 *</passage><passage>* page 5, line 20 - page 6, line 12 *</passage><passage>* page 9, line 24 - page 10, line 25 *</passage><passage>* page 19, line 13 - line 17 *</passage><passage>* page 20, line 19 - line 21 *</passage><passage>* page 23, line 9 - page 24, line 29 *</passage></rel-passage></citation><citation id="sr-cit0003"><patcit dnum="US2019104235A1" id="sr-pcit0003" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2019104235&amp;CY=ep"><document-id><country>US</country><doc-number>2019104235</doc-number><kind>A1</kind><name>SARKAR BHASWAR [US]</name><date>20190404</date></document-id></patcit><category>X</category><rel-claims>1,2,4-6,8-15</rel-claims><category>Y</category><rel-claims>3</rel-claims><rel-passage><passage>* figures 1, 2, 4-7, 14 *</passage></rel-passage><rel-passage><passage>* paragraph [0010] *</passage><passage>* paragraph [0052] - paragraph [0060] *</passage><passage>* paragraph [0069] - paragraph [0070] *</passage><passage>* paragraph [0078] - paragraph [0095] *</passage><passage>* paragraph [0104] - paragraph [0109] *</passage></rel-passage></citation><citation id="sr-cit0004"><patcit dnum="US2018374268A1" id="sr-pcit0004" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2018374268&amp;CY=ep"><document-id><country>US</country><doc-number>2018374268</doc-number><kind>A1</kind><name>NILES JAMES E [US]</name><date>20181227</date></document-id></patcit><category>X</category><rel-claims>1,8-15</rel-claims><category>Y</category><rel-claims>3</rel-claims><category>A</category><rel-claims>4-6</rel-claims><rel-passage><passage>* paragraph [0009] - paragraph [0019] *</passage></rel-passage><rel-passage><passage>* paragraph [0027] *</passage></rel-passage><rel-passage><passage>* paragraph [0050] - paragraph [0054] *</passage><passage>* paragraph [0069] - paragraph [0075] *</passage><passage>* paragraph [0079] - paragraph [0083] *</passage><passage>* figures 3, 5, 7, 8 *</passage></rel-passage></citation></srep-citations><srep-admin><examiners><primary-examiner><name>Döttling, Martin</name></primary-examiner></examiners><srep-office><addressbook><text>Munich</text></addressbook></srep-office><date-search-completed><date>20210503</date></date-search-completed></srep-admin><!--							The annex lists the patent family members relating to the patent documents cited in the above mentioned European search report.							The members are as contained in the European Patent Office EDP file on							The European Patent Office is in no way liable for these particulars which are merely given for the purpose of information.							For more details about this annex : see Official Journal of the European Patent Office, No 12/82						--><srep-patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2018374267</doc-number><kind>A1</kind><date>20181227</date></document-id></priority-application><text>NONE</text></patent-family><patent-family><priority-application><document-id><country>GB</country><doc-number>2505978</doc-number><kind>A</kind><date>20140319</date></document-id></priority-application><family-member><document-id><country>EP</country><doc-number>2896210</doc-number><kind>A2</kind><date>20150722</date></document-id></family-member><family-member><document-id><country>GB</country><doc-number>2505978</doc-number><kind>A</kind><date>20140319</date></document-id></family-member><family-member><document-id><country>WO</country><doc-number>2014041353</doc-number><kind>A2</kind><date>20140320</date></document-id></family-member></patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2019104235</doc-number><kind>A1</kind><date>20190404</date></document-id></priority-application><family-member><document-id><country>CN</country><doc-number>111201069</doc-number><kind>A</kind><date>20200526</date></document-id></family-member><family-member><document-id><country>EP</country><doc-number>3687643</doc-number><kind>A1</kind><date>20200805</date></document-id></family-member><family-member><document-id><country>JP</country><doc-number>2020534952</doc-number><kind>A</kind><date>20201203</date></document-id></family-member><family-member><document-id><country>US</country><doc-number>2019104235</doc-number><kind>A1</kind><date>20190404</date></document-id></family-member><family-member><document-id><country>US</country><doc-number>2019327392</doc-number><kind>A1</kind><date>20191024</date></document-id></family-member><family-member><document-id><country>WO</country><doc-number>2019067112</doc-number><kind>A1</kind><date>20190404</date></document-id></family-member></patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2018374268</doc-number><kind>A1</kind><date>20181227</date></document-id></priority-application><text>NONE</text></patent-family></srep-patent-family></srep-for-pub></search-report-data>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="US63002604B"><document-id><country>US</country><doc-number>63002604</doc-number><kind>B</kind><date>20200331</date></document-id></patcit><crossref idref="pcit0001">[0001]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
