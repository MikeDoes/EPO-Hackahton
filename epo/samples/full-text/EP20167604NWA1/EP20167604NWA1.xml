<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP20167604A1" file="EP20167604NWA1.xml" lang="en" country="EP" doc-number="3889886" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3889886</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>20167604.6</B210><B220><date>20200401</date></B220><B250>en</B250><B251EP>en</B251EP><B260>en</B260></B200><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>G06T   5/50        20060101AFI20200922BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G06T2207/10064     20130101 LA20200831BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>G06T2207/10056     20130101 LA20200831BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>G06T2207/10024     20130101 LA20200831BHEP        </text></classification-cpc><classification-cpc sequence="4"><text>G06T   5/50        20130101 FI20200831BHEP        </text></classification-cpc><classification-cpc sequence="5"><text>G06T2207/20221     20130101 LA20200831BHEP        </text></classification-cpc><classification-cpc sequence="6"><text>G06T2207/30016     20130101 LA20200831BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>SYSTEME, VERFAHREN UND COMPUTERPROGRAMME FÜR EIN MIKROSKOPSYSTEM UND ZUR BESTIMMUNG EINER TRANSFORMATIONSFUNKTION</B542><B541>en</B541><B542>SYSTEMS, METHODS AND COMPUTER PROGRAMS FOR A MICROSCOPE SYSTEM  AND FOR DETERMINING A TRANSFORMATION FUNCTION</B542><B541>fr</B541><B542>SYSTÈMES, PROCÉDÉS ET PROGRAMMES INFORMATIQUES POUR UN SYSTÈME DE MICROSCOPE ET POUR DÉTERMINER UNE FONCTION DE TRANSFORMATION</B542></B540><B590><B598>1a</B598></B590></B500><B700><B710><B711><snm>Leica Instruments (Singapore) Pte. Ltd.</snm><iid>101165653</iid><irf>LMS190901PAEP</irf><adr><str>12 Teban Gardens Crescent</str><city>608924 Singapore</city><ctry>SG</ctry></adr></B711></B710><B720><B721><snm>THEMELIS, George</snm><adr><str>Im Kuerzenen 5</str><city>88131 Lindau</city><ctry>DE</ctry></adr></B721><B721><snm>WILKEN, Tobias</snm><adr><str>Hohlegasse 16</str><city>4102 Binningen</city><ctry>CH</ctry></adr></B721></B720><B740><B741><snm>2SPL Patentanwälte PartG mbB</snm><iid>101239684</iid><adr><str>Postfach 15 17 23</str><city>80050 München</city><ctry>DE</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">Examples relate to systems, methods and computer programs for a microscope system and for determining a transformation function, and to a corresponding microscope system. The system for the microscope system comprises one or more processors and one or more storage devices. The system is configured to obtain first imaging sensor data from a first imaging sensor of a microscope of the microscope system and second imaging sensor data from a second imaging sensor of the microscope. the first imaging sensor data comprises sensor data on light sensed in a first plurality of mutually separated wavelength bands. The second imaging sensor data comprises sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The system is configured to generate a composite color image based on the first imaging sensor data and based on the second imaging sensor data. The composite color image is based on a plurality of color channels. The composite color image is generated using a transformation function to define a transformation to be performed between the imaging sensor data and the composite color image, such that the composite color image is generated using sensor data on light sensed in each wavelength band of the first and second plurality of mutually separated wavelength bands.
<img id="iaf01" file="imgaf001.tif" wi="79" he="68" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001"><b>Technical field</b></heading>
<p id="p0001" num="0001">Examples relate to systems, methods and computer programs for a microscope system and for determining a transformation function, and to a corresponding microscope system.</p>
<heading id="h0002"><b>Background</b></heading>
<p id="p0002" num="0002">Surgeon often use the color of tissue (e.g. of brain tissue) to distinguish suspicious tissue (e.g. lesions). However, in many cases, subtle tissue color differences are only seen by surgeons with long experience and trained visual acuity, which is difficult to learn. Multispectral reflectance imaging can capture very small or even invisible color differences by means of measuring very small spectral differences. However, for the known concepts of multispectral imaging, additional hardware may be necessary, in addition to the plurality of sensors of a modern microscope.</p>
<heading id="h0003"><b>Summary</b></heading>
<p id="p0003" num="0003">There may be a desire for an improved concept for providing color images, in which subtle differences between different types of tissue are visible.</p>
<p id="p0004" num="0004">This desire is addressed by the subject-matter of the independent claims.</p>
<p id="p0005" num="0005">Embodiments of the present disclosure are based on the finding, that multi-spectral imaging can be performed by combining imaging sensor data from an imaging sensor that is primarily used to perform fluorescence imaging with imaging sensor data from an imaging sensor that is primarily used for reflectance imaging. Embodiments of the present disclosure thus provide a system for a microscope system. The system comprises one or more processors and one or more storage devices. The system is configured to obtain first imaging sensor data from a first imaging sensor of a microscope of the microscope system and second imaging sensor data from a second imaging sensor of the microscope. The first imaging sensor data comprises<!-- EPO <DP n="2"> --> sensor data on light sensed in a first plurality of mutually separated wavelength bands. The second imaging sensor data comprises sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The system is configured to generate a composite color image based on the first imaging sensor data and based on the second imaging sensor data. The composite color image is based on a plurality of color channels. The composite color image is generated using a transformation function to define a transformation to be performed between the imaging sensor data and the composite color image, such that the composite color image is generated using sensor data on light sensed in each wavelength band of the first and second plurality of mutually separated wavelength bands.</p>
<p id="p0006" num="0006">Embodiments of the present disclosure further provide a microscope system comprising the system and the microscope with the first and second imaging sensor. One of the first and the second imaging sensor is an imaging sensor that is adapted to provide a fluorescence imaging functionality of the microscope system.</p>
<p id="p0007" num="0007">Through the use of two sets of imaging sensor data, a more accurate transformation function can be used to construct the color image, which may thus show subtle differences between different types of tissue. At the same time, the re-use of the fluorescence imaging sensor for reflectance imaging may enable a use of the approach without having to include additional sensors in a microscope system.</p>
<p id="p0008" num="0008">In various embodiments, the transformation is based on a set of transformation factors that each define a transformation to be performed between the imaging sensor data on light sensed in a wavelength band and a color channel of the composite color image. For example, the set of transformation factors may provide a one-to-one transformation between an intensity of light measured in one of the wavelength bands and a color channel.</p>
<p id="p0009" num="0009">For example, the set of transformation factors may comprise one transformation factor for each combination of wavelength band and color channel. Thus, a one-to-one transformation may be applied, e.g. using a matrix multiplication. In other words, the set of transformation factors may provide a transformation between the imaging sensor data of each wavelength<!-- EPO <DP n="3"> --> band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite color image.</p>
<p id="p0010" num="0010">For example, the composite color image may comprise three color channels (e.g. Red, Green and Blue). Each of the color channels may be generated based on a transformation of the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands. The transformation may be performed using the transformation function. By using the imaging sensor data of each of the wavelength bands for each of the color channels, even subtle color differences may be included in the composite color image.</p>
<p id="p0011" num="0011">In various embodiments, the transformation function may be implemented by a transformation matrix. The system may be configured to transform the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands using the transformation matrix. Transformation matrices provide a computationally efficient implementation of the transformation function.</p>
<p id="p0012" num="0012">In various embodiments, the system is configured to provide a display signal to a display of the microscope system using an interface of the system, to cause the display to show the composite color image. Thus, the composite color image may be shown to a user of the microscope system, e.g. to a surgeon.</p>
<p id="p0013" num="0013">Embodiments of the present disclosure further provide a system for determining a transformation function. The system comprises one or more processors and one or more storage devices. The system is configured to obtain first imaging sensor data of a reference object from a first imaging sensor of a microscope and second imaging sensor data of the reference object from a second imaging sensor of the microscope. The first imaging sensor data comprises sensor data on light sensed in a first plurality of mutually separated wavelength bands. The second imaging sensor data comprises sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The system is configured to obtain a composite reference image of the reference object. The composite reference image comprises a plurality of color channels. The system is configured to determine the transformation function by determining a set of transformation factors that provide an approximate<!-- EPO <DP n="4"> --> transformation between the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite reference image. The transformation function is based on the set of transformation factors. Through the use of two sets of imaging sensor data, a more accurate transformation function can be used to construct the color image, which may thus show subtle differences between different types of tissue.</p>
<p id="p0014" num="0014">In some embodiments, the system is configured to identify a set of transformation factors that yields a lower mismatch between the composite reference image and a transformed image that is generated based on the set of transformation factors than at least one other set of transformation factors. In other words, the system may be configured to iteratively search for transformation factors that reduce a mismatch between the composite reference image and the transformed image.</p>
<p id="p0015" num="0015">For example, the composite reference image of the reference image may define a plurality of colors of a plurality of portions of the reference object. The plurality of colors may comprise a pre-defined first subset of colors and a second subset of colors. The system may be configured to identify a set of transformation factors that yields a lower mismatch between the composite reference image and a transformed image that is generated based on the set of transformation factors than at least one other set of transformation factors for the pre-defined first subset of colors. In other words, transformation factors may be identified that reduce the mismatch for the first subset of colors, which may be of particular interest. For example, the pre-defined first subset of colors may be colors that are present as colors of organic tissue in a surgical setting. An improved acuity in the colors of the first subset may be more beneficial than in other colors.</p>
<p id="p0016" num="0016">In various embodiments, the system is configured to identify a set of transformation factors that reduces a mismatch value representing the mismatch between the composite reference image compared to at least one other set of transformation factors. The mismatch value may be calculated for the colors of the plurality colors. A mismatch for a color of the pre-defined first subset of colors may have a higher impact on the mismatch value than a mismatch for a color of the second subset of colors. Thus, the colors of the first subset may receive a higher weight than other colors in the determination of the transformation factors.<!-- EPO <DP n="5"> --></p>
<p id="p0017" num="0017">Embodiments of the present disclosure further provide a method for a microscope system. The method comprises obtaining first imaging sensor data from a first imaging sensor of a microscope of the microscope system and second imaging sensor data from a second imaging sensor of the microscope. The first imaging sensor data comprises sensor data on light sensed in a first plurality of mutually separated wavelength bands. The second imaging sensor data comprises sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The method comprises generating a composite color image based on the first imaging sensor data and based on the second imaging sensor data. The composite color image is based on a plurality of color channels. The composite color image is generated using a transformation function to define a transformation to be performed between the imaging sensor data and the composite color image, such that the composite color image is generated using sensor data on light sensed in each wavelength band of the first and second plurality of mutually separated wavelength bands.</p>
<p id="p0018" num="0018">Embodiments of the present disclosure further provide a method for determining a transformation function. The method comprises obtaining first imaging sensor data of a reference object from a first imaging sensor of a microscope and second imaging sensor data of the reference object from a second imaging sensor of the microscope. The first imaging sensor data comprises sensor data on light sensed in a first plurality of mutually separated wavelength bands. The second imaging sensor data comprises sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The method comprises obtaining a composite reference image of the reference object. The composite reference image comprises a plurality of color channels. The method comprises determining the transformation function by determining a set of transformation factors that provide an approximate transformation between the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite reference image. The transformation function is based on the set of transformation factors.<!-- EPO <DP n="6"> --></p>
<p id="p0019" num="0019">Embodiments of the present disclosure further provide a computer program with a program code for performing at least one of the methods when the computer program is run on a processor.</p>
<heading id="h0004"><b>Short description of the Figures</b></heading>
<p id="p0020" num="0020">Some examples of apparatuses and/or methods will be described in the following by way of example only, and with reference to the accompanying figures, in which
<dl id="dl0001" compact="compact">
<dt>Figs. 1a and 1b</dt><dd>show schematic diagrams of a system for a microscope system, and of a microscope system comprising the system;</dd>
<dt>Fig. 2</dt><dd>shows a flow chart of a method for a microscope system;</dd>
<dt>Fig. 3</dt><dd>shows a schematic diagram of a system for determining a transformation function;</dd>
<dt>Fig. 4</dt><dd>shows a flow chart of a method for determining a transformation function;</dd>
<dt>Fig. 5a</dt><dd>shows a schematic diagram of different colors present in an image frame;</dd>
<dt>Fig. 5b and 5c</dt><dd>show schematic diagrams of an intensity of different colors, as sensed in different frequency bands;</dd>
<dt>Fig.</dt><dd>5dshows an exemplary transformation matrix;</dd>
<dt>Fig. 6</dt><dd>shows a schematic diagram of a microscope and a lighting system;</dd>
<dt>Fig. 7a</dt><dd>shows a schematic diagram of an exemplary color table;</dd>
<dt>Fig. 7b</dt><dd>shows a diagram of an exemplary system of equations; and</dd>
<dt>Fig. 8</dt><dd>shows a schematic diagram of a microscope system comprising a microscope and a computer system.</dd>
</dl><!-- EPO <DP n="7"> --></p>
<heading id="h0005"><b>Detailed Description</b></heading>
<p id="p0021" num="0021">Various examples will now be described more fully with reference to the accompanying drawings in which some examples are illustrated. In the figures, the thicknesses of lines, layers and/or regions may be exaggerated for clarity.</p>
<p id="p0022" num="0022"><figref idref="f0001">Figs. 1a and 1b</figref> show schematic diagrams of a system 110 for a microscope system 100, and of a microscope system 100 comprising the system 100. The system 110 comprises one or more processors 114 and one or more storage devices 116. Optionally, the system further comprises an interface 112. The one or more processors are coupled to the interface and to the one or more storage devices. In general, the functionality of the system is provided by the one or more processors, e.g. in conjunction with the optional interface or with the one or more storage devices. For example, the system may be configured to obtain the imaging sensor data via the interface, and/or to store transformation factors of the transformation function using the one or more storage devices.</p>
<p id="p0023" num="0023">The system is configured to obtain first imaging sensor data from a first imaging sensor 122 of a microscope 120 of the microscope system and second imaging sensor data from a second imaging sensor 124 of the microscope. The first imaging sensor data comprises sensor data on light sensed in a first plurality of mutually separated wavelength bands. The second imaging sensor data comprises sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The system is configured to generate a composite color image based on the first imaging sensor data and based on the second imaging sensor data. The composite color image is based on a plurality of color channels. The composite color image is generated using a transformation function to define a transformation to be performed between the imaging sensor data and the composite color image, such that the composite color image is generated using sensor data on light sensed in each wavelength band of the first and second plurality of mutually separated wavelength bands.<!-- EPO <DP n="8"> --></p>
<p id="p0024" num="0024"><figref idref="f0001">Fig. 1b</figref> shows a block diagram of a microscope system comprising the microscope 120 and the system 110. The microscope system shown in <figref idref="f0001">Fig. 1b</figref> is a surgical microscope system, which may be used at a surgical site by a surgeon. The surgical microscope system shown in <figref idref="f0001">Fig. 1b</figref> comprises a number of optional components, such as a base unit 105 (comprising the system 110) with a (rolling) stand, an auxiliary display 140a, a lighting system 130, a (robotic or manual) arm 160 which holds the microscope 120 in place, and which is coupled to the base unit 105 and to the microscope 120, and steering handles 150 that are attached to the microscope 120. In addition to the first and second imaging sensors 122; 124, the microscope 120 may comprise optional ocular displays 140b and an optional auxiliary display. In the context of this application, the term "(surgical) microscope system" is used, in order to cover the portions of the system that are not part of the actual microscope (which comprises optical components), but which are used in conjunction with the microscope, such as the display or the lighting system. One of the first and the second imaging sensor is an imaging sensor that is adapted to provide a fluorescence imaging functionality of the microscope system.</p>
<p id="p0025" num="0025">Various embodiments of the present disclosure relate to a system, method and computer program for a microscope system. In general, a microscope is an optical instrument that is suitable for examining objects that are too small to be examined by the human eye (alone). For example, a microscope may provide an optical magnification of an object. In modern microscopes, the optical magnification is often provided for a camera or an imaging sensor, such as the first and second imaging sensors 122; 124 of the microscope 120 of <figref idref="f0001">Fig. 1a</figref>. The microscope 120 may further comprise one or more optical magnification components that are used to magnify a view on the sample.</p>
<p id="p0026" num="0026">There are a variety of different types of microscopes. If the microscope system is used in the medical or biological fields, the object being viewed through the microscope may be a sample of organic tissue, e.g. arranged within a petri dish or present in a part of a body of a patient. For example, the microscope system 100 may be a microscope system for use in a laboratory, e.g. a microscope that may be used to examine the sample of organic tissue in a petri dish. Alternatively, the microscope 120 may be part of a surgical microscope system 100, e.g. a microscope to be used during a surgical procedure. Such a system is shown in <figref idref="f0001">Fig. 1b</figref>, for example. Although embodiments are described in connection with a microscope system, they may also be applied, in a more general manner, to any optical device.<!-- EPO <DP n="9"> --></p>
<p id="p0027" num="0027">The system is configured to obtain the first and second imaging sensor data from the first and second imaging sensors 122; 124 of the microscope. For example, the first and second imaging sensors 122; 124 may comprise or be an APS (Active Pixel Sensor) - or a CCD (Charge-Coupled-Device)-based imaging sensor. For example, in APS-based imaging sensors, light is recorded at each pixel using a photodetector and an active amplifier of the pixel. APS-based imaging sensors are often based on CMOS (Complementary Metal-Oxide-Semiconductor) or S-CMOS (Scientific CMOS) technology. In CCD-based imaging sensors, incoming photons are converted into electron charges at a semiconductor-oxide interface, which are subsequently moved between capacitive bins in the imaging sensor modules by a control circuitry of the sensor imaging module to perform the imaging. The first and second imaging sensor data may be obtained by receiving the respective imaging sensor data from the imaging sensor (e.g. via the interface 112), by reading the respective imaging sensor data out from a memory of the respective imaging sensor (e.g. via the interface 112), or by reading the respective imaging sensor data from a storage device 116 of the system 110, e.g. after the imaging sensor data has been written to the storage device 116 by the respective imaging sensor or by another system or processor.</p>
<p id="p0028" num="0028">The first imaging sensor data is obtained from the first imaging sensor, and the second imaging sensor data is obtained from the second imaging sensor. In other words, the first imaging sensor data and the second imaging sensor data are obtained from different sensors. Accordingly, the first imaging sensor is different from the second imaging sensor.</p>
<p id="p0029" num="0029">In various embodiments, as has been pointed out before, one of the imaging sensors may be a sensor that is typically used for fluorescence imaging. For example, when fluorescence imaging is not used, the respective imaging sensor may be used to provide additional sensor data, e.g. to improve the color accuracy of the composite color image. Embodiments may take advantage of the fact that a surgical microscope equipped for fluorescence microscopy has two imaging systems, one with known response bands (e.g. the first plurality of mutually separated wavelength bands) for generating a visual (reflectance image (this can be a conventional RGB camera) and also a fluorescence imaging system with specifically defined wavelength bands (e.g. around 560nm, 630nm, 800nm for fluorescein, PPIX (Protoporphyrin IX), ICG (Indo-Cyanine Green) respectively, the second plurality of mutually separated wavelength bands). For example, the second imaging sensor may be adapted to provide a fluorescence imaging functionality of the microscope system. For example, in a first operating state<!-- EPO <DP n="10"> --> of the microscope 120, the system may be configured to use the first optical imaging sensor for reflectance imaging and to use the second optical imaging sensor for fluorescence imaging, and in a second operating state of the microscope 120, system may be configured to use the first and the second imaging sensor to perform reflectance imaging, by generating the composite color image. In other words, the composite color image may be a reflectance image, i.e. might not be based on fluorescence imaging. Accordingly, the wavelength bands of the second plurality of mutually separated wavelength bands may be wavelength bands that are used for fluorescence imaging (i.e. emission wavelength bands used in fluorescent imaging). Accordingly, the wavelength bands of the first plurality of mutually separated wavelength bands may be wavelength bands that are used for reflectance imaging, e.g. across the visible color spectrum. In some embodiments, however, the first plurality of mutually separated wavelength bands may exclude the wavelength bands that are being used for fluorescence imaging. Accordingly, the wavelength bands of the first plurality and of the second plurality of mutually separated wavelength bands might not mutually overlap. In other words, a wavelength band might either be covered by the first or by the second plurality of mutually separated wavelength bands. In various embodiments, both the first and the second plurality of mutually separated wavelength bands comprise exactly three (contiguous) wavelength bands.</p>
<p id="p0030" num="0030">In general, the microscope system may comprise a lighting system (i.e. illumination system) that is configured to illuminate the sample being viewed through the microscope. In embodiments, the lighting system may be used to support the reflectance imaging and the fluorescence imaging being performed using the illumination system. To generate the composite color image, which is a reflectance image, the sample may be illuminated in each of the first and second plurality of mutually independent wavelength bands. Accordingly, the system may be configured to control the lighting system such, that the sample is illuminated in each of the first and second plurality of mutually independent wavelength bands (e.g. in the second operating state). If fluorescence imaging is performed (in addition to reflectance imaging), the system may be configured to control the lighting system such, that the sample is illuminated in each of the first plurality of mutually separated wavelength bands (and not in the wavelength bands of the second plurality of mutually separated wavelength bands), and (if not already contained in the first plurality of mutually separated wavelength bands), in one or more excitation wavelength bands of the fluorescent material being used for the sample. In the microscope system, light from the emission wavelength band may be (logically) removed<!-- EPO <DP n="11"> --> from the illumination light and may also be directed solely to the fluorescence camera. When the system is not being used for microscopy, the additional information from reflectance imaging in the or each fluorescent emission band, including the NIR ICG (Near-InfraRed Indo-Cyanine Green) band, can be used to provide a more accurate reconstructed color image. Thus, the lighting system may be operated, by the system 110, to illuminate the site/sample in the emission wavelength band(s).</p>
<p id="p0031" num="0031">In general, both the emission of the illumination by the lighting system, and the wavelength bands being sensed by the imaging sensors may be defined by a filter that is being mounted in a light path of the sensor or lighting system (see e.g. Filters 620-640 of <figref idref="f0005">Fig. 6</figref>). For example, a filter mounted in the light path of the first imaging sensor may be a bandpass filter that is adapted to filter out light in wavelength bands outside the first plurality of mutually separated wavelength bands, e.g. such that (only) light having a wavelength within the first plurality of mutually separated wavelength bands is admitted to the first imaging sensor Accordingly, a filter mounted in the light path of the second imaging sensor may be a bandpass filter that is adapted to filter out light in wavelength bands outside the second plurality of mutually separated wavelength bands, e.g. such that (only) light having a wavelength within the second plurality of mutually separated wavelength bands is admitted to the second imaging sensor. A filter mounted in a light path of the lighting system may be adapted to pass through light in all of the first and second plurality of mutually separated wavelength bands.</p>
<p id="p0032" num="0032">In various embodiments, as shown in <figref idref="f0001">Figs. 1a</figref> and/or 1b, a beam splitter 126 is used to direct the light reflected or emitted by the sample to the first and second imaging sensor. For example, the beam splitter may be a polychroic mirror, that is configured to split the light such, that light at a given wavelength is either directed at the first imaging sensor or at the second imaging sensor. For example, the polychroic mirror may be adapted to direct light having a wavelength within the first plurality of mutually separated wavelength bands (only) to the first imaging sensor, and to direct light having a wavelength within the second plurality of mutually separated wavelength bands (only) to the second imaging sensor.</p>
<p id="p0033" num="0033">The system is configured to generate a composite color image based on the first and second imaging sensor data. In other words, the system is configured to generate a color image based on the imaging sensor data provided by two imaging sensors. Accordingly, the composite color image is a color image that is generated based on the imaging sensor provided by two<!-- EPO <DP n="12"> --> different imaging sensors. For example, the composite color image may be a multi-spectral color image that is generated by using imaging sensor data from two imaging sensors, the imaging sensor data of each of the sensors representing light in a plurality of mutually separated wavelength bands. Consequently, the composite color image is a multi-spectral color image that is generated based on light sensed in a plurality of mutually separated wavelength bands, the plurality of mutually separated wavelength bands being sensed by two different imaging sensors (e.g. one that is used for reflectance imaging, and one that is used for reflectance and fluorescence imaging). For example, the composite color image may be generated based on the first and second imaging sensor data to improve a color accuracy of the composite color image.</p>
<p id="p0034" num="0034">The composite color image is generated using the transformation function. which defines the transformation to be performed between the imaging sensor data and the composite color image. In contrast to other approaches, the transformation function is configured such, that the composite color image is generated using sensor data on light sensed in each wavelength band of the first and second plurality of mutually separated wavelength bands. In other words, the light sensed in all of the mutually separated wavelength bands of the first and second plurality may be combined to generate the composite color image, e.g. to obtain a composite color image having an improved color accuracy. In general, the transformation function may be seen as set of instructions that are used to convert between the first and second imaging sensor data, on one side, and the composite color image on the other side.</p>
<p id="p0035" num="0035">In general, the first and second imaging sensor data may each comprise a plurality of pixels that are generated by a plurality of sensor pixels of the respective imaging sensor (e.g. after demosaicing). The respective imaging sensor data may comprise, for each pixel of the plurality of pixels, a plurality of numeric values representing the light sensed in the plurality of modulated wavelength bands, by wavelength bands. For example, if light is sensed in three mutually separated wavelength bands by the first and/or second imaging sensor, the first and/or second imaging sensor data may (each) comprise three numeric values per pixel representing the light sensed in the three mutually separated wavelength bands. The plurality of pixels of the first and second imaging sensor data may be in a pre-defined relationship. Ideally, the plurality of pixels of the two imaging sensors may be generated such, that each pixel of the first imaging sensor data represents the same point on the sample as a corresponding pixel of the second imaging sensor data.<!-- EPO <DP n="13"> --></p>
<p id="p0036" num="0036">For each pixel of the plurality of pixels, the system may be configured to input the numeric values of a pixel of the first imaging sensor data and the numeric values of the corresponding pixel of the second imaging sensor data into the transformation function, and calculate numerical values representing the pixel in the plurality of color channels of the composite color image. For example, if both the first and second plurality of mutually separated wavelength bands comprise three wavelength bands (for a total of six wavelength bands, and thus a total of six numerical values representing the light in the six wavelength bands), and if the composite color image is based on three color channels (e.g. Red, Green and Blue), the transformation function may specify a total of eighteen (six by three) transformations to be performed between the imaging sensor data and the channels of the composite color image. These transformations may be defined as a multiplicative factors that can be multiplied with the numerical values of the individual pixels of the first and second imaging sensor data. Accordingly, the transformation function may be based on a set of transformation factors that each define a transformation to be performed between the imaging sensor data on light sensed in a wavelength band (i.e. between one of a plurality of numerical values of a pixel) and a color channel of the composite color image (e.g. one of the three color channels). In the above example, the set of transformation factors may comprise eighteen transformation factors. In other words, the set of transformation factors may comprise one transformation factor for each combination of wavelength band (i.e. numerical value of a pixel representing light sensed in the wavelength band) and color channel (i.e. numerical value of the color channel for the pixel). For example, the set of transformation factors may provide a transformation between the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite color image. The transformation function, and thus the set of transformation factors, may be applied (separately) to each pixel of the first and second imaging sensor data.</p>
<p id="p0037" num="0037">To improve performance, the transformation function may be defined as a matrix that can be multiplied with a vector comprising the first and second imaging sensor data. In other words, the transformation function may be implemented by a transformation matrix. The entries of the transformation matrix may be defined by the set of transformation factors). The system may be configured to transform the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands using the transformation matrix. For example, in <figref idref="f0004">Fig. 5d</figref>, an example of a transformation matrix is given.<!-- EPO <DP n="14"> --></p>
<p id="p0038" num="0038">As has been pointed out before, the composite color image may comprise three color channels, e.g. a red channel, a blue channel and a green channel (RGB). RGB is a channel model that is often used to represent color images, e.g. color images that are to be shown on a display. Each of the color channels of the composite color image may be generated based on a transformation of the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands. In other words, the numeric values of each of the first and second plurality of mutually separated wavelength bands may be used to calculate the numeric value of each of the color channels. The transformation may be performed using the transformation function, e.g. as shown above.</p>
<p id="p0039" num="0039">In various embodiments, the system is configured to provide a display signal to a display 140 of the microscope system using an interface 112 of the system 110, to cause the display to show the composite color image. In other words, the composite color image may be shown on a display of the microscope system, e.g. on an ocular display of the microscope system or on an auxiliary display of the microscope system.</p>
<p id="p0040" num="0040">The interface 112 may correspond to one or more inputs and/or outputs for receiving and/or transmitting information, which may be in digital (bit) values according to a specified code, within a module, between modules or between modules of different entities. For example, the interface 112 may comprise interface circuitry configured to receive and/or transmit information. In embodiments the one or more processors 114 may be implemented using one or more processing units, one or more processing devices, any means for processing, such as a processor, a computer or a programmable hardware component being operable with accordingly adapted software. In other words, the described function of the one or more processors 114 may as well be implemented in software, which is then executed on one or more programmable hardware components. Such hardware components may comprise a general-purpose processor, a Digital Signal Processor (DSP), a micro-controller, etc. In at least some embodiments, the one or more storage devices 116 may comprise at least one element of the group of a computer readable storage medium, such as an magnetic or optical storage medium, e.g. a hard disk drive, a flash memory, Floppy-Disk, Random Access Memory (RAM), Programmable Read Only Memory (PROM), Erasable Programmable Read Only Memory (EPROM), an Electronically Erasable Programmable Read Only Memory (EEPROM), or a network storage.<!-- EPO <DP n="15"> --></p>
<p id="p0041" num="0041">More details and aspects of the system and of the microscope system are mentioned in connection with the proposed concept or one or more examples described above or below (e.g. <figref idref="f0002 f0003 f0004 f0005 f0006">Fig. 2 to 8</figref>). The system and of the microscope system may comprise one or more additional optional features corresponding to one or more aspects of the proposed concept or one or more examples described above or below.</p>
<p id="p0042" num="0042"><figref idref="f0002">Fig. 2</figref> shows a flow chart of an embodiment of a corresponding method for a microscope system. The method comprises obtaining 210 first imaging sensor data from a first imaging sensor of a microscope of the microscope system and second imaging sensor data from a second imaging sensor of the microscope. The first imaging sensor data comprises sensor data on light sensed in a first plurality of mutually separated wavelength bands. The second imaging sensor data comprises sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The method comprises generating 220 a composite color image based on the first imaging sensor data and based on the second imaging sensor data. The composite color image is based on a plurality of color channels. The composite color image is generated using a transformation function to define a transformation to be performed between the imaging sensor data and the composite color image, such that the composite color image is generated using sensor data on light sensed in each wavelength band of the first and second plurality of mutually separated wavelength bands.</p>
<p id="p0043" num="0043">As indicated above, features described in connection with the system 110 and the microscope system 100 of <figref idref="f0001">Figs. 1a</figref> and/or 1b may be likewise applied to the method of <figref idref="f0002">Fig. 2</figref>.</p>
<p id="p0044" num="0044">More details and aspects of the method are mentioned in connection with the proposed concept or one or more examples described above or below (e.g. <figref idref="f0001">Fig. 1a to 1b</figref>, <figref idref="f0002 f0006">3 to 8</figref>). The method may comprise one or more additional optional features corresponding to one or more aspects of the proposed concept or one or more examples described above or below.</p>
<p id="p0045" num="0045">While <figref idref="f0001 f0002">Figs. 1a to 2</figref> relate to the application of the transformation function in the generation of the composite color image, the following <figref idref="f0002">Figs. 3 and 4</figref> relate to the generation of the<!-- EPO <DP n="16"> --> transformation function. Thus, the transformation function that is used in <figref idref="f0001 f0002">Figs. 1a to 2</figref> may be generated using the system, method and/or computer program presented in <figref idref="f0002">Figs. 3 or 4</figref>.</p>
<p id="p0046" num="0046"><figref idref="f0002">Fig. 3</figref> shows a schematic diagram of a system for determining a transformation function. The system 310 comprises one or more processors 314 and one or more storage devices 316. Optionally, the system further comprises an interface 312. The one or more processors are coupled to the interface and to the one or more storage devices. In general, the functionality of the system is provided by the one or more processors, e.g. in conjunction with the optional interface or with the one or more storage devices. For example, the system may be configured to obtain the imaging sensor data via the interface, and/or to store transformation factors of the transformation function and/or the composite reference image using the one or more storage devices.</p>
<p id="p0047" num="0047">The system is configured to obtain first imaging sensor data of a reference object 300 from a first imaging sensor 122 of a microscope 120 and second imaging sensor data of the reference object from a second imaging sensor 124 of the microscope. The first imaging sensor data comprises sensor data on light sensed in a first plurality of mutually separated wavelength bands, the second imaging sensor data comprising sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The system is configured to obtain a composite reference image of the reference object. The composite reference image comprises a plurality of color channels. The system is configured to determine the transformation function by determining a set of transformation factors that provide an approximate transformation (i.e. a transformation that yields a mismatch between the generated composite color image and the composite reference images that is smaller than a mismatch being yielded using another set of transformation factors or that is smaller than a threshold) between the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite reference image. The transformation function is based on the set of transformation factors.</p>
<p id="p0048" num="0048">For example, the system of <figref idref="f0002">Fig. 3</figref> may be implemented similar to the system 110 of <figref idref="f0001">Figs. 1a</figref> and/or 1b, e.g. by the same system. Accordingly, the system 110 and/or the system 310 may be configured to provide the functionality of the respective other system 310; 110.<!-- EPO <DP n="17"> --></p>
<p id="p0049" num="0049">Accordingly, the microscope 120 may be the microscope 120 of the microscope system 100 of <figref idref="f0001">Figs. 1a</figref> and/or 1b. Accordingly, the first and second imaging sensor data may also be implemented similar to the first and second imaging sensor data of <figref idref="f0001">Figs. 1a</figref> and/or 1b. Furthermore, the transformation function may be implemented similar to the transformation function of <figref idref="f0001 f0002">Figs. 1a to 2</figref>.</p>
<p id="p0050" num="0050">Some embodiments of the present disclosure relate to a system, a method and a computer program for determining a transformation function. As has been pointed out before in connection with <figref idref="f0001 f0002">Figs. 1a to 2</figref>, a transformation function defines the transformation to be performed between the imaging sensor data and a composite color image that is based on the imaging sensor data. In general, the transformation function may be seen as set of instructions that are used to convert between the first and second imaging sensor data, on one side, and the composite color image on the other side. Embodiments shown in connection with <figref idref="f0002">Figs. 3 and 4</figref> provide an approach for generating such a transformation function, using a composite reference image and using the imaging sensor data provided by the two imaging sensors.</p>
<p id="p0051" num="0051">In general, the composite reference image may be implemented similar to the composite color image that is introduced in connection with <figref idref="f0001">Figs. 1a</figref> and/or 1b - it is a color image, it is based on a plurality of color channels (e.g. three color channels - red, green and blue). The composite reference image may be different from the composite color image in that the numeric values representing the color channels of the individual pixels are reference values, i.e. values that define the numeric values that the transformation function that is being determined is to provide when applied to the first and second imaging sensor data showing the reference object. In other words - the first and second imaging sensor data show the reference object, and the reference composite image show the composite color image that is the desired result of the transformation function being applied to the first and second imaging sensor data. Preferably, the composite reference image comprises a plurality of pre-defined colors, with corresponding pre-defined numeric values representing the color channels of the individual pixels. For example, the reference object may be a color chart or color table, i.e. a table showing a plurality of colors each having a pre-defined representation in the plurality of channels of the composite reference image. Since the colors of the reference object are known from the composite reference image, the system can use the composite reference image as a desired result of the transformation function being applied on the imaging sensor data. Thus, the system may be configured to identify a set of transformation factors that defines a transformation to<!-- EPO <DP n="18"> --> be performed between the imaging sensor data and a composite color image, such that a difference between the composite color image and the composite reference image is reduced. Contrary to other systems, the set of transformation factors is identified for the imaging sensor data of both imaging sensors, thus potentially yielding twice as many transformation factors, e.g. a first subset of transformation factors for providing a transformation between the first imaging sensor data and the composite color image and a second subset of transformation factors for providing a transformation between the first imaging sensor data and the composite color image, e.g. such that the difference between the composite color image and the composite reference image is reduced compared to other sets of transformation factors. In other words, the system may be configured to identify a set of transformation factors that yields a lower mismatch (i.e. difference) between the composite reference image and a transformed image (i.e. composite color image) that is generated based on the set of transformation factors than at least one other set of transformation factors. For example, the system may be configured to determine the set of transformation factors by defining a set of equations using the first and second imaging sensor data and the composite reference image, and solving the set of equations for a set of potential transformation factors that provide the transformation between the first and second imaging sensor data and the composite reference image. This may be performed for a plurality of different colors, e.g. for the plurality of colors of the reference obj ect.</p>
<p id="p0052" num="0052">For example, the reference object may comprise a plurality of pre-defined colors (e.g. a color table), and the composite color image may comprise a plurality of portions representing the plurality of pre-defined colors. In other words, the composite reference image of the reference image may define a plurality of colors of a plurality of portions of the reference object, e.g. the plurality of pre-defined colors of the color table. The system may be configured to determine the set of transformation factors for the plurality of (pre-defined) colors, i.e. by identifying a set of transformation factors that provide, for each of the plurality of colors, an approximate transformation between the first and second imaging sensor data of a portion representing a (pre-defined color) and the pre-defined color, as represented by the plurality of channels of the composite reference image.</p>
<p id="p0053" num="0053">In applications like surgical microscopy, some colors may be more useful to the surgeon. For example, some colors may be indicative of pathologic tissue, but the difference between the color of pathologic tissue and the color of healthy tissue may be small. In some embodiments,<!-- EPO <DP n="19"> --> those colors may be treated with priority in the determination of the set of transformation factors. For example, the plurality of (pre-defined) colors may comprise a pre-defined first subset of colors and a second subset of colors. For example, the pre-defined first subset of colors may be colors that are pre-defined as being important for a specific application of the microscope. The second subset of colors may be the other colors, i.e. the colors that are not important for that specific application of the microscope. For example, the pre-defined first subset of colors may be colors that are present as colors of organic tissue in a surgical setting, e.g. colors that are indicative of healthy or pathologic tissue. The colors of the second subset of colors may be colors that are less relevant in identifying healthy or pathologic tissue. Accordingly, he system may be configured to identify a set of transformation factors that yields a lower mismatch between the composite reference image and a transformed image that is generated based on the set of transformation factors than at least one other set of transformation factors for the pre-defined first subset of colors. In some embodiments, the second subset of colors might not be considered in the identification of the set of transformation factors. In some other embodiments, however, the second subset of colors might be considered, albeit with a lower priority or weighting than the pre-defined first subset of colors. In other words, the system may be configured to identify a set of transformation factors that reduces a mismatch value representing the mismatch between the composite reference image compared to at least one other set of transformation factors. The mismatch value may be calculated for the colors of the plurality colors. A mismatch for a color of the pre-defined first subset of colors may have a higher impact on the mismatch value than a mismatch for a color of the second subset of colors. In other words, a mismatch for a color of the pre-defined first subset of colors may receive a higher weighting than a mismatch for a color of the pre-defined second subset in the identification of the set of transformation factors.</p>
<p id="p0054" num="0054">Embodiments of the present disclosure further provide a microscope system 100 (which may be implemented similar to the microscope system of <figref idref="f0001">Figs. 1a</figref> and/or 1b) comprising the system 310 (and optionally the system 110, if the systems are implemented separately) and the microscope 120 with the first 122 and second 124 imaging sensor. One of the first and the second imaging sensor may be an imaging sensor that is adapted to provide a fluorescence imaging functionality of the microscope system.</p>
<p id="p0055" num="0055">The interface 312 may correspond to one or more inputs and/or outputs for receiving and/or transmitting information, which may be in digital (bit) values according to a specified code,<!-- EPO <DP n="20"> --> within a module, between modules or between modules of different entities. For example, the interface 312 may comprise interface circuitry configured to receive and/or transmit information. In embodiments the one or more processors 314 may be implemented using one or more processing units, one or more processing devices, any means for processing, such as a processor, a computer or a programmable hardware component being operable with accordingly adapted software. In other words, the described function of the one or more processors 314 may as well be implemented in software, which is then executed on one or more programmable hardware components. Such hardware components may comprise a general-purpose processor, a Digital Signal Processor (DSP), a micro-controller, etc. In at least some embodiments, the one or more storage devices 316 may comprise at least one element of the group of a computer readable storage medium, such as an magnetic or optical storage medium, e.g. a hard disk drive, a flash memory, Floppy-Disk, Random Access Memory (RAM), Programmable Read Only Memory (PROM), Erasable Programmable Read Only Memory (EPROM), an Electronically Erasable Programmable Read Only Memory (EEPROM), or a network storage.</p>
<p id="p0056" num="0056">More details and aspects of the system and of the microscope system are mentioned in connection with the proposed concept or one or more examples described above or below (e.g. <figref idref="f0001 f0002">Fig. 1a to 2</figref>, <figref idref="f0002 f0006">4 to 8</figref>). The system and of the microscope system may comprise one or more additional optional features corresponding to one or more aspects of the proposed concept or one or more examples described above or below.</p>
<p id="p0057" num="0057"><figref idref="f0002">Fig. 4</figref> shows a flow chart of an embodiment of a method for determining a transformation function. The method comprises obtaining 410 first imaging sensor data of a reference object from a first imaging sensor of a microscope and second imaging sensor data of the reference object from a second imaging sensor of the microscope. The first imaging sensor data comprises sensor data on light sensed in a first plurality of mutually separated wavelength band. The second imaging sensor data comprises sensor data on light sensed in a second plurality of mutually separated wavelength bands. The wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging. The method comprises obtaining 420 a composite reference image of the reference object, the composite reference image comprising a plurality of color channels. The method comprises determining 430 the transformation function by determining a set of transformation factors that provide an<!-- EPO <DP n="21"> --> approximate transformation between the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite reference image. The transformation function is based on the set of transformation factors.</p>
<p id="p0058" num="0058">As indicated above, features described in connection with the system 310of <figref idref="f0002">Fig. 3</figref> may be likewise applied to the method of <figref idref="f0002">Fig. 4</figref>.</p>
<p id="p0059" num="0059">More details and aspects of the method are mentioned in connection with the proposed concept or one or more examples described above or below (e.g. <figref idref="f0001 f0002">Fig. 1a to 3</figref>, <figref idref="f0003 f0006">5a to 8</figref>). The method may comprise one or more additional optional features corresponding to one or more aspects of the proposed concept or one or more examples described above or below.</p>
<p id="p0060" num="0060">Various embodiments of the present disclosure are based on using the fluorescence camera of a microscope system for multispectral reflectance imaging.</p>
<p id="p0061" num="0061">As has been pointed out before, surgeons often use the tissue (brain) color in order to distinguish tissue that is suspicious with regards to lesions. However, subtle tissue color differences might only be seen by surgeons with long experience and trained visual acuity, which is difficult to learn.</p>
<p id="p0062" num="0062">Multispectral reflectance imaging can capture very small or even invisible color differences by means of measuring very small spectral differences. However, for conventional multispectral imaging, additional hardware may be be necessary, in addition to the plurality of sensors of a modern microscope, which is typically equipped with 3D and fluorescence cameras.</p>
<p id="p0063" num="0063">Embodiments of the present disclosure may use the existing fluorescence sensor of a surgical microscope system for multispectral reflectance imaging when fluorescence mode is not active. This may enable the use of the function without additional cost, size, and complexity, by means of software.</p>
<p id="p0064" num="0064">Various embodiments may use multispectral imaging to produce accurate color tissue imaging. This can be done by measuring more than three spectral bands, e.g. six spectral bands (e.g. three spectral bands each from the first and second imaging sensor data), and then<!-- EPO <DP n="22"> --> digitally recombine the spectral information to calculate the RGB values (i.e. the combined color image). Thereby, the color capturing may be more accurate.</p>
<p id="p0065" num="0065">For imaging systems which use multiple sensors for reflectance and fluorescence imaging, multispectral reflectance imaging can be done by using the fluorescence bands in reflectance mode. This may be enabled by providing illumination in the fluorescence bands</p>
<p id="p0066" num="0066">Subtle color differences, with diagnostic importance, may be digitally enhanced so be easily seen even by untrained surgeons.</p>
<p id="p0067" num="0067"><figref idref="f0003">Fig. 5a</figref> shows a schematic diagram of different colors present in an image frame. In <figref idref="f0003">Fig. 5a</figref>, three regions 510-530 of organic tissue (e.g. brain tissue are shown), with each region having a specific color, with region 510 having a blueish color, region 520 having a greenish color, and region 530 having a reddish color.</p>
<p id="p0068" num="0068"><figref idref="f0003">Fig. 5b</figref> and <figref idref="f0004">5c</figref> show schematic diagrams of an intensity of the different colors of the different regions 510-530, as sensed in different frequency bands 1 - 6 , which are sensed by two sensors (1 and 2 of the indices), each sensor sensing light in three wavelength bands (A-C), thereby generating imaging sensor data S<sub>A1</sub>, S<sub>A2</sub>, S<sub>B1</sub>, S<sub>B2</sub>, S<sub>C1</sub> and S<sub>C2</sub>. As can be seen in <figref idref="f0004">Fig. 5c</figref>, the sensors may sense the light in frequency bands that are narrower than is shown in <figref idref="f0003">Fig. 5b</figref>, making the frequency bands mutually separated frequency bands. The different sensors are highlighted with different background patterns.</p>
<p id="p0069" num="0069">The sensor data that is generated in the mutually separated frequency bands may be used to generate an RGB image, e.g. via a transformation matrix comprising a set of transformation factors. <figref idref="f0004">Fig. 5d</figref> shows an exemplary transformation matrix, of size 3-by-6, comprising 18 transformation factors F<sub>11</sub> - F<sub>36</sub> (the indices showing the row and column). This transformation matrix is multiplied with a (6-by-1) matrix comprising the sensor data S<sub>A1</sub>, S<sub>A2</sub>, S<sub>B1</sub>, S<sub>B2</sub>, S<sub>C1</sub> and S<sub>C2</sub> to obtain the RGB values of the composite color image (as 3-by-1 matrix).</p>
<p id="p0070" num="0070"><figref idref="f0005">Fig. 6</figref> shows a schematic diagram of a microscope and a lighting system. As has been pointed out before, sensor data of two imaging sensors are used, of a first imaging sensor (e.g. a CCD sensor) 122 that is used for reflectance imaging, and of a second imaging sensor (e.g. a CCD sensor) 124 that is primarily used for fluorescence imaging. The first imaging sensor may be<!-- EPO <DP n="23"> --> configured to provide a color reflectance image, and the second imaging sensor is provided a fluorescence image (in a first operation mode) and a reflectance image (e.g. in three bands, in a second operation mode). Both imaging sensors are combined with mirrors 630; 640 which may filter light that is outside the frequency bands to be sensed by the respective imaging sensor. A polychroic mirror 650 is used to divide the light in the different wavelength bands and to direct the respective wavelength bands to the sensors. A lighting system 610 is used (in combination with a filter 620 to filter the light emitted by the lighting system) to illuminate the object that is imaged by the imaging sensors. <figref idref="f0005">Fig. 6</figref> further shows the light in the different wavelength bands (in different line styles), with visible light 660 being admitted to the first imaging sensor, and light in emission wavelength bands being admitted to the second imaging sensor, the light in the emission wavelength bands either being emitted 670 by the lighting system, or by a fluorescent material 680 of the object that is excited in a fluorescence excitation wavelength band (e.g. by the visible light 660).</p>
<p id="p0071" num="0071">In <figref idref="f0005">Figs. 7a</figref> and <figref idref="f0006">7b</figref>, a concept for generating the transformation function is illustrated. As has been introduced in connection with <figref idref="f0002">Figs. 3 and 4</figref>, the transformation function may be determined by using a reference color table with "known" RGB values of the color samples of the color table. <figref idref="f0005">Fig. 7a</figref> shows a schematic diagram of an exemplary color table, comprising color fields 1 to 20. The color table may be recorded by the two imaging sensors, and the corresponding sensor data may be determined for the individual mutually separated wavelength bands. The known RGB values and the sensor data of the individual wavelength bands may be input into an equation system, and the equation system may be solved, such that a difference between the obtained colors and the known RGB values is reduce or minimized for the different color samples. <figref idref="f0006">Fig. 7b</figref> shows a diagram of an exemplary system of equations, with columns for the number of the sample, columns for the sensor data of the different wavelength bands, and columns for the known RGB values.</p>
<p id="p0072" num="0072">More details and aspects of the concept are mentioned in connection with the proposed concept or one or more examples described above or below (e.g. <figref idref="f0001 f0002">Fig. 1a to 4</figref>, <figref idref="f0006">8</figref>). The concept may comprise one or more additional optional features corresponding to one or more aspects of the proposed concept or one or more examples described above or below.</p>
<p id="p0073" num="0073">Some embodiments relate to a microscope comprising a system as described in connection with one or more of the <figref idref="f0001 f0002 f0003 f0004 f0005 f0006">Figs. 1 to 7b</figref>. Alternatively, a microscope may be part of or connected<!-- EPO <DP n="24"> --> to a system as described in connection with one or more of the <figref idref="f0001 f0002 f0003 f0004 f0005 f0006">Figs. 1 to 7b</figref>. <figref idref="f0006">Fig. 8</figref> shows a schematic diagram of a microscope system comprising a microscope and a computer system. <figref idref="f0006">Fig. 8</figref> shows a schematic illustration of a system 800 configured to perform a method described herein. The system 800 comprises a microscope 810 and a computer system 820. The microscope 810 is configured to take images and is connected to the computer system 820. The computer system 820 is configured to execute at least a part of a method described herein. The computer system 820 may be configured to execute a machine learning algorithm. The computer system 820 and microscope 810 may be separate entities but can also be integrated together in one common housing. The computer system 820 may be part of a central processing system of the microscope 810 and/or the computer system 820 may be part of a subcomponent of the microscope 810, such as a sensor, an actor, a camera or an illumination unit, etc. of the microscope 810.</p>
<p id="p0074" num="0074">The computer system 820 may be a local computer device (e.g. personal computer, laptop, tablet computer or mobile phone) with one or more processors and one or more storage devices or may be a distributed computer system (e.g. a cloud computing system with one or more processors and one or more storage devices distributed at various locations, for example, at a local client and/or one or more remote server farms and/or data centers). The computer system 820 may comprise any circuit or combination of circuits. In one embodiment, the computer system 820 may include one or more processors which can be of any type. As used herein, processor may mean any type of computational circuit, such as but not limited to a microprocessor, a microcontroller, a complex instruction set computing (CISC) microprocessor, a reduced instruction set computing (RISC) microprocessor, a very long instruction word (VLIW) microprocessor, a graphics processor, a digital signal processor (DSP), multiple core processor, a field programmable gate array (FPGA), for example, of a microscope or a microscope component (e.g. camera) or any other type of processor or processing circuit. Other types of circuits that may be included in the computer system 820 may be a custom circuit, an application-specific integrated circuit (ASIC), or the like, such as, for example, one or more circuits (such as a communication circuit) for use in wireless devices like mobile telephones, tablet computers, laptop computers, two-way radios, and similar electronic systems. The computer system 820 may include one or more storage devices, which may include one or more memory elements suitable to the particular application, such as a main memory in the form of random access memory (RAM), one or more hard drives, and/or one or more drives that handle removable media such as compact disks (CD), flash memory cards, digital video disk<!-- EPO <DP n="25"> --> (DVD), and the like. The computer system 820 may also include a display device, one or more speakers, and a keyboard and/or controller, which can include a mouse, trackball, touch screen, voice-recognition device, or any other device that permits a system user to input information into and receive information from the computer system 820.</p>
<p id="p0075" num="0075">Some or all of the method steps may be executed by (or using) a hardware apparatus, like for example, a processor, a microprocessor, a programmable computer or an electronic circuit. In some embodiments, some one or more of the most important method steps may be executed by such an apparatus.</p>
<p id="p0076" num="0076">Depending on certain implementation requirements, embodiments of the invention can be implemented in hardware or in software. The implementation can be performed using a non-transitory storage medium such as a digital storage medium, for example a floppy disc, a DVD, a Blu-Ray, a CD, a ROM, a PROM, and EPROM, an EEPROM or a FLASH memory, having electronically readable control signals stored thereon, which cooperate (or are capable of cooperating) with a programmable computer system such that the respective method is performed. Therefore, the digital storage medium may be computer readable.</p>
<p id="p0077" num="0077">Some embodiments according to the invention comprise a data carrier having electronically readable control signals, which are capable of cooperating with a programmable computer system, such that one of the methods described herein is performed.</p>
<p id="p0078" num="0078">Generally, embodiments of the present invention can be implemented as a computer program product with a program code, the program code being operative for performing one of the methods when the computer program product runs on a computer. The program code may, for example, be stored on a machine readable carrier.</p>
<p id="p0079" num="0079">Other embodiments comprise the computer program for performing one of the methods described herein, stored on a machine readable carrier.</p>
<p id="p0080" num="0080">In other words, an embodiment of the present invention is, therefore, a computer program having a program code for performing one of the methods described herein, when the computer program runs on a computer.<!-- EPO <DP n="26"> --></p>
<p id="p0081" num="0081">A further embodiment of the present invention is, therefore, a storage medium (or a data carrier, or a computer-readable medium) comprising, stored thereon, the computer program for performing one of the methods described herein when it is performed by a processor. The data carrier, the digital storage medium or the recorded medium are typically tangible and/or non-transitionary. A further embodiment of the present invention is an apparatus as described herein comprising a processor and the storage medium.</p>
<p id="p0082" num="0082">A further embodiment of the invention is, therefore, a data stream or a sequence of signals representing the computer program for performing one of the methods described herein. The data stream or the sequence of signals may, for example, be configured to be transferred via a data communication connection, for example, via the internet.</p>
<p id="p0083" num="0083">A further embodiment comprises a processing means, for example, a computer or a programmable logic device, configured to, or adapted to, perform one of the methods described herein.</p>
<p id="p0084" num="0084">A further embodiment comprises a computer having installed thereon the computer program for performing one of the methods described herein.</p>
<p id="p0085" num="0085">A further embodiment according to the invention comprises an apparatus or a system configured to transfer (for example, electronically or optically) a computer program for performing one of the methods described herein to a receiver. The receiver may, for example, be a computer, a mobile device, a memory device or the like. The apparatus or system may, for example, comprise a file server for transferring the computer program to the receiver.</p>
<p id="p0086" num="0086">In some embodiments, a programmable logic device (for example, a field programmable gate array) may be used to perform some or all of the functionalities of the methods described herein. In some embodiments, a field programmable gate array may cooperate with a microprocessor in order to perform one of the methods described herein. Generally, the methods are preferably performed by any hardware apparatus.</p>
<p id="p0087" num="0087">As used herein the term "and/or" includes any and all combinations of one or more of the associated listed items and may be abbreviated as "/".<!-- EPO <DP n="27"> --></p>
<p id="p0088" num="0088">Although some aspects have been described in the context of an apparatus, it is clear that these aspects also represent a description of the corresponding method, where a block or device corresponds to a method step or a feature of a method step. Analogously, aspects described in the context of a method step also represent a description of a corresponding block or item or feature of a corresponding apparatus.<!-- EPO <DP n="28"> --></p>
<heading id="h0006"><b>List of reference Signs</b></heading>
<p id="p0089" num="0089">
<dl id="dl0002" compact="compact">
<dt>100</dt><dd>Microscope system</dd>
<dt>105</dt><dd>Base unit</dd>
<dt>110</dt><dd>System</dd>
<dt>112</dt><dd>Interface</dd>
<dt>114</dt><dd>One or more processors</dd>
<dt>116</dt><dd>One or more storage devices</dd>
<dt>120</dt><dd>Microscope</dd>
<dt>122</dt><dd>First imaging sensor</dd>
<dt>124</dt><dd>Second imaging sensor</dd>
<dt>126</dt><dd>Beam splitter/polychroic mirror</dd>
<dt>130</dt><dd>Lighting system</dd>
<dt>140a/b</dt><dd>Displays</dd>
<dt>150</dt><dd>Steering handles</dd>
<dt>160</dt><dd>Arm</dd>
<dt>210</dt><dd>Obtaining first and second imaging sensor data</dd>
<dt>220</dt><dd>Generating a composite color image</dd>
<dt>300</dt><dd>Reference object</dd>
<dt>310</dt><dd>System</dd>
<dt>312</dt><dd>Interface</dd>
<dt>314</dt><dd>One or more processors</dd>
<dt>316</dt><dd>One or more storage devices</dd>
<dt>410</dt><dd>Obtaining first and second imaging sensor data</dd>
<dt>420</dt><dd>Obtaining a composite reference image</dd>
<dt>430</dt><dd>Determining a transformation function</dd>
<dt>510-530</dt><dd>Regions having different colors</dd>
<dt>610</dt><dd>Lighting system</dd>
<dt>620-640</dt><dd>Filters</dd>
<dt>650</dt><dd>Polychroic mirror</dd>
<dt>660</dt><dd>Visible light</dd>
<dt>670</dt><dd>Light emitted by the lighting system and reflected by object</dd>
<dt>680</dt><dd>Light emitted by the object<!-- EPO <DP n="29"> --></dd>
<dt>800</dt><dd>System</dd>
<dt>810</dt><dd>Microscope</dd>
<dt>820</dt><dd>Computer System</dd>
</dl></p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="30"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A system (110) for a microscope system (100), the system (110) comprising one or more processors (114) and one or more storage devices (116), wherein the system is configured to:
<claim-text>obtain first imaging sensor data from a first imaging sensor (122) of a microscope (120) of the microscope system and second imaging sensor data from a second imaging sensor (124) of the microscope, the first imaging sensor data comprising sensor data on light sensed in a first plurality of mutually separated wavelength bands, the second imaging sensor data comprising sensor data on light sensed in a second plurality of mutually separated wavelength bands, wherein the wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging; and</claim-text>
<claim-text>generate a composite color image based on the first imaging sensor data and based on the second imaging sensor data, the composite color image being based on a plurality of color channels,</claim-text>
<claim-text>wherein the composite color image is generated using a transformation function to define a transformation to be performed between the imaging sensor data and the composite color image, such that the composite color image is generated using sensor data on light sensed in each wavelength band of the first and second plurality of mutually separated wavelength bands.</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The system according to claim 1, wherein the transformation is based on a set of transformation factors that each define a transformation to be performed between the imaging sensor data on light sensed in a wavelength band and a color channel of the composite color image.</claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The system according to claim 2, wherein the set of transformation factors comprises one transformation factor for each combination of wavelength band and color channel,<br/>
<!-- EPO <DP n="31"> -->and/or wherein the set of transformation factors provide a transformation between the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite color image.</claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>The system according to one of the claims 1 to 3, wherein the composite color image comprises three color channels, wherein each of the color channels is generated based on a transformation of the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands, the transformation being performed using the transformation function.</claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>The system according to one of the claims 1 to 4, wherein the transformation function is implemented by a transformation matrix, wherein the system is configured to transform the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands using the transformation matrix.</claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>The system according to one of the claims 1 to 5, wherein the system is configured to provide a display signal to a display (140) of the microscope system using an interface (112) of the system (110), to cause the display to show the composite color image.</claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>A system (310) for determining a transformation function, the system comprising one or more processors (314) and one or more storage devices (316), wherein the system is configured to:
<claim-text>obtain first imaging sensor data of a reference object (300) from a first imaging sensor (122) of a microscope (120) and second imaging sensor data of the reference object from a second imaging sensor (124) of the microscope, the first imaging sensor data comprising sensor data on light sensed in a first plurality of mutually separated wavelength bands, the second imaging sensor data comprising sensor data on light sensed in a second plurality of mutually separated wavelength bands, wherein the wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging;<!-- EPO <DP n="32"> --></claim-text>
<claim-text>obtain a composite reference image of the reference object, the composite reference image comprising a plurality of color channels; and</claim-text>
<claim-text>determine the transformation function by determining a set of transformation factors that provide an approximate transformation between the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite reference image, the transformation function being based on the set of transformation factors.</claim-text></claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>The system according to claim 7, wherein the system is configured to identify a set of transformation factors that yields a lower mismatch between the composite reference image and a transformed image that is generated based on the set of transformation factors than at least one other set of transformation factors.</claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>The system according to claim 8, wherein the composite reference image of the reference image defines a plurality of colors of a plurality of portions of the reference object, the plurality of colors comprising a pre-defined first subset of colors and a second subset of colors, wherein the system is configured to identify a set of transformation factors that yields a lower mismatch between the composite reference image and a transformed image that is generated based on the set of transformation factors than at least one other set of transformation factors for the pre-defined first subset of colors.</claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>The system according to claim 9, wherein the pre-defined first subset of colors are colors that are present as colors of organic tissue in a surgical setting.</claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>The system according to one of the claims 9 or 10, wherein the system is configured to identify a set of transformation factors that reduces a mismatch value representing the mismatch between the composite reference image compared to at least one other set of transformation factors, wherein the mismatch value is calculated for the colors of the plurality colors, wherein a mismatch for a color of the pre-defined first subset of colors has a higher impact on the mismatch value than a mismatch for a color of the second subset of colors.<!-- EPO <DP n="33"> --></claim-text></claim>
<claim id="c-en-0012" num="0012">
<claim-text>A microscope system (100) comprising the system (110) according to one of the claims 1 to 6 and the microscope (120) with the first (122) and second (124) imaging sensor, wherein one of the first and the second imaging sensor is an imaging sensor that is adapted to provide a fluorescence imaging functionality of the microscope system.</claim-text></claim>
<claim id="c-en-0013" num="0013">
<claim-text>A method for a microscope system, the method comprising:
<claim-text>obtaining (210) first imaging sensor data from a first imaging sensor of a microscope of the microscope system and second imaging sensor data from a second imaging sensor of the microscope, the first imaging sensor data comprising sensor data on light sensed in a first plurality of mutually separated wavelength bands, the second imaging sensor data comprising sensor data on light sensed in a second plurality of mutually separated wavelength bands, wherein the wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging; and</claim-text>
<claim-text>generating (220) a composite color image based on the first imaging sensor data and based on the second imaging sensor data, the composite color image being based on a plurality of color channels,</claim-text>
<claim-text>wherein the composite color image is generated using a transformation function to define a transformation to be performed between the imaging sensor data and the composite color image, such that the composite color image is generated using sensor data on light sensed in each wavelength band of the first and second plurality of mutually separated wavelength bands.</claim-text></claim-text></claim>
<claim id="c-en-0014" num="0014">
<claim-text>A method for determining a transformation function, the method comprising:
<claim-text>obtaining (410) first imaging sensor data of a reference object from a first imaging sensor of a microscope and second imaging sensor data of the reference object from a second imaging sensor of the microscope, the first imaging sensor data comprising sensor data on light sensed in a first plurality of mutually separated wavelength bands,<!-- EPO <DP n="34"> --> the second imaging sensor data comprising sensor data on light sensed in a second plurality of mutually separated wavelength bands, wherein the wavelength bands of the first plurality of mutually separated wavelength bands or of the second plurality of mutually separated wavelength bands are wavelength bands that are used for fluorescence imaging;</claim-text>
<claim-text>obtaining (420) a composite reference image of the reference object, the composite reference image comprising a plurality of color channels; and</claim-text>
<claim-text>determining (430) the transformation function by determining a set of transformation factors that provide an approximate transformation between the imaging sensor data of each wavelength band of the first and second plurality of mutually separated wavelength bands and each of the color channels of the composite reference image, the transformation function being based on the set of transformation factors.</claim-text></claim-text></claim>
<claim id="c-en-0015" num="0015">
<claim-text>Computer program with a program code for performing at least one of the methods according to claim 13 or 14 when the computer program is run on a processor.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="35"> -->
<figure id="f0001" num="1a,1b"><img id="if0001" file="imgf0001.tif" wi="142" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> -->
<figure id="f0002" num="2,3,4"><img id="if0002" file="imgf0002.tif" wi="123" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> -->
<figure id="f0003" num="5a,5b"><img id="if0003" file="imgf0003.tif" wi="151" he="220" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> -->
<figure id="f0004" num="5c,5d"><img id="if0004" file="imgf0004.tif" wi="151" he="171" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> -->
<figure id="f0005" num="6,7a"><img id="if0005" file="imgf0005.tif" wi="92" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> -->
<figure id="f0006" num="7b,8"><img id="if0006" file="imgf0006.tif" wi="131" he="179" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="157" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="155" he="233" type="tif"/></search-report-data><search-report-data date-produced="20200908" id="srepxml" lang="en" srep-office="EP" srep-type="ep-sr" status="n"><!--
 The search report data in XML is provided for the users' convenience only. It might differ from the search report of the PDF document, which contains the officially published data. The EPO disclaims any liability for incorrect or incomplete data in the XML for search reports.
 -->

<srep-info><file-reference-id>LMS190901PAEP</file-reference-id><application-reference><document-id><country>EP</country><doc-number>20167604.6</doc-number></document-id></application-reference><applicant-name><name>Leica Instruments (Singapore) Pte. Ltd.</name></applicant-name><srep-established srep-established="yes"/><srep-invention-title title-approval="yes"/><srep-abstract abs-approval="yes"/><srep-figure-to-publish figinfo="by-applicant"><figure-to-publish><fig-number>1a</fig-number></figure-to-publish></srep-figure-to-publish><srep-info-admin><srep-office><addressbook><text>MN</text></addressbook></srep-office><date-search-report-mailed><date>20200928</date></date-search-report-mailed></srep-info-admin></srep-info><srep-for-pub><srep-fields-searched><minimum-documentation><classifications-ipcr><classification-ipcr><text>G06T</text></classification-ipcr></classifications-ipcr></minimum-documentation></srep-fields-searched><srep-citations><citation id="sr-cit0001"><patcit dnum="US2012061590A1" id="sr-pcit0001" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2012061590&amp;CY=ep"><document-id><country>US</country><doc-number>2012061590</doc-number><kind>A1</kind><name>KHOJASTEH MEHRNOUSH [CA] ET AL</name><date>20120315</date></document-id></patcit><category>X</category><rel-claims>1-15</rel-claims><rel-passage><passage>* paragraphs [0034],  [0058],  [0061],  [0081],  [0091],  [0092],  [0098],  [0099],  [0121]; figures 1,5A,7A *</passage></rel-passage></citation><citation id="sr-cit0002"><patcit dnum="EP3205254A1" id="sr-pcit0002" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=EP3205254&amp;CY=ep"><document-id><country>EP</country><doc-number>3205254</doc-number><kind>A1</kind><name>LEICA INSTR  PTE LTD [SG]</name><date>20170816</date></document-id></patcit><category>A</category><rel-claims>1-15</rel-claims><rel-passage><passage>* paragraphs [0006],  [0016] *</passage></rel-passage></citation></srep-citations><srep-admin><examiners><primary-examiner><name>Krawczyk, Grzegorz</name></primary-examiner></examiners><srep-office><addressbook><text>Munich</text></addressbook></srep-office><date-search-completed><date>20200908</date></date-search-completed></srep-admin><!--							The annex lists the patent family members relating to the patent documents cited in the above mentioned European search report.							The members are as contained in the European Patent Office EDP file on							The European Patent Office is in no way liable for these particulars which are merely given for the purpose of information.							For more details about this annex : see Official Journal of the European Patent Office, No 12/82						--><srep-patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2012061590</doc-number><kind>A1</kind><date>20120315</date></document-id></priority-application><family-member><document-id><country>CA</country><doc-number>2762886</doc-number><kind>A1</kind><date>20101125</date></document-id></family-member><family-member><document-id><country>US</country><doc-number>2012061590</doc-number><kind>A1</kind><date>20120315</date></document-id></family-member><family-member><document-id><country>WO</country><doc-number>2010132990</doc-number><kind>A1</kind><date>20101125</date></document-id></family-member></patent-family><patent-family><priority-application><document-id><country>EP</country><doc-number>3205254</doc-number><kind>A1</kind><date>20170816</date></document-id></priority-application><family-member><document-id><country>CN</country><doc-number>107137053</doc-number><kind>A</kind><date>20170908</date></document-id></family-member><family-member><document-id><country>EP</country><doc-number>3205254</doc-number><kind>A1</kind><date>20170816</date></document-id></family-member><family-member><document-id><country>JP</country><doc-number>6456416</doc-number><kind>B2</kind><date>20190123</date></document-id></family-member><family-member><document-id><country>JP</country><doc-number>2017148503</doc-number><kind>A</kind><date>20170831</date></document-id></family-member><family-member><document-id><country>US</country><doc-number>2017237958</doc-number><kind>A1</kind><date>20170817</date></document-id></family-member><family-member><document-id><country>US</country><doc-number>2021021795</doc-number><kind>A1</kind><date>20210121</date></document-id></family-member></patent-family></srep-patent-family></srep-for-pub></search-report-data>
</ep-patent-document>
