<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP21162299A1" file="EP21162299NWA1.xml" lang="en" country="EP" doc-number="3889740" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3889740</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>21162299.8</B210><B220><date>20210312</date></B220><B250>en</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>202016839056</B310><B320><date>20200402</date></B320><B330><ctry>US</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>G06F   3/01        20060101AFI20210812BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>A61B   5/18        20060101ALI20210812BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>B60W  50/14        20200101ALI20210812BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>G06K   9/00        20060101ALI20210812BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G06F2203/011       20130101 LA20210810BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>B60W  50/14        20130101 LI20210810BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>G06F   3/015       20130101 LI20210810BHEP        </text></classification-cpc><classification-cpc sequence="4"><text>G06F   3/013       20130101 FI20210810BHEP        </text></classification-cpc><classification-cpc sequence="5"><text>A61B   5/18        20130101 LI20210810BHEP        </text></classification-cpc><classification-cpc sequence="6"><text>G06K   9/00845     20130101 LI20210810BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>AUF AFFEKTIV-KOGNITIVER BELASTUNG BASIERENDER DIGITALER ASSISTENT</B542><B541>en</B541><B542>AFFECTIVE-COGNITIVE LOAD BASED DIGITAL ASSISTANT</B542><B541>fr</B541><B542>ASSISTANT NUMÉRIQUE À BASE DE CHARGE AFFECTIVE-COGNITIVE</B542></B540><B590><B598>3</B598></B590></B500><B700><B710><B711><snm>Harman International Industries, Incorporated</snm><iid>101475493</iid><irf>J110872EP</irf><adr><str>400 Atlantic Street, 15th Floor</str><city>Stamford, CT 06901</city><ctry>US</ctry></adr></B711></B710><B720><B721><snm>BOULANGER, Adam</snm><adr><str>400 Atlantic Street,
15th Floor</str><city>Stamford, CT 06901</city><ctry>US</ctry></adr></B721><B721><snm>KRATZ, Sven</snm><adr><str>400 Atlantic Street,
15th Floor</str><city>Stamford, CT 06901</city><ctry>US</ctry></adr></B721><B721><snm>VERBEKE, Joseph</snm><adr><str>400 Atlantic Street,
15th Floor</str><city>Stamford, CT 06901</city><ctry>US</ctry></adr></B721><B721><snm>SESHADRI, Priya</snm><adr><str>400 Atlantic Street,
15th Floor</str><city>Stamford, CT 06901</city><ctry>US</ctry></adr></B721><B721><snm>BURMISTROV, Evgeny</snm><adr><str>400 Atlantic Street,
15th Floor</str><city>Stamford, CT 06901</city><ctry>US</ctry></adr></B721><B721><snm>MANSOURIAN, Neeka</snm><adr><str>400 Atlantic Street,
15th Floor</str><city>Stamford, CT 06901</city><ctry>US</ctry></adr></B721><B721><snm>MARTI, Stefan</snm><adr><str>400 Atlantic Street,
15th Floor</str><city>Stamford, CT 06901</city><ctry>US</ctry></adr></B721></B720><B740><B741><snm>Rummler, Felix</snm><iid>100819180</iid><adr><str>Maucher Jenkins 
Liebigstrasse 39</str><city>80538 München</city><ctry>DE</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">Embodiments of the present disclosure set forth a computer-implemented method comprising receiving, from at least one sensor, sensor data associated with an environment, computing, based on the sensor data, a cognitive load associated with a user within the environment, computing, based on the sensor data, an affective load associated with an emotional state of the user, determining, based on both the cognitive load at the affective load, an affective-cognitive load, determining, based on the affective-cognitive load, a user readiness state associated with the user, and causing one or more actions to occur based on the user readiness state.
<img id="iaf01" file="imgaf001.tif" wi="86" he="79" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001"><b>BACKGROUND</b></heading>
<heading id="h0002"><b>Field of the Various Embodiments</b></heading>
<p id="p0001" num="0001">The various embodiments relate generally to digital assistants and, more specifically, to an affective-cognitive load based digital assistant.</p>
<heading id="h0003"><b>Description of the Related Art</b></heading>
<p id="p0002" num="0002">Various digital systems include digital assistants that assist users to perform tasks. For example, various vehicles include an advanced driver assistance system (ADAS) that assists a driver in handling a vehicle. Such an ADAS may include a driver monitoring system (DMS) that monitors the driver in order to assess the driver's handling of the vehicle and enables the ADAS to respond to the state of the driver by providing various levels of assistance, such as by generating notifications, automating various driving tasks, etc.</p>
<p id="p0003" num="0003">A user typically splits focus between multiple objects when interacting with a given environment. For example, a driver may split focus between driving a vehicle and conducting a conversation with another person. In certain situations, the driver may not properly adjust to provide enough focus on driving tasks in order to adequately navigate complex driving situations that sporadically occur, such as an increase in traffic, adverse weather conditions, sudden obstacles, and so forth.</p>
<p id="p0004" num="0004">In general, an ADAS may easily detect distracting objects and may determine whether a driver is focused on the road. However, the driver monitoring system may not be able to determine the amount of focus the driver has when performing driving tasks. For example, the DMS may include a camera that is used to determine whether the driver's eyes are focused on the road, but the DMS may not account for other objects and stimuli that otherwise lessen the focus on successfully performing driving tasks. As a result, ADAS may not be able to effectively assist the driver and the driver may fail to modify their behavior to properly handle the necessary driving tasks.</p>
<p id="p0005" num="0005">In light of the above, more effective techniques for monitoring the status of the user interacting in an environment would be useful.<!-- EPO <DP n="2"> --></p>
<heading id="h0004"><b>SUMMARY</b></heading>
<p id="p0006" num="0006">One embodiment sets forth a computer-implemented method comprising receiving, from at least one sensor, sensor data associated with an environment, computing, based on the sensor data, a cognitive load associated with a user within the environment, computing, based on the sensor data, an affective load associated with an emotional state of the user, determining, based on both the cognitive load at the affective load, an affective-cognitive load, determining, based on the affective-cognitive load, a user readiness state associated with the user, and causing one or more actions to occur based on the user readiness state.</p>
<p id="p0007" num="0007">Further embodiments provide, among other things, a method and a system configured to implement the computer-readable storage medium set forth above.</p>
<p id="p0008" num="0008">At least one technological advantage of the disclosed techniques over previous digital assistant systems is that computing a user readiness state based on a combination of direct measurements that estimate the cognitive load of a user and the emotional state of the user provides a more accurate indication of the ability of the user to handle one or more tasks.</p>
<heading id="h0005"><b>BRIEF DESCRIPTION OF THE DRAWINGS</b></heading>
<p id="p0009" num="0009">So that the manner in which the above recited features of the various embodiments can be understood in detail, a more particular description of the inventive concepts, briefly summarized above, may be had by reference to various embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of the inventive concepts and are therefore not to be considered limiting of scope in any way, and that there are other equally effective embodiments.
<ul id="ul0001" list-style="none" compact="compact">
<li><figref idref="f0001">Figure 1</figref> illustrates a block diagram of the user readiness system configured to implement one or more aspects of the present disclosure.</li>
<li><figref idref="f0002">Figure 2</figref> illustrates a passenger compartment of a vehicle that is configured to implement the user readiness system of <figref idref="f0001">Figure 1</figref>, according to various embodiments.</li>
<li><figref idref="f0003">Figure 3</figref> illustrates a block diagram of the user readiness application of <figref idref="f0001">Figure 1</figref>, according to various embodiments.</li>
<li><figref idref="f0004">Figure 4A</figref> illustrates an example lookup table of affective-cognitive load values associated with the user readiness application of <figref idref="f0001">Figure 1</figref>, according to various embodiments.</li>
<li><figref idref="f0005">Figure 4B</figref> illustrates another example lookup table of affective-cognitive load values associated with the user readiness application of <figref idref="f0001">Figure 1</figref>, according to various embodiments.<!-- EPO <DP n="3"> --></li>
<li><figref idref="f0006">Figure 5</figref> illustrates an affective-cognitive load computed from various biometric values derived by the user readiness application of <figref idref="f0001">Figure 1</figref>, according to various embodiments.</li>
<li><figref idref="f0007">Figure 6</figref> illustrates an example vehicle system that includes an affective-cognitive load based digital assistant of <figref idref="f0003">Figure 3</figref>, according to various embodiments.</li>
<li><figref idref="f0008">Figure 7</figref> is a flow diagram of method steps for generating an output signal based on an affective-cognitive load, according to various embodiments.</li>
</ul></p>
<heading id="h0006"><b>DETAILED DESCRIPTION</b></heading>
<p id="p0010" num="0010">In the following description, numerous specific details are set forth to provide a more thorough understanding of the various embodiments. However, it will be apparent to one of skilled in the art that the inventive concepts may be practiced without one or more of these specific details.</p>
<heading id="h0007">System Overview</heading>
<p id="p0011" num="0011"><figref idref="f0001">Figure 1</figref> illustrates a block diagram of user readiness system 100 configured to implement one or more aspects of the present disclosure. As shown, user readiness system 100 includes, without limitation, computing device 110, sensor(s) 120, input/output (I/O) device(s) 130, and network 150. Computing device 110 includes processing unit 112 and memory 114, where memory 114 stores user readiness application 140 and database 142. Network 150 includes data store 152.</p>
<p id="p0012" num="0012">In operation, processing unit 112 receives sensor data from sensor(s) 120. Processing unit 112 executes user readiness system 140 to in order to process the sensor data and determine various biometric values associated with the psychophysiological state a user. Such biometric values include a cognitive load that estimates an amount of brain activity that a user is employing, and an affective load that estimates an emotion (specified as a pre-defined emotion or a set of emotion parameterized metrics associated with a psychophysiological state) that a user is experiencing. In various embodiments, the affective load may include one or more separate emotional metrics, such as separate arousal and/or valence values. Additionally or alternatively, the affective load may comprise one or more discrete emotional states and/or associated values of each. Upon determining biometric values for a user, user readiness application 140 combines the biometric values via one or more estimation algorithms to determine an affective-cognitive load (ACL) as a composite of the biometric values. In various embodiments, user readiness application 140 compares the ACL with other ACL data included<!-- EPO <DP n="4"> --> in database 142 and/or data store 152 in order to map the ACL to a user readiness that estimates the ability of the user to handle one or more tasks. User readiness application 140 and/or other applications (not shown) may then generate output signals based on the determined user readiness state. For example, user readiness application 140 may be a component of an advanced driver assistance system (ADAS) that automates certain tasks and/or provides notifications via one or more output signals once user readiness application 140 determines that the user readiness is outside of a target value and/or target range.</p>
<p id="p0013" num="0013">As noted above, computing device 110 can include processing unit 112 and memory 114. Computing device 110 can be a device that includes one or more processing units 112, such as a system-on-a-chip (SoC). In various embodiments, computing device 110 may be a mobile computing device, such as a tablet computer, mobile phone, media player, and so forth. In some embodiments, computing device 110 may be a head unit included in a vehicle system. Generally, computing device 110 can be configured to coordinate the overall operation of user readiness system 100. The embodiments disclosed herein contemplate any technically-feasible system configured to implement the functionality of user readiness system 100 via computing device 110.</p>
<p id="p0014" num="0014">Various examples of computing device 110 include mobile devices (<u>e.g.</u>, cellphones, tablets, laptops, etc.), wearable devices (<u>e.g.</u>, watches, rings, bracelets, headphones, etc.), consumer products (<u>e.g.</u>, gaming, gambling, etc.), smart home devices (<u>e.g.</u>, smart lighting systems, security systems, digital assistants, etc.), communications systems (<u>e.g.</u>, conference call systems, video conferencing systems, etc.), and so forth. Computing device 110 may be located in various environments including, without limitation, home environment (<u>e.g.</u>, office, kitchen, etc.), road vehicle environments (<u>e.g.</u>, consumer car, commercial truck, etc.), aerospace and/or aeronautical environments (<u>e.g.</u>, airplanes, helicopters, spaceships, etc.), nautical and submarine environments, and so forth.</p>
<p id="p0015" num="0015">Processing unit 112 may include a central processing unit (CPU), a digital signal processing unit (DSP), a microprocessor, an application-specific integrated circuit (ASIC), a neural processing unit (NPU), a graphics processing unit (GPU), a field-programmable gate array (FPGA), and so forth. Processing unit 112 generally comprises a programmable processor that executes program instructions to manipulate input data. In some embodiments, processing unit 112 may include any number of processing cores, memories, and other modules for facilitating program execution. For example, processor unit 112 could receive input from a user<!-- EPO <DP n="5"> --> via I/O devices 130 and generate pixels for display on I/O device 130 (<u>e.g.</u>, a display device). In some embodiments, processing unit 112 can be configured to execute user readiness application 140 in order to analyze acquired sensor data and estimate the ability of a user to handle specific tasks by determining a user readiness state. In such instances, user readiness application 140 may output the estimated user readiness state to one or more I/O modules 130 in order for the I/O modules 130 that respond to the determined user readiness state (<u>e.g.</u>, output signals to provide assistance, output signals to enable a user to perform manual tasks, etc.).</p>
<p id="p0016" num="0016">Memory 114 can include a memory module or collection of memory modules. Memory 114 generally comprises storage chips such as random access memory (RAM) chips that store application programs and data for processing by processing unit 112. In various embodiments, memory 114 may include non-volatile memory, such as optical drives, magnetic drives, flash drives, or other storage. In some embodiments, separate data stores, such as data store 152 included in network 150 ("cloud storage") may supplement memory 114. User readiness application 140 within memory 114 can be executed by processing unit 112 to implement the overall functionality of the computing device 110 and, thus, to coordinate the operation of the user readiness system 100 as a whole.</p>
<p id="p0017" num="0017">User readiness application 140 processes acquire sensor data associated with a user and/or environment in order to determine various metrics associated with the user's brain activity and/or emotion. In various embodiments, user readiness application 140 may receive sensor data from one or more sensors 120 and may analyze the sensor data in order to determine the cognitive load of the user and/or the affective load, which may include separate emotional parameters that indicate the emotion being experienced by the user. User readiness application 140 determines, based on both the cognitive load and the affective load, an affective-cognitive load (ACL) of the user that indicates the user's overall mental activity and ability to manage a set of tasks. In various embodiments, user readiness application 140 may compare the ACL with other ACL values and/or thresholds in order to map the ACL to a specific user readiness state. User readiness application 140 may then output the user readiness state to one or more I/O devices 130 that respond to the determined user readiness state. For example, user readiness application 140 may determine that a current user readiness state is below the threshold level required to successfully navigate the current traffic conditions. User readiness application 140 could then send the user readiness state an ADAS that sends an output signal to I/O device 130 (<u>e.g.</u>, a display device) in order to play a notification sound to alert the driver. In some instances,<!-- EPO <DP n="6"> --> the ADAS could also respond by performing one or more automated tasks (<u>e.g.</u>, assisted driving).</p>
<p id="p0018" num="0018">Database 142 can store values and other data retrieved by processing unit 112 to coordinate the operation of user readiness application 140. In various embodiments, processing unit 112 may be configured to store values in database 142 and/or retrieve values stored in database 142. For example, database 142 could store historical ACL values, lookup tables, ACL algorithms, mappings of sensor values to emotional parameterized metrics, mappings of ACL values to driver readiness levels, and so forth. In some embodiments, database 142 may store values retrieved from data store 152. In such instances, database 142 may receive periodic updates and provide values to user readiness application 140 between the periodic updates.</p>
<p id="p0019" num="0019">In various embodiments, database 142 can include one or more lookup tables, where the lookup tables store entries that include mappings between values. For example, database 142 could include a set of ACL lookup tables that includes entries of mappings of biometric values (<u>e.g.</u>, cognitive load, affective load, arousal, valence, etc.), to ACL values. Additionally or alternatively, database 142 can include a set of ACL lookup tables that maps biometric values to pre-defined ACL levels (<u>e.g.</u>, high ACL, medium ACL, low ACL, etc.), and/or a set of ACL lookup tables that maps pre-defined biometric values (<u>e.g.</u>, a defined value for the psychophysiological trait of "angry") to specific ACL values and/or pre-defined ACL levels.</p>
<p id="p0020" num="0020">Sensor(s) 120 may include one or more devices that perform measurements and/or acquire data related to certain subjects in an environment. In various embodiments, sensor(s) 120 may generate sensor data that is related to the cognitive load and/or affective load of the user. For example, sensor(s) 120 could collect biometric data related to the user (<u>e.g.</u>, heart rate, brain activity, skin conductance, blood oxygenation, pupil size, galvanic skin response, blood-pressure level, average blood glucose concentration, etc.). Additionally or alternatively, sensor(s) 120 can generate sensor data related to objects in the environment that are not the user. For example, sensor(s) 120 could generate sensor data about the operation of a vehicle, including the speed of the vehicle, pedal position, steering wheel position, ambient temperature in the vehicle, amount of light within the vehicle, and so forth. In some embodiments, sensor(s) 120 may be coupled to and/or included within computing device 110 and send sensor data to processing unit 112. Processing unit 112 executes user readiness application 140 in order to determine a user readiness state based on a determined cognitive-affective load that is derived from the acquired sensor data.<!-- EPO <DP n="7"> --></p>
<p id="p0021" num="0021">In various embodiments, sensor(s) 120 may acquire sensor data that user readiness application 140 processes in order to classify an emotion that the user is experiencing. For example, sensor(s) 120 could include a user-facing camera that records the face of the user as image data. User readiness application 140 could then analyze the image data in order to determine the facial expression of the user, and then map the facial expression to a specific emotion. In another example, sensor(s) 120 could include sensors in various parts of the vehicle (<u>e.g.</u>, driver's seat passenger seat, steering wheel, etc.) that acquire biological and/or physiological signals of a user (<u>e.g.</u>, perspiration, heart rate, heart-rate variability (HRV), blood flow, blood-oxygen levels, breathing rate, galvanic skin response (GSR), sounds created by a user, behaviors of a user, etc.). In such instances, user readiness application 140 could compute one or more quantitative emotional parameterized metrics, such as emotional arousal (A) and/or emotional valence (V) that indicate the emotion the user is experiencing.</p>
<p id="p0022" num="0022">In various embodiments, the sensor(s) 120 may also acquire data that user readiness application 140 processes in order to compute a cognitive load that a user is experiencing. For example, sensor(s) 120 could include a pupil sensor (<u>e.g.</u>, a camera focused on the eyes of the user) that acquires image data about at least one pupil of the user. User readiness application 140 could then perform various pupillometry techniques to detect eye parameters (<u>e.g.</u>, fluctuations in the user's pupil diameter, direction of the pupil is gazing, eye lid position, etc.) in order to estimate a cognitive load (CL) of the user. In another example, sensor(s) 120 could include heart rate sensors and/or other biometric sensors that acquire biological and/or physiological signals of the user (<u>e.g.</u>, heart rate, breathing rate, eye motions, GSR, neural brain activity, etc.). In such instances, user readiness application 140 could compute the cognitive load from one or more of the acquired biological and/or physiological signals.</p>
<p id="p0023" num="0023">In various embodiments, the sensor(s) 120 may include optical sensors, such as RGB cameras, infrared cameras, depth cameras, and/or camera arrays, which include two or more of such cameras. Other optical sensors may include imagers and laser sensors. In some embodiments, sensor(s) 120 may include physical sensors, such as touch sensors, pressure sensors, position sensors (<u>e.g.</u>, an accelerometer and/or an inertial measurement unit (IMU)), motion sensors, and so forth, that register the body position and/or movement of the user. In such instances, user readiness application 140 may analyze the acquired sensor data to determine the movement of the user, and then correlate such movement with specific emotions (<u>e.g.</u>, boredom, fatigue, arousal, etc.) and/or a cognitive load of the user.<!-- EPO <DP n="8"> --></p>
<p id="p0024" num="0024">In various embodiments, the sensor(s) 120 may include physiology sensors, such as heart-rate monitors, electroencephalography (EEG) systems, radio sensors, thermal sensors, galvanic skin response sensors (<u>e.g.</u>, sensors that measure change in electrical resistance of skin caused by emotional stress), contactless sensor systems, magnetoencephalography (MEG) systems, and so forth. In various embodiments, user readiness application 140 may execute spectral entropy, weighted mean frequency, bandwidth, and/or spectral edge frequency to determine cognitive load from the acquired sensor data.</p>
<p id="p0025" num="0025">In addition, in some embodiments, sensor(s) 120 may include acoustic sensors, such as a microphone and/or a microphone array that acquires sound data. Such sound data may be processed by user readiness application 140 performing various natural language (NL) processing techniques, sentiment analysis, and/or speech analysis in order to determine the semantic meaning of the phrases spoken in the environment and/or infer emotional parameterized metrics from the semantic meaning. In another example, user readiness application 140 could analyze the acquired sound data using voice-tone analysis in order to infer emotion from the speech signal included in the sound data. In some embodiments, user readiness application 140 may execute various analysis techniques relating to the spectral centroid frequency and/or amplitude of the sound signal in order to classify the sound signal to a specific value for the cognitive load.</p>
<p id="p0026" num="0026">In some embodiments, sensor(s) 120 may include behavioral sensors that detect the activity of the user within the environment. Such behavioral sensors may include devices that acquire related activity data, such as devices that acquire application usage data and/or mobile device usage data. In such instances, user readiness application 140 may estimate the cognitive load and/or the emotional parameterized metrics by determining the activities in which the user is currently engaged. For example, a given application could be classified as being a fun, social application in which a user engages when happy and active, and/or is making the user happy and active. In such instances, user readiness application 140 could correlate the usage of the given application with a pre-defined emotion (<u>e.g.</u>, excited) and/or pre-defined emotional parameterized metrics (a high arousal value and a positive valence value).</p>
<p id="p0027" num="0027">I/O device(s) 130 may include devices capable of receiving input, such as a keyboard, a mouse, a touch-sensitive screen, a microphone and other input devices for providing input data to computing device 110. In various embodiments, I/O device(s) 130 may include devices capable of providing output, such as a display screen, loudspeakers, and the like. One or<!-- EPO <DP n="9"> --> more of I/O devices 130 can be incorporated in computing device 110 or may be external to computing device 110. In some embodiments, computing device 110 and/or one or more I/O device(s) 130 may be components of an advanced driver assistance system. In various embodiments, user readiness application 140 may determine a cognitive load and/or an emotional load based on inputs received by one or more I/O devices 130. For example, the vehicle could include a head unit that includes a user interface. In such instances, user readiness application 140 could determine the cognitive load and/or the emotional load of the user based on one or more inputs received via the head unit.</p>
<p id="p0028" num="0028">Network 150 may enable communications between computing device 110 and other devices in network via wired and/or wireless communications protocols, satellite networks, V2X networks, including Bluetooth, Bluetooth low energy (BLE), wireless local area network (WiFi), cellular protocols, and/or near-field communications (NFC). In various embodiments, network 150 may include one or more data stores 152 that store data associated with sensor data, biometric values, affective-cognitive loads, and/or driver readiness levels. In various embodiments, user readiness application 140 and/or other digital assistants included in other devices may retrieve information from the data store 152. In such instances, user readiness application 140 may analyze the retrieved data as part of determining the ACL of the user, comparing the ACL to situations that involve specific ACL values, and so forth.</p>
<p id="p0029" num="0029"><figref idref="f0002">Figure 2</figref> illustrates a passenger compartment 200 of a vehicle that is configured to implement the user readiness system 100 of <figref idref="f0001">Figure 1</figref>, according to various embodiments. As shown, passenger compartment 200 includes, without limitation, dashboard 210, windshield 220, and head unit 230. In various embodiments, passenger compartment 200 may include any number of additional components that implement any technically-feasible functionality. For example, passenger compartment 200 could include a rear-view camera (not shown).</p>
<p id="p0030" num="0030">As shown, head unit 230 is located in the center of dashboard 210. In various embodiments, head unit 230 may be mounted at any location within passenger compartment 200 in any technically-feasible fashion that does not block the windshield 220. Head unit 230 may include any number and type of instrumentation and applications and may provide any number of input and output mechanisms. For example, head unit 230 could enable users (<u>e.g.</u>, the driver and/or passengers) to control entertainment functionality. In some embodiments, head unit 230 may include navigation functionality and/or advanced driver assistance (ADA) functionality designed to increase driver safety, automate driving tasks, and so forth.<!-- EPO <DP n="10"> --></p>
<p id="p0031" num="0031">Head unit 230 supports any number of input and output data types and formats, as known in the art. For example, head unit 230 could include built-in Bluetooth for hands-free calling and/or audio streaming, universal serial bus (USB) connections, speech recognition, rear-view camera inputs, video outputs for any number and type of displays, and any number of audio outputs. In general, any number of sensors (<u>e.g.</u>, sensor(s) 120), displays, receivers, transmitters, etc., may be integrated into head unit 230, or may be implemented externally to head unit 230. In various embodiments, external devices may communicate with head unit 230 in any technically-feasible fashion.</p>
<p id="p0032" num="0032">While driving, the driver of the vehicle is exposed to a variety of stimuli that are related to either a primary task (<u>e.g.</u>, guiding the vehicle) and/or any number of secondary tasks. For example, the driver could see via windshield 220, lane markers 240, cyclist 242, a specialized car 244, and/or pedestrian 246. In response, the driver could steer the vehicle to track lane markers 240 while avoiding cyclist 242 and pedestrian 246, and then apply the brake pedal to allow police car 244 to cross the road in front of the vehicle. Further, the driver could concurrently or intermittently participate in conversation 250, listen to music 260, and/or attempt to soothe crying baby 270.</p>
<p id="p0033" num="0033">As shown, differing driving environments and/or engagement in secondary activities (<u>e.g.</u>, deep thinking, emotional conversations, etc.) typically increase the cognitive load experienced by the driver and may contribute to an unsafe driving environment for the driver and for in the proximity of the vehicle. In addition, the emotion experienced by the driver may also contribute to the unsafe driving environment, as the emotion that the user is experiencing may increase or decrease the ability of the driver to handle driving tasks.</p>
<heading id="h0008"><b>The Affective-Cognitive Load Based Digital Assistant</b></heading>
<p id="p0034" num="0034"><figref idref="f0003">Figure 3</figref> illustrate affective-cognitive load-based assistant 300 included in the user readiness system 100 of <figref idref="f0001">Figure 1</figref>, according to various embodiments. As shown, affective-cognitive load-based assistant 300 includes sensor(s) 120, user readiness application 140, and output device 340. User readiness application 140 includes biometric computation module 320 (including cognitive load computation module 314 and emotion computation module 316) and user readiness computation module 330.</p>
<p id="p0035" num="0035">In operation, biometric computation module 320 includes various modules 314, 316 that analyze sensor data 322 received from sensor(s) 120 in order to determine one or more<!-- EPO <DP n="11"> --> biometric values, including cognitive load 324 and/or emotion metrics 326. Biometric computation module 320 sends biometric values 324, 326 to user readiness computation module 330 that performs one or more algorithmic techniques in order to determine an affective-cognitive load (ACL) of the user. User readiness computation module 330 may then map the ACL onto user readiness state 332 that indicates the ability of the user to handle tasks. User readiness computation module 330 sends the user readiness state 332 to one or more output device(s) 340 that provide output signal(s) 342 that affect the user and/or modify the behavior of the vehicle. For example, user readiness application 140 could send user readiness state 332 to separate output devices 340, including to an ADAS and/or to a separate application on a device associated with the driver (<u>e.g.</u>, an application running on a wearable device). In such instances, both output devices 340 could respond by providing output signals 342, such as a notification signal that causes the wearable device to provide a haptic response. In some instances, an output device may directly affect the vehicle, such as when output signals 342 include one or more driving adjustment signals that adjust the direction and/or speed of the vehicle.</p>
<p id="p0036" num="0036">In various embodiments, biometric computation module 320 receives sensor data 322 from sensor(s) 120. Biometric computation module may include separate modules that analyze portions of sensor data 322 in order to provide cognitive load 324 and/or emotion metrics 326 (<u>e.g.</u>, arousal and valence). In some embodiments, modules 314, 316 may analyze the same portions of sensor data. For example, cognitive load computation module 314 and emotion computation module 316 may separately receive image data included in sensor data 322. In such instances, cognitive load computation module 314 may analyze the image data using various pupillometry techniques and/or eye motion data to determine cognitive load 324, while emotion computation module 316 may analyze the image data to determine the facial expression, classify the facial expression as a pre-defined emotion, and acquire emotion metrics 326 corresponding to the pre-defined emotion.</p>
<p id="p0037" num="0037">User readiness computation module 330 applies various algorithms to determine an affective-cognitive load as a function of cognitive load 324 (CL) and the emotion metrics 326 that form an affective load (AL), where ACL = <i>f</i> (CL, AL). In various embodiments, a set of ACLs may be stored in database 142. In some embodiments, user readiness computation module 330 may refer to one or more lookup tables in order to find an entry corresponding to the values for cognitive load 324 and emotion metrics 326 and may determine a corresponding ACL included in the entry. Additionally or alternatively, upon determining the ACL, user readiness computation module 330 may determine a user readiness state 332 applicable to the determined<!-- EPO <DP n="12"> --> ACL. In some embodiments, the lookup table may also include an applicable action that is to be taken for the corresponding ACL. For example, a lookup table entry corresponding to a negative valence value, high arousal, and high cognitive load may indicate a high ACL and may also include commands for the ADAS to override the manual driving actions performed by the user.</p>
<p id="p0038" num="0038">In various embodiments, user readiness computation module 330 may use various algorithmic techniques to compute an ACL value from one or more of cognitive load 324 (CL) and/or emotion metrics 326, including a valence value (V), and an arousal value (A). In such instances, user readiness computation module 330 may compare the ACL value to one or more thresholds in order to determine user readiness state 332. In some embodiments, user readiness computation module 330 may select a specific algorithm to employ in order to emphasize one metric relative to other metrics. For example, user readiness computation module 330 could emphasize cognitive load 324 relative to emotion metrics 326 by applying equation 1 or equation 2 (where the offset value modifies the ACL relative to a specific threshold). In such instances, the squared value of CL causes cognitive load 324 to be the major indicator of ACL.<maths id="math0001" num="(1)"><math display="block"><mrow><mi mathvariant="italic">ACL</mi><mo>=</mo><mfenced separators=""><mfrac><mn>1</mn><mrow><msup><mi mathvariant="italic">CL</mi><mn>2</mn></msup></mrow></mfrac><mo>×</mo><mfrac><mn>1</mn><mi>A</mi></mfrac></mfenced><mo>×</mo><mi>V</mi></mrow></math><img id="ib0001" file="imgb0001.tif" wi="149" he="9" img-content="math" img-format="tif"/></maths><maths id="math0002" num="(2)"><math display="block"><mrow><mi mathvariant="italic">ACL</mi><mo>=</mo><mfenced separators=""><mfrac><mn>1</mn><mrow><msup><mfenced separators=""><mi mathvariant="italic">CL</mi><mo>−</mo><mn>0.5</mn></mfenced><mn>2</mn></msup></mrow></mfrac><mo>×</mo><mfrac><mn>1</mn><mi>A</mi></mfrac></mfenced><mo>×</mo><mi>V</mi></mrow></math><img id="ib0002" file="imgb0002.tif" wi="149" he="9" img-content="math" img-format="tif"/></maths></p>
<p id="p0039" num="0039">In another example, user readiness computation module 330 could apply equation 3 such that the ACL is based on a combination of cognitive load 324 and the arousal value, which is modified by whether the valence value is positive or negative.<maths id="math0003" num="(3)"><math display="block"><mrow><mi mathvariant="italic">ACL</mi><mo>=</mo><mfenced separators=""><mi mathvariant="italic">CL</mi><mo>+</mo><mi>A</mi></mfenced><mo>×</mo><mi>V</mi></mrow></math><img id="ib0003" file="imgb0003.tif" wi="149" he="6" img-content="math" img-format="tif"/></maths></p>
<p id="p0040" num="0040">In some embodiments, user readiness computation module 330 weighs certain values in order to emphasize a specific range of values. For example, user readiness computation module 330 could apply equation 4 or 5 in order to emphasize that positive valence values are more desirable than negative valence values.<maths id="math0004" num="(4)"><math display="block"><mrow><mi mathvariant="italic">ACL</mi><mo>=</mo><mfenced separators=""><mfrac><mn>1</mn><mi mathvariant="italic">CL</mi></mfrac><mo>+</mo><mfrac><mn>1</mn><mi>A</mi></mfrac></mfenced><mo>×</mo><mi>V</mi></mrow></math><img id="ib0004" file="imgb0004.tif" wi="149" he="9" img-content="math" img-format="tif"/></maths><maths id="math0005" num="(5)"><math display="block"><mrow><mfrac><mn>1</mn><mi mathvariant="italic">ACL</mi></mfrac><mo>=</mo><mi mathvariant="italic">CL</mi><mo>+</mo><mi>A</mi><mo>×</mo><mfenced><mfrac><mn>1</mn><mrow><mi>V</mi><mo>+</mo><mn>1</mn></mrow></mfrac></mfenced></mrow></math><img id="ib0005" file="imgb0005.tif" wi="149" he="9" img-content="math" img-format="tif"/></maths><!-- EPO <DP n="13"> --></p>
<p id="p0041" num="0041">In some embodiments, one metric may be excluded. For example, user readiness computation module 330 could apply equation 6 such that the ACL is based on cognitive load 324 and the arousal value while ignoring the valence value.<maths id="math0006" num="(6)"><math display="block"><mrow><mfrac><mn>1</mn><mi mathvariant="italic">ACL</mi></mfrac><mo>=</mo><mi mathvariant="italic">CL</mi><mo>+</mo><mi>A</mi></mrow></math><img id="ib0006" file="imgb0006.tif" wi="149" he="9" img-content="math" img-format="tif"/></maths></p>
<p id="p0042" num="0042">In various embodiments, user readiness computation module 330 may adjust the selected algorithm and/or equation when computing the ACL. In such instances, user readiness computation module 330 may periodically select a different algorithm and/or different equation. For example, when traffic is light, user readiness module 330 can initially select an algorithm that emphasizes cognitive load 324. When affective-cognitive load-based assistant 300 subsequently determines that traffic is heavy, user readiness computation module 330 may then select a different algorithm that weighs valence values more heavily and/or indicates that positive valence values are more desirable than negative valence values.</p>
<p id="p0043" num="0043">In some embodiments, user readiness computation module 330 may incorporate other measurements associated with the user. In some embodiments, user readiness computation module 330 may derive measures focus and engagement by simultaneously analyzing a combination of cognitive and emotional information. For example, user readiness computation module 330 could employ various statistical methods, machine-learning (ML) methods, state machines, and/or various other data structures in order to determine how results of focus and engagement (and/or other user metrics), are derived from cognitive load 324, and/or emotion metrics 326.</p>
<p id="p0044" num="0044">Additionally or alternatively, ML models may be trained from any combination of cognitive load 324, emotion metrics 326, and/or other metrics. The ML models may be trained to predict specific higher-order states of the user, such as a driver attending to a specific driving task. The ML model may then be able to generate values, such as an engagement metric, to output module 340. For example, an ADAS could receive an engagement metric, modeled from affective and cognitive signals, indicating the engagement level of a driver. In such instances, the engagement metric may be used by user readiness application 140 and/or other applications or devices to affect the control of vehicle, such that when the drivers are less engaged, certain components like (<u>e.g.</u>, ADAS functionalities) are more proactive.</p>
<p id="p0045" num="0045">Output module 340 receives user readiness state 332, which represents the ability of the user to make a correct decision at the right time when performing a task. In certain situations,<!-- EPO <DP n="14"> --> such as driving, user readiness state 332 indicates how ready the driver is to drive or act in the correct manner in a driving situation. In various embodiments, output device 340 generates output signals 342 based on user readiness state 332. For example, when user readiness state 332 indicates that the user is capable of handling a task within the environment, output signals 342 may cause one or more devices to execute tasks or provide outputs that maintain the current state of the user (<u>e.g.</u>, maintain playback of an audio track). Conversely, when user readiness state 332 indicates that the user is not capable of handling a task within the environment, output signals 342 may cause one or more devices to execute tasks or provide outputs that assist the user in performing the requisite tasks.</p>
<p id="p0046" num="0046">In some embodiments, user readiness state 332 may indicate an expected state of the user at a future point. For example, in some embodiments, user readiness application 140 could execute various predictive techniques to determine that, based on current input values, a predicted user readiness state. In such instances, user readiness application 140 could determine whether the user will need assistance at the future point. In such instances, user readiness application 140 could generate one or more output signals 342 that modify the one or more components of the vehicle in anticipation of the user's state by the future point, thereby assisting the user in anticipation of the user's expected readiness state.</p>
<heading id="h0009"><b>Determining User Readiness from Affective-Cognitive Load</b></heading>
<p id="p0047" num="0047"><figref idref="f0004">Figure 4A</figref> illustrates example lookup tables of affective-cognitive load values associated with the user readiness application 140 of <figref idref="f0001">Figure 1</figref>, according to various embodiments. As shown, tables 400 include lookup table 410 for positive valence values and lookup table 430 for negative valence values. <figref idref="f0005">Figure 4B</figref> illustrates another example lookup table of affective-cognitive load values associated with the user readiness application of <figref idref="f0001">Figure 1</figref>, according to various embodiments. As shown, tables 440 include lookup table 450 for positive valence values and lookup table 460 for negative valence values. In each lookup table 400, 450, a given entry is coded as either a ready state (<u>e.g.</u>, light grey), some emotional and cognitive load (<u>e.g.</u>, medium grey), significant emotional and cognitive load (<u>e.g.</u>, dark grey), and/or high emotional and cognitive load (<u>e.g.</u>, black).</p>
<p id="p0048" num="0048">Lookup tables 400 represent conditions where the target range of the ACL is low. For example, lookup tables 400 could correspond to conditions where user performance is more effective as the ACL lowers. For example, when the ACL stems mainly from secondary tasks<!-- EPO <DP n="15"> --> (<u>e.g.</u>, texting, singing, etc.), lookup tables 440 represent conditions where the target range of the ACL is medium. For example, lookup tables 440 correspond to conditions where user performance is more effective as within a certain range. For example, when a user is engaging in too little cognitive stimulation and therefore a low ACL (<u>e.g.</u>, bored, tired, etc.), the ACL indicates that driver readiness 332 decreases not only for high ACL, but also for low composite CL. In various embodiments, a range of high ACL values is the least desirable range and requires the most intervention by affective-cognitive load assistant 300.</p>
<p id="p0049" num="0049">In operation, user readiness computation module 330 may use cognitive load 324 and emotion metrics 326 (<u>e.g.</u>, a specific valence value and a specific arousal value) in order to determine a specific entry in table 410 or 430. In such instances, each metric is separated into two or more discrete states. Lookup tables 410, 430 reflect certain heuristics between specific combinations of values, ACL, and user readiness state 332. For example, a combination of high cognitive load 324, a positive emotion (positive valence), and a high amount of emotion (high arousal) is less detrimental to user performance than a combination of high cognitive load 324, a negative emotion (negative valence), and a high amount of emotion. In such instances, the entry for the first combination may indicate a lower ACL and result in a user readiness state 332 within a target range (shown as entry 412), while the second combination may indicate a higher ACL and result in a user readiness state 332 outside the target range (shown as entry 432).</p>
<p id="p0050" num="0050">In some embodiments, lookup tables 410, 430 may be generated from historical data associated with a plurality of users. In such instances, database 142 the most recent lookup tables 410, 430 and may be referred to by user readiness computation module 330. In some embodiments, user readiness application 140 may adjust one or more of lookup tables 410, 430 based on one or more adjustment weights associated with a user. For example, user readiness application 140 could perform various baselining and/or ML techniques using historical ACL values and performance metrics as training data in order to apply weights to ACL determination algorithms. For example, user readiness application 140 could analyze personalized user data and determine that a particular user requires a minimum ACL and/or a negative valence value in order to perform certain driving tasks effectively. In such instances, user readiness application 140 may adjust one or more thresholds and/or one or more entries included in lookup tables 410, 430 and store the personalized adjustments in database 142.</p>
<p id="p0051" num="0051"><figref idref="f0006">Figure 5</figref> illustrates an affective-cognitive load computed from various biometric values derived by the user readiness application 140 of <figref idref="f0001">Figure 1</figref>, according to various<!-- EPO <DP n="16"> --> embodiments. As shown, graph 500 illustrates ACL is a function of cognitive load 324 levels for a given arousal value.</p>
<p id="p0052" num="0052">In various embodiments, user readiness computation module 330 may apply equation 7 or 8 in order to compute ACL as a continuous function along a Weibull distribution.<maths id="math0007" num="(7)"><math display="block"><mrow><mi mathvariant="italic">ACL</mi><mo>=</mo><mi>A</mi><mo>×</mo><msup><mi mathvariant="italic">CL</mi><mi>A</mi></msup><mo>×</mo><msup><mi>e</mi><mrow><mo>−</mo><msup><mi mathvariant="italic">CL</mi><mi>A</mi></msup></mrow></msup></mrow></math><img id="ib0007" file="imgb0007.tif" wi="149" he="7" img-content="math" img-format="tif"/></maths><maths id="math0008" num="(8)"><math display="block"><mrow><mi mathvariant="italic">ACL</mi><mo>=</mo><mi mathvariant="italic">CL</mi><mo>×</mo><msup><mi>A</mi><mi mathvariant="italic">CL</mi></msup><mo>×</mo><msup><mi>e</mi><mrow><mo>−</mo><msup><mi>A</mi><mi mathvariant="italic">CL</mi></msup></mrow></msup></mrow></math><img id="ib0008" file="imgb0008.tif" wi="149" he="7" img-content="math" img-format="tif"/></maths></p>
<p id="p0053" num="0053">Line 510 illustrates a Weibull distribution of ACLs corresponding to a range of cognitive loads and a low arousal value. For low arousal levels, the ACL remains approximately constant for a range of cognitive loads 324 (except for very-low cognitive loads). In such instances, the ACL never rises above a certain upper limit, indicating that the driver readiness state for low arousal values never reaches the maximum value. For example, reaction times for a user could be relatively slower for low arousal values.</p>
<p id="p0054" num="0054">Line 520 illustrates a Weibull distribution of ACLs corresponding to a range of cognitive loads and a medium arousal value. For medium arousal levels, the ACL and associated user readiness state 332 may be higher for a certain range of composite loads 324. For example, above a certain cognitive load 324, the ACL for medium arousal values is higher than ACL values for lower arousal levels, indicating that the active mind of the user enables the user to handle more-demanding tasks. Once cognitive load 324 reaches an upper threshold, the cognitive ACL and user readiness state 332 begin to decrease (<u>e.g.</u>, secondary tasks distracting the user).</p>
<p id="p0055" num="0055">Line 530 illustrates a Weibull distribution of ACLs corresponding to a range of cognitive loads and a high arousal value. When in a high emotional state, the range of best-possible performance (<u>e.g.</u>, highest ACL values) is narrower than with lower arousal levels, as seen by lines 510, 520. Further the maximum ACL and associated user readiness state 332 are significantly lower than the maximum ACL associated with lower arousal levels.</p>
<p id="p0056" num="0056"><figref idref="f0007">Figure 6</figref> illustrates an example vehicle system 600 that includes an affective-cognitive load digital assistant 300 of <figref idref="f0003">Figure 3</figref>, according to various embodiments. As shown, vehicle system 600 includes sensing module 620, head unit 230, network 150, and output module 340. Sensing module 620 includes driver-facing sensors 622 (<u>e.g.</u>, a camera),<!-- EPO <DP n="17"> --> compartment non-driver-facing sensors 624 (<u>e.g.</u>, steering wheel sensors, pedal sensors, etc.), and vehicle sensors 626 (<u>e.g.</u>, speedometer, accelerometer, etc.). Head unit 230 includes entertainment subsystem 612, navigation subsystem 614, network module 616, and advanced driver assistance system (ADAS) 618. Output module 340 includes ADAS notifications 642, ADAS parameters 644, human-machine interface (HMI) 646, vehicle behaviors 648, application parameters 652, and application events 654.</p>
<p id="p0057" num="0057">In various embodiments, user readiness application 140 may be included in ADAS 618 and may receive sensor data from one or more sensors 622, 624, 626 included in sensing module 620. In various embodiments, user readiness application 140 may further receive data from navigation subsystem 614 and/or network module 616. User readiness application 140 analyzes the received data to compute an ACL and determine a user readiness state 332 associated with the cognitive load and emotional state of the driver.</p>
<p id="p0058" num="0058">In various embodiments, ADAS 618 may send the user readiness state 332 to output module 340 that generates one or more output signals. In alternative embodiments, ADAS 618 may generate output signals based on the user readiness state 332. In various embodiments, the output signal may include one or more of ADAS notification 642, ADAS parameter 644, vehicle behavior 648, application parameter 652, and/or application event 654.</p>
<p id="p0059" num="0059">Sensing module 620 includes multiple types of sensors, including driver-facing sensors 622 (<u>e.g.</u>, cameras, motion sensors, etc.), compartment non-driver facing sensors 624 (<u>e.g.</u>, motion sensors, pressure sensors, temperature sensors, etc.), and vehicle sensors (<u>e.g.</u>, outward-facing cameras, accelerometers, etc.). In various embodiments, sensing module 620 provides a combination of sensor data that describes the context in which combined affective-cognitive load are being observed in more detail. For example, sensing module 620 could provide a set of values associated with the operation of the vehicle (<u>e.g.</u>, angular velocity of rear tires, velocity of the pedal movement, velocity of the vehicle, etc.). In such instances, user readiness application 140 could determine a cognitive load value and/or an emotional load value based on the received values, such as by comparing the measured velocity of the vehicle compared to the speed limit of the location, and/or the velocity of surrounding vehicles.</p>
<p id="p0060" num="0060">In various embodiments, vehicle sensors 626 may further include other external sensors. Such external sensors may include optical sensors, acoustic sensors, road vibration sensors, temperature sensors, etc. In some embodiments, sensing module and/or network module<!-- EPO <DP n="18"> --> 616 may acquire other external data, such as geolocation data (<u>e.g.</u>, GNNS systems, including a global positioning system (GPS), Glonass, Galileo, etc.). In some embodiments, navigation data and/or geolocation data may be combined to predict changes to the ACL based on expected driving conditions. For example, an expected traffic jam may cause user readiness application 140 to predict an increase in the ACL upon the vehicle reaching affected area.</p>
<p id="p0061" num="0061">Network module 616 translates results of sensor module 620. In various embodiments, network module 616 may retrieve specific values, such as sensing data 662, connected vehicle data 664, and/or historical data (<u>e.g.</u>, previous ACLs, calculations that were computed by remote devices, etc.). For example, user readiness computation module 330 could compare the current ACL with previous performances before mapping the ACL to a driver readiness value. In some embodiments, the driver readiness 332 may include a notification indicating whether the driver has improved, remained more focused, engaged, etc., compared to past performances.</p>
<p id="p0062" num="0062">In some embodiments, network module 616 may transmit data acquired by head unit, such as one or more ACLs, user readiness state 332, and/or sensing data 662 acquired by sensor module 620. In such instances, one or more devices connected to network 150 may merge data received from network module 616 with data from other vehicles, and/or infrastructure before being consumed by computation modules. For example, one or more devices may accumulate and compile sensing data in order to associate driving conditions with required driver readiness.</p>
<p id="p0063" num="0063">For example, one or more devices could accumulate multiple ACL computations into an aggregate measure of the focus or engagement of a group. For example, a smart home system that includes affective-cognitive load based assistant 300 may compute ACLs for each member in a room and may generate an aggregate user readiness state corresponding to the multiple members in the rooms before determining the brightness of lighting of a given room. In another example, affective-cognitive load-based assistant 300 could emphasize certain members (e.g., emphasize the ACLs of a group of guests).</p>
<p id="p0064" num="0064">Output module 340 performs one or more actions in response to a user readiness state 332 provided by ADAS 618. For example, output module 340 may generate one or more output signals 341 in response to user readiness state 332 that modifies an application and/or interface. For example, output module 340 could generate one or more output signals 341 to modify HMI 646 to display notification messages and/or alerts. In another example, output module 340 could<!-- EPO <DP n="19"> --> generate one or more output signals to modify an application. In such instances, the output signal 341 may include application parameters 652 and/or application events 654.</p>
<p id="p0065" num="0065">In various embodiments, ADAS notifications 642 may include light indications, such as ambient lights and mood lights, audio notifications, voice notifications (<u>e.g.</u>, a voice assistant), visual notification messages, haptic notifications in the vehicle (<u>e.g.</u>, steering wheel, seat, head rest, etc.) or wearable device or touchless haptic notifications, etc.</p>
<p id="p0066" num="0066">In various embodiments, ADAS parameters 644 may include various operating parameters, settings, or actions. For example, ADAS parameters 644 could include vehicle climate control settings (<u>e.g.</u>, window controls, passenger compartment temperature, increasing fan speed, etc.), and/or olfactory parameters, such as emitting specific fragrances that are calming or stimulating. In various embodiments, ADAS parameters 644 may include emergency calling parameters, such as triggering the dialing of one or more emergency phone numbers or suggesting that the user connect to a specific contact situations that may require immediate assistance and/or response.</p>
<p id="p0067" num="0067">In various embodiments, ADAS parameters 644 may dynamically activate L2+/L3+ capabilities, such as lane assist, collision avoidance, and/or autonomous driving. In some embodiments, ADAS parameter 644 may be a binary activation signal (on/off); alternatively, ADAS parameters 644 may be activation signal that provides a more-gradual activation (<u>e.g.</u>, with varying degrees of automated correction when the driver seems to deviate from their lane). In some embodiments, ADAS parameters 644 may dynamically activate the collision avoidance systems. For example, output module 340 may dynamically generate ADAS parameters 644 that adapt the parameters of the system (<u>e.g.</u>, warning time, brake intensity, etc.) depending on user readiness state 332.</p>
<p id="p0068" num="0068">In some embodiments, other systems may generate responses based on user readiness state 332. For example, navigation subsystem 614 could generate specific route suggestions based on the user readiness state, such as avoiding routes that require significant focus or attention. Additionally or alternatively, entertainment subsystem 612 may play specific tracks associated with specific moods in order to maintain user readiness state 332, or to alter user readiness state 332.</p>
<p id="p0069" num="0069"><figref idref="f0008">Figure 7</figref> is a flow diagram of method steps for generating an output signal based on an affective-cognitive load, according to various embodiments. Although the method steps are<!-- EPO <DP n="20"> --> described with respect to the systems of <figref idref="f0001 f0002 f0003 f0004 f0005 f0006 f0007">Figures 1-6</figref>, persons skilled in the art will understand that any system configured to perform the method steps, in any order, falls within the scope of the various embodiments.</p>
<p id="p0070" num="0070">As shown, method 700 begins at step 701, where user readiness application 140 acquires sensor data. In various embodiments, various sensor(s) 120 acquire sensor data related to the brain activity and/or emotional state of a user. For example, sensor 120 may include a camera that acquires sensor data focused on the face of the user.</p>
<p id="p0071" num="0071">At step 703, user readiness application 140 computes a cognitive load from the sensor data. In various embodiments, user readiness application 140 may analyze portions of the acquired sensor data in order to estimate the cognitive load currently being experienced by a user. For example, user readiness application 140 may perform various pupillometry techniques on received image data in order to determine fluctuations in the pupil of the user. User readiness application 140 may then compute the cognitive load from the pupil data. Additionally, or alternatively, user readiness application 140 may perform various eye motion analyses on received image data in order to determine eye saccades, eye fixations, and the like, from the eyes of the user. User readiness application 140 may then compute the cognitive load from the eye saccades, eye fixations, and the like.</p>
<p id="p0072" num="0072">At step 705, user readiness application 140 computes one or more emotion metrics from the sensor data. In various embodiments, it may analyze portions of the acquired sensor data in order to estimate emotion parameterized metrics currently being experienced by a user. For example, user readiness application 140 may perform various facial expression estimation techniques on received image data in order to determine the emotion being experienced by the user. In another example, additionally or alternatively to facial expression estimation, user readiness application 140 may perform various voice tone analyses on received audio data in order to determine the emotion being experienced by the user. User readiness application 140 may map the estimated emotion to specific arousal and valence values.</p>
<p id="p0073" num="0073">At step 707, user readiness application 140 determines an affective-cognitive load (ACL) as a composite of the cognitive load and emotion metrics. In various embodiments, user readiness application 140 may execute one or more algorithmic techniques in order to compute an affective-cognitive load (ACL) of the user. For example, user readiness application 140 may compute the ACL as a function of the inverses of the cognitive load and the arousal, multiplied<!-- EPO <DP n="21"> --> by the valence (<u>e.g.</u>, equation 4). Such an algorithm emphasizes positive emotional valence values for a given set of cognitive load and arousal values. In another example, user readiness application 140 could refer to one or more lookup tables to find an entry that specifies a specific ACL for a specific combination of cognitive load, arousal, and valence.</p>
<p id="p0074" num="0074">At step 709, user readiness application 140 compares the ACL to one or more threshold ACL values in order to determine a user readiness state. In various embodiments, user readiness application 140 may compare the ACL value to one or more thresholds that separate user readiness states. For example, user readiness application 140 may compare ACL to a minimum threshold and a maximum threshold in order to determine that ACL is within a target ACL threshold range associated with a particular user readiness state. In such instances, user readiness application 140 may determine that heuristics indicate that the target ACL range corresponds to a medium level of user readiness, indicating that the user is engaged enough to respond to stimuli without being overwhelmed. In other embodiments, the minimum threshold separate a medium ACL range from a lower ACL range that corresponds to the target ACL range. In such instances, a medium ACL value could correspond to a user readiness state outside the target range, indicating that some assistance to the user may be necessary to perform the required tasks.</p>
<p id="p0075" num="0075">At step 711, user readiness application 140 may optionally cause output device 340 to generate an output signal based on the user readiness state. In various embodiments, user readiness application 140 may send to output module 340 the user readiness state. In such instances, receiving the user readiness state causes output module 340 to generate one or more output signals. In various embodiments, the output signal may cause one or more devices, such as an assisted driving system, display device, and/or feedback system, to adjust one or more parameters associated with handling a task. For example, upon receiving a user readiness state indicating that intervention is required, output device 340 could generate one or more output signals to assist the user in handling a task and/or lowering the ACL such that the user will soon be able to successfully handle the task.</p>
<p id="p0076" num="0076">In sum, an affective-cognitive based load digital assistant receives sensor data and determines the readiness of a user to handle a task within an environment. Various sensors acquire sensor data associated with the user or the environment and sends the sensor data to a user readiness application included in the affective-cognitive load based digital assistant. The user readiness application computes from the sensor data various biometric values associated<!-- EPO <DP n="22"> --> with the psychophysiological state of a user. Such biometric values include a cognitive load that estimates the amount of brain activity that a user is employing, and an affective load that estimates an emotional load that a user is experiencing. In various embodiments, the affective load may include one or more separate emotional metrics, such as separate arousal and/or valence values. Upon determining separate biometric values for a user, the user readiness application analyzes the cognitive load and affective load using one or more algorithmic techniques to determine an affective-cognitive load (ACL), which indicates the user's overall mental activity. The user readiness application then determines a user readiness state, which indicates the ability of the user to manage a set of tasks. The user readiness state determined by the user readiness application causes output devices and/or other applications to assist the user at a degree that corresponds to the determined user readiness state.</p>
<p id="p0077" num="0077">At least one technological advantage of the affective-cognitive load based digital assistant is that computing a user readiness state as a composite of direct measurements that estimate the cognitive load and the emotional state of the user provides a more accurate indication of the ability of the user to handle tasks. In particular, by combining emotion recognition and cognitive load, the affective-cognitive load based digital assistant can describe both the quality of the cognition (the operations being done by the mind, measured as cognitive load), as well as the emotion. Further, the mental state estimated by the affective-cognitive load digital assistant is associated with autonomic body systems making the estimations less susceptible to error. As a result, systems that assist the user in conducting certain tasks can more accurately assist the user based on the estimated brain activity and emotional state of the user.
<ol id="ol0001" ol-style="">
<li>1. In various embodiments, a computer-implemented method comprises receiving, from at least one sensor, sensor data associated with an environment, computing, based on the sensor data, a cognitive load associated with a user within the environment, computing, based on the sensor data, an affective load associated with an emotional state of the user, determining, based on both the cognitive load at the affective load, an affective-cognitive load, determining, based on the affective-cognitive load, a user readiness state associated with the user, and causing one or more actions to occur based on the user readiness state.</li>
<li>2. The computer-implemented method of clause 1, where the affective load comprises an arousal value and a valence value, and determining the affective-cognitive load comprises searching, based on the cognitive load, the arousal value, and the valence value, one or more lookup tables that include entries specifying affective-cognitive loads, and identifying<!-- EPO <DP n="23"> --> an entry included in the one or more lookup tables, wherein the entry corresponds to a set of the cognitive load, the arousal value, and the valence value.</li>
<li>3. The computer-implemented method of clause 1 or 2, where the sensor data comprises image data.</li>
<li>4. The computer-implemented method of any of clauses 1-3, where the affective load comprises an arousal value and a valence value, and determining the affective-cognitive load comprises applying an algorithmic technique to compute the affective-cognitive load as a function of the cognitive load and at least one of the arousal value or the valence value.</li>
<li>5. The computer-implemented method of any of clauses 1-4, where the algorithmic technique includes applying a Weibull distribution function based on the cognitive load and the arousal value.</li>
<li>6. The computer-implemented method of any of clauses 1-5, where computing the cognitive load comprises determining one or more eye parameters from image data included in the sensor data, and computing the cognitive load from the one or more eye parameters, and computing the affective load comprises determining a pre-defined emotion from the sensor data, and identifying an arousal value corresponding to the pre-defined emotion, and identifying a valence value corresponding to the pre-defined emotion, where the arousal value and the valence value are included in the affective load.</li>
<li>7. The computer-implemented method of any of clauses 1-6, where causing the one or more actions to occur based on the user readiness state comprises causing an output device to generate an output signal based on the user readiness state.</li>
<li>8. The computer-implemented method of any of clauses 1-7, where the output signal causes the output device to change at least one operating parameter.</li>
<li>9. The computer-implemented method of any of clauses 1-8, where the output signal causes at least one notification message to be emitted from the output device.</li>
<li>10. The computer-implemented method of any of clauses 1-9, where the output signal causes one or more lights to change brightness.</li>
<li>11. In various embodiments, one or more non-transitory computer-readable media store instructions that, when executed by one or more processors, cause the one or more<!-- EPO <DP n="24"> --> processors to perform the steps of receiving, from at least one sensor, sensor data associated with an environment, computing, based on the sensor data, a cognitive load associated with a user within the environment, computing, based on the sensor data, an affective load associated with an emotional state of the user, applying an algorithmic technique to compute an affective-cognitive load as a function of the cognitive load and the affective load, determining, based on the affective-cognitive load, a user readiness state associated with the user, and causing one or more actions to occur based on the user readiness state.</li>
<li>12. The one or more non-transitory computer-readable media of clause 11, wherein the sensor data comprises image data.</li>
<li>13. The one or more non-transitory computer-readable media of clause 11 or 12, where computing the cognitive load comprises determining one or more eye parameters from image data included in the sensor data, and computing the cognitive load from the one or more eye parameters, and computing the affective load comprises determining a pre-defined emotion from the sensor data, and identifying an arousal value corresponding to the pre-defined emotion, and identifying a valence value corresponding to the pre-defined emotion, where the arousal value and the valence value are included in the affective load.</li>
<li>14. The one or more non-transitory computer-readable media of any of clauses 11-13, where the sensor data comprises biometric data including at least one of a pupil size, a heart rate, a galvanic skin response, or a blood oxygenation level.</li>
<li>15. The one or more non-transitory computer-readable media of any of clauses 11-14, further including instructions that, when executed by the one or more processors, cause the one or more processors to perform the steps of computing, based on the sensor data, a second cognitive load associated with a second user within the environment, computing, based on the sensor data, a second affective load associated with a second emotional state of the second user, determining, based on both the second cognitive load at the affective load, a second affective-cognitive load, combining the affective-cognitive load and the second affective-cognitive load to generate an aggregate affective-cognitive load, determining, based on the aggregate affective-cognitive load, an aggregate user readiness state associated with both the user and the second user, and causing one or more actions to occur based on the aggregate user readiness state.</li>
<li>16. In various embodiments, an affective-cognitive load based device comprises at least one sensor configured to acquire sensor data, a memory storing a user readiness<!-- EPO <DP n="25"> --> application, and a processor that is coupled at least to the memory and, when executing the user readiness application, is configured to receive, from the at least one sensor, sensor data associated with an environment, compute, based on the sensor data, a cognitive load associated with a user within the environment, compute, based on the sensor data, at least one of an arousal value or a valence value associated with an emotional state of the user, determine, based on the cognitive load and at least one of the arousal value or the valence value, an affective-cognitive load, determine, based on the affective-cognitive load, a user readiness state associated with the user, and cause one or more actions to occur based on the user readiness state.</li>
<li>17. The affective-cognitive load-based device of clause 16, where the sensor data comprises image data.</li>
<li>18. The affective-cognitive load-based device of clause 16 or 17, where determining the affective-cognitive load comprises applying an algorithmic technique to compute the affective-cognitive load as a function of the cognitive load and at least one of the arousal value or the valence value.</li>
<li>19. The affective-cognitive load-based device of any of clauses 16-18, where the affective-cognitive load-based device is included in an advanced driver assistance system (ADAS) of a vehicle.</li>
<li>20. The affective-cognitive load-based device of any of clauses 16-19, wherein the processor configured to cause one or more actions to occur comprises generating an output signal that causes the ADAS to change at least one ADAS parameter.</li>
</ol></p>
<p id="p0078" num="0078">Any and all combinations of any of the claim elements recited in any of the claims and/or any elements described in this application, in any fashion, fall within the contemplated scope of the present invention and protection.</p>
<p id="p0079" num="0079">The descriptions of the various embodiments have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.</p>
<p id="p0080" num="0080">Aspects of the present embodiments may be embodied as a system, method or computer program product. Accordingly, aspects of the present disclosure may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware,<!-- EPO <DP n="26"> --> resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a "module," a "system," or a "computer." In addition, any hardware and/or software technique, process, function, component, engine, module, or system described in the present disclosure may be implemented as a circuit or set of circuits. Furthermore, aspects of the present disclosure may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.</p>
<p id="p0081" num="0081">Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable storage medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system, apparatus, or device.</p>
<p id="p0082" num="0082">Aspects of the present disclosure are described above with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the disclosure. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general-purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine. The instructions, when executed via the processor of the computer or other programmable data processing apparatus, enable the implementation of the functions/acts specified in the flowchart and/or block diagram block or blocks. Such processors may be, without limitation, general purpose processors, special-purpose processors, application-specific processors, or field-programmable gate arrays.<!-- EPO <DP n="27"> --></p>
<p id="p0083" num="0083">The flowchart and block diagrams in the figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present disclosure. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.</p>
<p id="p0084" num="0084">While the preceding is directed to embodiments of the present disclosure, other and further embodiments of the disclosure may be devised without departing from the basic scope thereof, and the scope thereof is determined by the claims that follow.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="28"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A computer-implemented method comprising:
<claim-text>receiving, from at least one sensor, sensor data associated with an environment;</claim-text>
<claim-text>computing, based on the sensor data, a cognitive load associated with a user within the environment;</claim-text>
<claim-text>computing, based on the sensor data, an affective load associated with an emotional state of the user;</claim-text>
<claim-text>determining, based on both the cognitive load at the affective load, an affective-cognitive load;</claim-text>
<claim-text>determining, based on the affective-cognitive load, a user readiness state associated with the user; and</claim-text>
<claim-text>causing one or more actions to occur based on the user readiness state.</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The computer-implemented method of claim 1, wherein:
<claim-text>the affective load comprises an arousal value and a valence value; and</claim-text>
<claim-text>determining the affective-cognitive load comprises:
<claim-text>searching, based on the cognitive load, the arousal value, and the valence value, one or more lookup tables that include entries specifying affective-cognitive loads, and</claim-text>
<claim-text>identifying an entry included in the one or more lookup tables, wherein the entry corresponds to a set of the cognitive load, the arousal value, and the valence value.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The computer-implemented method of claim 1 or 2, wherein the sensor data comprises image data.</claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>The computer-implemented method of any preceding claim, wherein:
<claim-text>the affective load comprises an arousal value and a valence value; and</claim-text>
<claim-text>determining the affective-cognitive load comprises applying an algorithmic technique to compute the affective-cognitive load as a function of the cognitive load and at least one of the arousal value or the valence value.</claim-text></claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>The computer-implemented method of claim 4, wherein the algorithmic technique includes applying a Weibull distribution function based on the cognitive load and the arousal value.<!-- EPO <DP n="29"> --></claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>The computer-implemented method of any preceding claim, wherein:
<claim-text>computing the cognitive load comprises:
<claim-text>determining one or more eye parameters from image data included in the sensor data, and</claim-text>
<claim-text>computing the cognitive load from the one or more eye parameters; and</claim-text></claim-text>
<claim-text>computing the affective load comprises:
<claim-text>determining a pre-defined emotion from the sensor data, and</claim-text>
<claim-text>identifying an arousal value corresponding to the pre-defined emotion, and</claim-text>
<claim-text>identifying a valence value corresponding to the pre-defined emotion,</claim-text>
<claim-text>wherein the arousal value and the valence value are included in the affective load.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>The computer-implemented method of any preceding claim, wherein causing the one or more actions to occur based on the user readiness state comprises causing an output device to generate an output signal based on the user readiness state.</claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>The computer-implemented method of claim 7, wherein the output signal causes one or more of the output device to change at least one operating parameter, at least one notification message to be emitted from the output device, or one or more lights to change brightness.</claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>The computer-implemented method of any preceding claim, wherein the sensor data comprises biometric data including at least one of a pupil size, a heart rate, a galvanic skin response, or a blood oxygenation level.</claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>The computer-implemented method of any preceding claim, further comprising:
<claim-text>computing, based on the sensor data, a second cognitive load associated with a second user within the environment;</claim-text>
<claim-text>computing, based on the sensor data, a second affective load associated with a second emotional state of the second user;</claim-text>
<claim-text>determining, based on both the second cognitive load at the affective load, a second affective-cognitive load;</claim-text>
<claim-text>combining the affective-cognitive load and the second affective-cognitive load to generate an aggregate affective-cognitive load;</claim-text>
<claim-text>determining, based on the aggregate affective-cognitive load, an aggregate user readiness state associated with both the user and the second user; and</claim-text>
<claim-text>causing one or more actions to occur based on the aggregate user readiness state.</claim-text><!-- EPO <DP n="30"> --></claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>One or more non-transitory computer-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to perform the method of any one of claim 1-10.</claim-text></claim>
<claim id="c-en-0012" num="0012">
<claim-text>An affective-cognitive load-based device, comprising:
<claim-text>at least one sensor configured to acquire sensor data;</claim-text>
<claim-text>a memory storing a user readiness application; and</claim-text>
<claim-text>a processor that is coupled at least to the memory and, when executing the user readiness application, is configured to:
<claim-text>receive, from the at least one sensor, sensor data associated with an environment;</claim-text>
<claim-text>compute, based on the sensor data, a cognitive load associated with a user within the environment;</claim-text>
<claim-text>compute, based on the sensor data, at least one of an arousal value or a valence value associated with an emotional state of the user;</claim-text>
<claim-text>determine, based on the cognitive load and at least one of the arousal value or the valence value, an affective-cognitive load;</claim-text>
<claim-text>determine, based on the affective-cognitive load, a user readiness state associated with the user; and</claim-text>
<claim-text>cause one or more actions to occur based on the user readiness state.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0013" num="0013">
<claim-text>The affective-cognitive load-based device of claim 12, wherein determining the affective-cognitive load comprises applying an algorithmic technique to compute the affective-cognitive load as a function of the cognitive load and at least one of the arousal value or the valence value.</claim-text></claim>
<claim id="c-en-0014" num="0014">
<claim-text>The affective-cognitive load-based device of claim 12 or 13, wherein the affective-cognitive load-based device is included in an advanced driver assistance system (ADAS) of a vehicle.</claim-text></claim>
<claim id="c-en-0015" num="0015">
<claim-text>The affective-cognitive load-based device of claim 14, wherein the processor configured to cause one or more actions to occur comprises generating an output signal that causes the ADAS to change at least one ADAS parameter.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="31"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="108" he="215" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="32"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="165" he="220" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> -->
<figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="165" he="180" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> -->
<figure id="f0004" num="4A"><img id="if0004" file="imgf0004.tif" wi="165" he="213" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> -->
<figure id="f0005" num="4B"><img id="if0005" file="imgf0005.tif" wi="165" he="212" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> -->
<figure id="f0006" num="5"><img id="if0006" file="imgf0006.tif" wi="162" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> -->
<figure id="f0007" num="6"><img id="if0007" file="imgf0007.tif" wi="165" he="232" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> -->
<figure id="f0008" num="7"><img id="if0008" file="imgf0008.tif" wi="120" he="233" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="157" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="155" he="233" type="tif"/></search-report-data><search-report-data date-produced="20210810" id="srepxml" lang="en" srep-office="EP" srep-type="ep-sr" status="n"><!--
 The search report data in XML is provided for the users' convenience only. It might differ from the search report of the PDF document, which contains the officially published data. The EPO disclaims any liability for incorrect or incomplete data in the XML for search reports.
 -->

<srep-info><file-reference-id>J110872EP</file-reference-id><application-reference><document-id><country>EP</country><doc-number>21162299.8</doc-number></document-id></application-reference><applicant-name><name>Harman International Industries, Incorporated</name></applicant-name><srep-established srep-established="yes"/><srep-invention-title title-approval="yes"/><srep-abstract abs-approval="yes"/><srep-figure-to-publish figinfo="by-applicant"><figure-to-publish><fig-number>3</fig-number></figure-to-publish></srep-figure-to-publish><srep-info-admin><srep-office><addressbook><text>MN</text></addressbook></srep-office><date-search-report-mailed><date>20210819</date></date-search-report-mailed></srep-info-admin></srep-info><srep-for-pub><srep-fields-searched><minimum-documentation><classifications-ipcr><classification-ipcr><text>G06F</text></classification-ipcr><classification-ipcr><text>B60W</text></classification-ipcr><classification-ipcr><text>A61B</text></classification-ipcr><classification-ipcr><text>G06K</text></classification-ipcr></classifications-ipcr></minimum-documentation></srep-fields-searched><srep-citations><citation id="sr-cit0001"><patcit dnum="US2019239795A1" id="sr-pcit0001" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2019239795&amp;CY=ep"><document-id><country>US</country><doc-number>2019239795</doc-number><kind>A1</kind><name>KOTAKE YASUYO [JP] ET AL</name><date>20190808</date></document-id></patcit><category>X</category><rel-claims>1-15</rel-claims><rel-passage><passage>* abstract; figures 1-16C *</passage><passage>* paragraphs [0106] - [0116],  [0127] - [0129],  [0134] - [0140],  [0151],  [0206] - [0208],  [0220] - [0222] *</passage></rel-passage></citation><citation id="sr-cit0002"><patcit dnum="US2019197330A1" id="sr-pcit0002" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2019197330&amp;CY=ep"><document-id><country>US</country><doc-number>2019197330</doc-number><kind>A1</kind><name>MAHMOUD ABDELRAHMAN N [US] ET AL</name><date>20190627</date></document-id></patcit><category>A</category><rel-claims>1-15</rel-claims><rel-passage><passage>* abstract; figures 1-17 *</passage><passage>* paragraphs [0045] - [0048] *</passage></rel-passage></citation></srep-citations><srep-admin><examiners><primary-examiner><name>Köhn, Andreas</name></primary-examiner></examiners><srep-office><addressbook><text>Munich</text></addressbook></srep-office><date-search-completed><date>20210810</date></date-search-completed></srep-admin><!--							The annex lists the patent family members relating to the patent documents cited in the above mentioned European search report.							The members are as contained in the European Patent Office EDP file on							The European Patent Office is in no way liable for these particulars which are merely given for the purpose of information.							For more details about this annex : see Official Journal of the European Patent Office, No 12/82						--><srep-patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2019239795</doc-number><kind>A1</kind><date>20190808</date></document-id></priority-application><family-member><document-id><country>CN</country><doc-number>109890289</doc-number><kind>A</kind><date>20190614</date></document-id></family-member><family-member><document-id><country>EP</country><doc-number>3562398</doc-number><kind>A2</kind><date>20191106</date></document-id></family-member><family-member><document-id><country>JP</country><doc-number>2018102617</doc-number><kind>A</kind><date>20180705</date></document-id></family-member><family-member><document-id><country>US</country><doc-number>2019239795</doc-number><kind>A1</kind><date>20190808</date></document-id></family-member><family-member><document-id><country>WO</country><doc-number>2018122633</doc-number><kind>A1</kind><date>20180705</date></document-id></family-member><family-member><document-id><country>WO</country><doc-number>2018122729</doc-number><kind>A2</kind><date>20180705</date></document-id></family-member></patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2019197330</doc-number><kind>A1</kind><date>20190627</date></document-id></priority-application><text>NONE</text></patent-family></srep-patent-family></srep-for-pub></search-report-data>
</ep-patent-document>
