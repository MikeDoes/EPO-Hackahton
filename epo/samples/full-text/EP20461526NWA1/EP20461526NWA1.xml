<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP20461526A1" file="EP20461526NWA1.xml" lang="en" country="EP" doc-number="3889738" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3889738</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>20461526.4</B210><B220><date>20200404</date></B220><B250>en</B250><B251EP>en</B251EP><B260>en</B260></B200><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>G06F   3/01        20060101AFI20200914BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>A61B   5/00        20060101ALI20200914BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>A61B   5/486       20130101 LI20200904BHEP        </text></classification-cpc><classification-cpc sequence="2"><text>G06F   3/011       20130101 LI20200903BHEP        </text></classification-cpc><classification-cpc sequence="3"><text>G06F   3/017       20130101 FI20200903BHEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>SYSTEM UND VERFAHREN ZUM KALIBRIEREN EINER BENUTZERSCHNITTSTELLE</B542><B541>en</B541><B542>A SYSTEM AND A METHOD FOR CALIBRATING A USER INTERFACE</B542><B541>fr</B541><B542>SYSTÈME ET PROCÉDÉ D'ÉTALONNAGE D'UNE INTERFACE UTILISATEUR</B542></B540><B590><B598>1</B598></B590></B500><B700><B710><B711><snm>Neuroforma Sp. z o.o.</snm><iid>101856092</iid><irf>191146</irf><adr><str>Zurawia 71</str><city>15-540 Bialystok</city><ctry>PL</ctry></adr></B711></B710><B720><B721><snm>Pawlisz, Maciej</snm><adr><str>Modrzewiowa 6</str><city>84-207 Bojano</city><ctry>PL</ctry></adr></B721></B720><B740><B741><snm>Kancelaria Eupatent.pl Sp. z.o.o</snm><iid>101564934</iid><adr><str>Ul. Kilinskiego 185</str><city>90-348 Lodz</city><ctry>PL</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">A method for calibrating a user interface, the method comprising the steps of: selecting (201) a type of exercise to be performed by a user; accessing (202) a movements database (107) to access definitions of exercises wherein each exercise comprises related information on involved body parts of a user, which shall be detected and tracked as well as a range of motion; determining (203), based on a selected exercise definition, where at least one body part of the user shall be positioned; generating instructions (205) for the user to move and position according to selected exercise requirements; awaiting assuming by the user the position according to exercise requirements by analysing video signal captured by the camera (103).
<img id="iaf01" file="imgaf001.tif" wi="108" he="112" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001">TECHNICAL FIELD</heading>
<p id="p0001" num="0001">The present invention relates to a system and a method for calibrating a user interface. The invention is usable in particular at optimizing a camera view for gaming or physical exercise systems.</p>
<heading id="h0002">BACKGROUND OF THE INVENTION</heading>
<p id="p0002" num="0002">Virtual reality or augmented reality gaming or physical exercise systems are becoming more and more popular. In such systems, a user is instructed to perform certain movements, a camera captures the user's movements and a computer system tracks and analyses these movements.</p>
<p id="p0003" num="0003">For example, a US patent application <patcit id="pcit0001" dnum="US2018315247A"><text>US2018315247</text></patcit> discloses a virtual or augmented reality rehabilitation which includes comparing reference range of motion capabilities of a body part that mirrors or matches the target body part (e.g., from a camera or a database) to captured range of motion capabilities of the target body part. The comparison may result in an assessment, a virtual animation, a virtual component, or other output, such as to be displayed on a display or an augmented reality display.</p>
<p id="p0004" num="0004">A<patcit id="pcit0002" dnum="WO2015103359A"><text> PCT application WO2015103359</text></patcit> discloses a system and a method that implement video capture technology, in combination with computer program engines and proprietary algorithms to capture, analyze and objectively score human movement, and to identify, differentially diagnose and assess the root causes of observed pathokinematic (pathological movement) patterns. In this system, the user is relatively far from the camera so that the camera may capture the complete posture as well as the surrounding mat and area.</p>
<heading id="h0003">SUMMARY AND OBJECTS OF THE PRESENT INVENTION</heading>
<p id="p0005" num="0005">There is a need to optimize a camera view in a system that tracks the user's movements to calibrate the user interface accordingly to the type of movement that is going to be performed, in order to allow tracking of movement with optimal precision.<!-- EPO <DP n="2"> --></p>
<p id="p0006" num="0006">In the present invention, the user interface is calibrated such that the camera view is optimized and adjusted to the range of movement that is performed by the user. Therefore, movements of smaller objects in a small range (such as fingers of static hand) can be viewed closer than movements of larger objects (such as a dancing person).</p>
<p id="p0007" num="0007">The invention relates to a method for calibrating a user interface, the method comprising the steps of: selecting a type of exercise to be performed by a user; accessing a movements database to access definitions of exercises wherein each exercise comprises related information on involved body parts of a user, which shall be detected and tracked as well as a range of motion; determining, based on a selected exercise definition, where at least one body part of the user shall be positioned; generating instructions for the user to move and position according to selected exercise requirements; awaiting assuming by the user the position according to exercise requirements by analysing video signal captured by the camera.</p>
<p id="p0008" num="0008">Preferably, said selection step is executed by generating and outputting a graphical user interface and receiving from the user a selection of an exercise.</p>
<p id="p0009" num="0009">Preferably, the method further comprises a step of generating the selected exercise using augmented reality such that the augmented reality objects are reachable by said user.</p>
<p id="p0010" num="0010">Preferably, prior to said step of generating instructions the method is configured to execute a step of generating in the output video signal, a representation of user's body parts outline, superimposed on images captured by a camera, where the user is expected to be positioned.</p>
<p id="p0011" num="0011">Preferably, said head shall have a predefined position as well as size in an image captured by the camera.</p>
<p id="p0012" num="0012">Preferably, said size refers to a percentage of the image area.</p>
<p id="p0013" num="0013">Preferably, said outline where the user is expected to be positioned is an reference position.</p>
<p id="p0014" num="0014">Preferably, the current position of a user is acceptable as long as it differs from said reference position within a given threshold, wherein said method executes a step of manipulating said image in order to obtain said reference<!-- EPO <DP n="3"> --> position in the image.</p>
<p id="p0015" num="0015">Preferably, said instructions are visual instructions embedded in output video signal.</p>
<p id="p0016" num="0016">Preferably, said instructions are audible instructions.</p>
<p id="p0017" num="0017">Preferably, once the user has assumed the required position the method is further configured to signal this fact to the user.</p>
<p id="p0018" num="0018">Preferably, prior to said selection step the method is configured to establish the user's body proportions based on a series of test exercises aimed at measuring such body proportions.</p>
<p id="p0019" num="0019">The invention also relates to a computer program comprising program code means for performing all the steps of the computer-implemented method according as described herein when said program is run on a computer, as well to a computer readable medium storing computer-executable instructions performing all the steps of the computer-implemented method as described herein when executed on a computer.</p>
<p id="p0020" num="0020">The invention further relates to a system for calibrating a user interface wherein the system comprises: a camera; an augmented reality engine configured to generate computer objects that enhance image captured by said camera; a controller, the system being characterized in that the controller is configured to execute all steps of the method according to any of claims 1-12.</p>
<heading id="h0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p0021" num="0021">These and other objects of the invention presented herein are accomplished by providing a system and method for calibrating a user interface. Further details and features of the present invention, its nature and various advantages will become more apparent from the following detailed description of the preferred embodiments shown in a drawing, in which:
<ul id="ul0001" list-style="none" compact="compact">
<li><figref idref="f0001">Fig. 1</figref> presents a diagram of the system according to the present invention;</li>
<li><figref idref="f0002">Fig. 2</figref> presents a diagram of the method according to the present invention;</li>
<li><figref idref="f0003 f0004">Figs. 3A-B</figref> present examples of user interface during calibration;</li>
<li><figref idref="f0005">Fig. 4</figref> presents examples of monitored movements of a user; and</li>
<li><figref idref="f0005">Fig. 5</figref> shows examples of an embodiment using outlines for positioning.</li>
</ul><!-- EPO <DP n="4"> --></p>
<heading id="h0005">NOTATION AND NOMENCLATURE</heading>
<p id="p0022" num="0022">Some portions of the detailed description which follows are presented in terms of data processing procedures, steps or other symbolic representations of operations on data bits that can be performed on computer memory. Therefore, a computer executes such logical steps thus requiring physical manipulations of physical quantities.</p>
<p id="p0023" num="0023">Usually these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated in a computer system. For reasons of common usage, these signals are referred to as bits, packets, messages, values, elements, symbols, characters, terms, numbers, or the like.</p>
<p id="p0024" num="0024">Additionally, all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Terms such as "processing" or "creating" or "transferring" or "executing" or "determining" or "detecting" or "obtaining" or "selecting" or "calculating" or "generating" or the like, refer to the action and processes of a computer system that manipulates and transforms data represented as physical (electronic) quantities within the computer's registers and memories into other data similarly represented as physical quantities within the memories or registers or other such information storage.</p>
<p id="p0025" num="0025">A computer-readable (storage) medium, such as referred to herein, typically may be non-transitory and/or comprise a non-transitory device. In this context, a non-transitory storage medium may include a device that may be tangible, meaning that the device has a concrete physical form, although the device may change its physical state. Thus, for example, non-transitory refers to a device remaining tangible despite a change in state.</p>
<p id="p0026" num="0026">As utilized herein, the term "example" means serving as a non-limiting example, instance, or illustration. As utilized herein, the terms "for example" and "e.g." introduce a list of one or more non-limiting examples, instances, or illustrations.<!-- EPO <DP n="5"> --></p>
<heading id="h0006">DESCRIPTION OF EMBODIMENTS</heading>
<p id="p0027" num="0027">The present invention aims at calibrating a user interface. In particular, it targets optimizing camera view to match given exercise so that exercises involving relatively small movements may be executed closer to the camera and thereby tracked with greater precision.</p>
<p id="p0028" num="0028">The above may require positioning a user in a field of view of a camera such that at least one body part of said user covers a predefined section of said field of view so that the user is able to move and reach augmented reality objects presented to the user.</p>
<p id="p0029" num="0029">In an example embodiment, a user's face may be detected, measured and then the user may be requested to move in order to reach a certain position. Based on user's face measurements it is determined what is the users theoretical movements reach and the users may be guided to position themselves so as to be able to reach the augmented reality targets (i.e. such that the user is not positioned too far or too close).</p>
<p id="p0030" num="0030">In one embodiment, after a user has been positioned as required by the present system, a camera view may be adjusted e.g. cropped or zoomed to an area of interest focusing on the user and discarding irrelevant portions of captured images.</p>
<p id="p0031" num="0031"><figref idref="f0001">Fig. 1</figref> presents a diagram of the system according to the present invention. The system may be realized using dedicated components or custom made FPGA or ASIC circuits. Alternatively, the system may be implemented by means of a standard computer, such as a stand-alone PC computer, a tablet, a smartphone, etc. The system comprises a data bus 101 communicatively coupled to a memory 104. Additionally, other components of the system are communicatively coupled to the system bus 101 so that they may be managed by a controller 105.</p>
<p id="p0032" num="0032">The memory 104 may store computer program or programs executed by the controller 105 in order to execute steps of the method according to the present invention.</p>
<p id="p0033" num="0033">The controller 105 receives data from a camera 103 and processes the data to obtain information on presence and positioning of users body parts in the<!-- EPO <DP n="6"> --> captured image.</p>
<p id="p0034" num="0034">The system may work with any type of camera, such as a typical Internet communication camera, a higher resolution 2D camera such as an FHD or 4K camera or more sophisticated 3D cameras..</p>
<p id="p0035" num="0035">The use of said camera 103 may also allow gesture control of the complete system as known in the art. Alternatively, voice control may be employed (not shown) or other control methods known to persons skilled in the art.</p>
<p id="p0036" num="0036">A movements database 107 is configured to store definitions of movements that contain information on involved body parts of a user, which shall be detected and tracked as well as a range of motion.</p>
<p id="p0037" num="0037">The records present in the movements database 107 may define for example expected key positioning of body parts or sections of body parts e.g. a crook of an elbow, an arm, palm, an elbow, fingers, eyes, ears, feet etc. as will be recognised by a person skilled in the art.</p>
<p id="p0038" num="0038">In an alternative embodiment, such record may be defined using a series of points in space that are expected to be matched usually withing a given threshold in order to detect a matching movement.</p>
<p id="p0039" num="0039"><figref idref="f0005">Fig. 4</figref> presents examples of such movements. In case of the top-left scenario a user is expected to move both stretched arms up and down (i.e. virtually drawing portions of a circle). The system is to capture and identify these movements as well as match the identified movements with a definition comprised in a record of the movements database 107.</p>
<p id="p0040" num="0040">The definition of an exercise comprises information on where a given one or body parts of the user shall be positioned in the captured image. The size of the respective body parts in the image is also taken into account as typically a head (of an adult) of a given size in the image will require the user to be closer to the camera than for example in case the expected head size is much smaller.</p>
<p id="p0041" num="0041">It is clear that different persons may have different proportions of a body. In general, the further a person is from the camera, the smaller the person's head is in the image while the other body parts present in the image decrease proportionally e.g. reach of hands. In case, for example child's body proportions are set in a configuration as matching adult's proportions, bringing such child to a<!-- EPO <DP n="7"> --> required distance from the camera will result in that the child's head will have an expected size in the image and therefore other body parts will have an expected size as well (in particular reach of hands).</p>
<p id="p0042" num="0042">Particular body proportions of a particular person may however be measured and stored. Such proportions may be measured physically and input to the system as configuration parameters or alternatively the system may establish such proportions based on a series of test exercises aimed at measuring the body proportions. For example, a person may be asked to stand sufficiently far from the camera to have the full body in the image and to stretch a hand as far as possible to one side at a given angle with respect to the body - then a proportion between an arm and a head may be determined.</p>
<p id="p0043" num="0043">An augmented reality engine 106 is responsible for creating an interactive experience comprising camera-based images of a real-world environment where computer generated objects enhance objects present in the real world. Examples of such augmented reality images have been presented in <figref idref="f0003 f0004">Fig. 3A-B</figref>.</p>
<p id="p0044" num="0044">A video output generator 102 is responsible for combining the camera signal with the augmented reality graphics in order to output a signal to a suitable display.</p>
<p id="p0045" num="0045">A calibration module 108 may be configured to establish body proportions of a user as mentioned above. This module may also be responsible for positioning a person, within an area captured by the respective camera, preferably using visual or audible cues. Optionally, the calibration module 108 may also inform a user that camera positioning must be changed because a required positioning of a user is not possible at given camera setup e.g. the camera is directed too low. Positioning of said camera need not be very precise, as long as a user image may be manipulated e.g. cropped to a required positioning in the image.</p>
<p id="p0046" num="0046"><figref idref="f0002">Fig. 2</figref> presents a diagram of the method according to the present invention. The method starts at step 201 by selecting a type of exercise to perform. To this end, a graphical user interface may be presented on a display so that a user may select an exercise or a set of exercises forming a training session.<!-- EPO <DP n="8"> --></p>
<p id="p0047" num="0047">Subsequently, at step 202, the system accesses the movements database 107 to access definitions of exercises wherein each exercise comprises related information on involved body parts of a user, which shall be detected and tracked as well as a range of motion.</p>
<p id="p0048" num="0048">Next, at step 203, the process determines, based on exercise record in the movements database 107 where at least one body part, such as head, of the user shall be positioned e.g. (center, side, top, bottom) as well what size (percentage of image area) shall it have in the image (closer / farther).</p>
<p id="p0049" num="0049">Then, at step 204, the method is configured to present or otherwise embed in the output video signal, a representation of users body parts outline(s), superimposed on images captured by the camera 103, where the user is expected to be positioned (for example an ideal reference position). As explained earlier, the actual positioning of a user may differ within a given threshold as long as a user image may be manipulated e.g. cropped to a required positioning in the image.</p>
<p id="p0050" num="0050"><figref idref="f0005">Fig. 5</figref> presents two use examples where an outline 501, 502 is presented during calibration and thereafter cropping is effected and the respective exercises 503, 504 are made when there is present a camera focus around the specific area of exercise.</p>
<p id="p0051" num="0051">In a more general embodiment, the user is guided by the system to assume an expected initial position which typically is within a certain threshold with respect to said ideal reference position.</p>
<p id="p0052" num="0052">Frequently, during a series of exercises there will be a need for a user to return to such expected initial position.</p>
<p id="p0053" num="0053">The guiding may be effected by audio and/or visual cues. Such ideal reference position may be defined differently for each exercise or a set of exercises. Therefore, the outline step 204 is optional.</p>
<p id="p0054" num="0054">The current positioning of a user is determined in captured images and based on that determination the system may establish what the user has to do physically in order to position within said ideal reference position.</p>
<p id="p0055" num="0055">At step 205, the user may be given instructions (displayed or audible or both) to move and position according to exercise requirements. Once the user has assumed (which is tracked by the camera image analysis) the required position<!-- EPO <DP n="9"> --> this fact may be signalled to the user i.e. the exercise shall than be performed from this initial position. The aforementioned manipulation of the images e.g. cropped to a required positioning in the image, may take place at this stage.</p>
<p id="p0056" num="0056">Once properly positioned, the augmented reality engine (106) is instructed to generate augmented reality objects/targets.</p>
<p id="p0057" num="0057">Preferably such generated objects/targets are reachable by said user (i.e. without making a single step.</p>
<p id="p0058" num="0058">Alternatively, such proper positioning is a position in which a selected exercise may be correctly executed without making any movements that are not parts of the exercise.</p>
<p id="p0059" num="0059"><figref idref="f0003">Figs. 3A</figref>, <figref idref="f0004">3B</figref> present examples of augmented reality images. An image output by the present system may comprise image captured by the camera 102, with a user 303 in the image, with augmented reality features and gamification elements e.g. round identifier 301, level identifier 305 or gaming score 304.</p>
<p id="p0060" num="0060">As shown in <figref idref="f0003">Fig. 3A</figref>, the user may be instructed to position the user's head 303 at a particular position 302. This allows to center the user at a particular space relationship.</p>
<p id="p0061" num="0061">As shown in <figref idref="f0004">Fig. 3B</figref>, if the system detects that the user is too far away from the camera, the user may be instructed to move closer by providing a larger icon 302 indicating the position of the head 303.</p>
<p id="p0062" num="0062">As shown in <figref idref="f0003">Fig. 3C</figref>, the user may be expected to stretch a hand 313 towards particular augmented reality objects 312 (e.g. a boxing bag).</p>
<p id="p0063" num="0063">At least parts of the methods according to the invention may be computer implemented. Accordingly, the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a "circuit", "module" or "system".</p>
<p id="p0064" num="0064">Furthermore, the present invention may take the form of a computer program product embodied in any tangible medium of expression having computer usable program code embodied in the medium.<!-- EPO <DP n="10"> --></p>
<p id="p0065" num="0065">It can be easily recognized, by one skilled in the art, that the aforementioned method for calibrating a user interface may be performed and/or controlled by one or more computer programs. Such computer programs are typically executed by utilizing the computing resources in a computing device. Applications are stored on a non-transitory medium. An example of a non-transitory medium is a non-volatile memory, for example a flash memory while an example of a volatile memory is RAM. The computer instructions are executed by a processor. These memories are exemplary recording media for storing computer programs comprising computer-executable instructions performing all the steps of the computer-implemented method according the technical concept presented herein.</p>
<p id="p0066" num="0066">While the invention presented herein has been depicted, described, and has been defined with reference to particular preferred embodiments, such references and examples of implementation in the foregoing specification do not imply any limitation on the invention. It will, however, be evident that various modifications and changes may be made thereto without departing from the broader scope of the technical concept. The presented preferred embodiments are exemplary only, and are not exhaustive of the scope of the technical concept presented herein.</p>
<p id="p0067" num="0067">Accordingly, the scope of protection is not limited to the preferred embodiments described in the specification, but is only limited by the claims that follow.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="11"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A method for calibrating a user interface, the method comprising the steps of:
<claim-text>- selecting (201) a type of exercise to be performed by a user;</claim-text>
<claim-text>- accessing (202) a movements database (107) to access definitions of exercises wherein each exercise comprises related information on involved body parts of a user, which shall be detected and tracked as well as a range of motion;</claim-text>
<claim-text>- determining (203), based on a selected exercise definition, where at least one body part of the user shall be positioned;</claim-text>
<claim-text>- generating instructions (205) for the user to move and position according to selected exercise requirements;</claim-text>
<claim-text>- awaiting assuming by the user the position according to exercise requirements by analysing video signal captured by the camera (103).</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The method according to claim 1 wherein said selection step (201) is executed by generating and outputting a graphical user interface and receiving from the user a selection of an exercise.</claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The method according to any of previous claims, wherein the method further comprises a step of generating the selected exercise using augmented reality (106) such that the augmented reality objects are reachable by said user.</claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>The method according to any of previous claims, wherein prior to said step of generating instructions (205) the method is configured to execute a step of generating (204) in the output video signal, a representation of user's body parts outline, superimposed on images captured by a camera (103), where the user is expected to be positioned.</claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>The method according to claim 4 wherein said head shall have a predefined position as well as size in an image captured by the camera (103).<!-- EPO <DP n="12"> --></claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>The method according to claim 5 wherein said size refers to a percentage of the image area.</claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>The method according to any of previous claims, wherein said outline where the user is expected to be positioned is an reference position.</claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>The method according to claim 7 wherein the current position of a user is acceptable as long as it differs from said reference position within a given threshold, wherein said method executes a step of manipulating said image in order to obtain said reference position in the image.</claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>The method according to any of previous claims, wherein said instructions (205) are visual instructions embedded in output video signal.</claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>The method according to any of previous claims, wherein said instructions (205) are audible instructions.</claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>The method according to any of previous claims, wherein once the user has assumed the required position the method is further configured to signal this fact to the user.</claim-text></claim>
<claim id="c-en-0012" num="0012">
<claim-text>The method according to any of previous claims, wherein prior to said selection step (201) the method is configured to establish the user's body proportions based on a series of test exercises aimed at measuring such body proportions.</claim-text></claim>
<claim id="c-en-0013" num="0013">
<claim-text>A computer program comprising program code means for performing all the steps of the computer-implemented method according to any of claims 1-12 when said program is run on a computer.</claim-text></claim>
<claim id="c-en-0014" num="0014">
<claim-text>A computer readable medium storing computer-executable instructions<!-- EPO <DP n="13"> --> performing all the steps of the computer-implemented method according to any of claims 1-12 when executed on a computer.</claim-text></claim>
<claim id="c-en-0015" num="0015">
<claim-text>A system for calibrating a user interface wherein the system comprises:
<claim-text>- a camera (103);</claim-text>
<claim-text>- an augmented reality engine (106) configured to generate computer objects that enhance image captured by said camera (103);</claim-text>
<claim-text>- a controller (105),<br/>
the system being <b>characterized in that</b></claim-text>
<claim-text>- the controller (105) is configured to execute all steps of the method according to any of claims 1-12.</claim-text></claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="14"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="162" he="168" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="15"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="121" he="194" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="16"> -->
<figure id="f0003" num="3A,3C"><img id="if0003" file="imgf0003.tif" wi="151" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="17"> -->
<figure id="f0004" num="3B"><img id="if0004" file="imgf0004.tif" wi="155" he="111" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="18"> -->
<figure id="f0005" num="4,5"><img id="if0005" file="imgf0005.tif" wi="160" he="233" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="157" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="155" he="233" type="tif"/></search-report-data><search-report-data date-produced="20200904" id="srepxml" lang="en" srep-office="EP" srep-type="ep-sr" status="n"><!--
 The search report data in XML is provided for the users' convenience only. It might differ from the search report of the PDF document, which contains the officially published data. The EPO disclaims any liability for incorrect or incomplete data in the XML for search reports.
 -->

<srep-info><file-reference-id>191146</file-reference-id><application-reference><document-id><country>EP</country><doc-number>20461526.4</doc-number></document-id></application-reference><applicant-name><name>Neuroforma Sp. z o.o.</name></applicant-name><srep-established srep-established="yes"/><srep-invention-title title-approval="yes"/><srep-abstract abs-approval="yes"/><srep-figure-to-publish figinfo="by-applicant"><figure-to-publish><fig-number>1</fig-number></figure-to-publish></srep-figure-to-publish><srep-info-admin><srep-office><addressbook><text>DH</text></addressbook></srep-office><date-search-report-mailed><date>20200918</date></date-search-report-mailed></srep-info-admin></srep-info><srep-for-pub><srep-fields-searched><minimum-documentation><classifications-ipcr><classification-ipcr><text>G06F</text></classification-ipcr><classification-ipcr><text>G06K</text></classification-ipcr><classification-ipcr><text>A61B</text></classification-ipcr></classifications-ipcr></minimum-documentation></srep-fields-searched><srep-citations><citation id="sr-cit0001"><patcit dnum="US2017136296A1" id="sr-pcit0001" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2017136296&amp;CY=ep"><document-id><country>US</country><doc-number>2017136296</doc-number><kind>A1</kind><name>BARRERA OSVALDO ANDRES [US] ET AL</name><date>20170518</date></document-id></patcit><category>X</category><rel-claims>1-15</rel-claims><rel-passage><passage>* paragraphs [0075] - [0082] *</passage><passage>* paragraph [0102] *</passage><passage>* figures 1-12 *</passage></rel-passage></citation><citation id="sr-cit0002"><patcit dnum="US2019295436A1" id="sr-pcit0002" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2019295436&amp;CY=ep"><document-id><country>US</country><doc-number>2019295436</doc-number><kind>A1</kind><name>RUBINSTEIN YIGAL DAN [US] ET AL</name><date>20190926</date></document-id></patcit><category>A</category><rel-claims>1-15</rel-claims><rel-passage><passage>* paragraphs [0020] - [0033] *</passage><passage>* paragraphs [0052] - [0056] *</passage><passage>* paragraphs [0064] - [0065]; figures 1-4,8-11 *</passage></rel-passage></citation><citation id="sr-cit0003"><patcit dnum="US2019102950A1" id="sr-pcit0003" url="http://v3.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US2019102950&amp;CY=ep"><document-id><country>US</country><doc-number>2019102950</doc-number><kind>A1</kind><name>LU HILL DOUGLAS [US] ET AL</name><date>20190404</date></document-id></patcit><category>A</category><rel-claims>1-15</rel-claims><rel-passage><passage>* paragraphs [0025] - [0030] *</passage><passage>* paragraphs [0036] - [0048] *</passage><passage>* figures 1-6 *</passage></rel-passage></citation></srep-citations><srep-admin><examiners><primary-examiner><name>de Biolley, Luc</name></primary-examiner></examiners><srep-office><addressbook><text>The Hague</text></addressbook></srep-office><date-search-completed><date>20200904</date></date-search-completed></srep-admin><!--							The annex lists the patent family members relating to the patent documents cited in the above mentioned European search report.							The members are as contained in the European Patent Office EDP file on							The European Patent Office is in no way liable for these particulars which are merely given for the purpose of information.							For more details about this annex : see Official Journal of the European Patent Office, No 12/82						--><srep-patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2017136296</doc-number><kind>A1</kind><date>20170518</date></document-id></priority-application><text>NONE</text></patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2019295436</doc-number><kind>A1</kind><date>20190926</date></document-id></priority-application><text>NONE</text></patent-family><patent-family><priority-application><document-id><country>US</country><doc-number>2019102950</doc-number><kind>A1</kind><date>20190404</date></document-id></priority-application><text>NONE</text></patent-family></srep-patent-family></srep-for-pub></search-report-data>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="US2018315247A"><document-id><country>US</country><doc-number>2018315247</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0001">[0003]</crossref></li>
<li><patcit id="ref-pcit0002" dnum="WO2015103359A"><document-id><country>WO</country><doc-number>2015103359</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0002">[0004]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
