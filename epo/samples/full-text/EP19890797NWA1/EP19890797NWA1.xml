<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP19890797A1" file="EP19890797NWA1.xml" lang="en" country="EP" doc-number="3889646" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3889646</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121><B121EP>published in accordance with Art. 153(4) EPC</B121EP></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>19890797.4</B210><B220><date>20191118</date></B220><B240><B241><date>20210609</date></B241></B240><B250>ja</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>2018225762</B310><B320><date>20181130</date></B320><B330><ctry>JP</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>G01S  17/89        20200101AFI20200605BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>G06T   7/00        20170101ALI20200605BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>G06T   7/521       20170101ALI20200605BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>G08G   1/16        20060101ALI20200605BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G06K   9/62        20130101 LI20200626BCEP        </text></classification-cpc><classification-cpc sequence="2"><text>G06T   7/521       20170101 LI20200626BCEP        </text></classification-cpc><classification-cpc sequence="3"><text>G01S  17/89        20130101 LI20200626BCEP        </text></classification-cpc><classification-cpc sequence="4"><text>G08G   1/16        20130101 LI20200626BCEP        </text></classification-cpc><classification-cpc sequence="5"><text>G06T   7/00        20130101 LI20200626BCEP        </text></classification-cpc><classification-cpc sequence="6"><text>G06K   9/00        20130101 LI20200626BCEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>FAHRZEUGINTERNES OBJEKTIDENTIFIZIERUNGSSYSTEM, KRAFTFAHRZEUG, FAHRZEUGLAMPE, KLASSIFIKATORLERNVERFAHREN UND RECHENOPERATIONSVORRICHTUNG</B542><B541>en</B541><B542>ON-VEHICLE OBJECT IDENTIFICATION SYSTEM, AUTOMOBILE, VEHICLE LAMP, CLASSIFIER LEARNING METHOD, AND ARITHMETIC OPERATION DEVICE</B542><B541>fr</B541><B542>SYSTÈME D'IDENTIFICATION D'OBJET EMBARQUÉ, AUTOMOBILE, LAMPE DE VÉHICULE, PROCÉDÉ D'APPRENTISSAGE DE CLASSIFICATEUR ET DISPOSITIF D'OPÉRATION ARITHMÉTIQUE</B542></B540><B590><B598>1</B598></B590></B500><B700><B710><B711><snm>KOITO MANUFACTURING CO., LTD.</snm><iid>101833226</iid><irf>P091215EP</irf><adr><str>4-8-3, Takanawa,</str><city>Minato-ku
Tokyo 108-8711</city><ctry>JP</ctry></adr></B711></B710><B720><B721><snm>NAGASHIMA, Toru</snm><adr><str>c/o Koito Manufacturing Co., Ltd., Shizuoka Plant,
500, Kitawaki, Shimizu-ku</str><city>Shizuoka-shi Shizuoka 424-8764</city><ctry>JP</ctry></adr></B721></B720><B740><B741><snm>Algemeen Octrooi- en Merkenbureau B.V.</snm><iid>100061505</iid><adr><str>P.O. Box 645</str><city>5600 AP Eindhoven</city><ctry>NL</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP><B860><B861><dnum><anum>JP2019045095</anum></dnum><date>20191118</date></B861><B862>ja</B862></B860><B870><B871><dnum><pnum>WO2020110802</pnum></dnum><date>20200604</date><bnum>202023</bnum></B871></B870></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">A vehicular object identification system 10 includes a distance sensor 20 and a processing device 40. The distance sensor 20 scans a single beam in the horizontal direction so as to measure the distances to points P on the surface of an object OBJ. The processing device 40 includes a classifier 42 that is capable of identifying the kind of the object OBJ based on point cloud data PCD that corresponds to the single scan line acquired by the distance sensor 20. The classifier 42 is implemented based on a learned model generated by machine learning. The machine learning is executed using multiple items of point cloud data that correspond to multiple scan lines acquired by measuring a predetermined object by means of a LiDAR that supports the multiple scan lines in the vertical direction.<img id="iaf01" file="imgaf001.tif" wi="130" he="81" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001">[TECHNICAL FIELD]</heading>
<p id="p0001" num="0001">The present invention relates to an object identification system.</p>
<heading id="h0002">[BACKGROUND ART]</heading>
<p id="p0002" num="0002">Candidates of vehicle sensors include Light Detection and Ranging, Laser Imaging Detection and Ranging (LiDAR), cameras, millimeter-wave radars, ultrasonic sonars, and so forth. In particular, LiDAR has advantages as compared with other sensors. Examples of such advantages include: (i) an advantage of being capable of identifying an object based on point group data; (ii) an advantage in employing active sensing of providing high-precision detection even in bad weather conditions; (iii) an advantage of providing wide-range measurement; etc. Accordingly, LiDAR is anticipated to become mainstream in vehicle sensing systems.</p>
<heading id="h0003">[Related Art Documents]</heading>
<heading id="h0004">[Patent Documents]</heading>
<heading id="h0005">[Patent document 1]</heading>
<p id="p0003" num="0003">
<ul id="ul0001" list-style="none" compact="compact">
<li><patcit id="pcit0001" dnum="JP2017056935A"><text>Japanese Patent Application Laid Open No. 2017-56935</text></patcit> [Patent document 2]</li>
<li><patcit id="pcit0002" dnum="JP2009098023A"><text>Japanese Patent Application Laid Open No. 2009-98023</text></patcit></li>
</ul></p>
<heading id="h0006">[DISCLOSURE OF THE INVENTION]</heading>
<heading id="h0007">[PROBLEM TO BE SOLVED BY THE INVENTION]</heading>
<p id="p0004" num="0004">The precision of object identification based on the point group data generated by the LiDAR increases according to an increase in the resolution of the point group data. However, this involves a drastic increase in calculation costs. In consideration of a case in which the<!-- EPO <DP n="2"> --> LiDAR is mounted on a vehicle, in some cases, it may be necessary to mount a low-cost, low-end processing device. In this case, such an arrangement naturally requires the number of scan lines to be reduced.</p>
<p id="p0005" num="0005">The present invention has been made in view of such a situation. Accordingly, it is an exemplary purpose of an embodiment of the present invention to provide a system, apparatus, and method that are capable of identifying an object using only a small number of horizontal lines.</p>
<heading id="h0008">[MEANS TO SOLVE THE PROBLEM]</heading>
<p id="p0006" num="0006">An embodiment of the present invention relates to a vehicular object identification system. The vehicular object identification system includes: a distance sensor structured to scan a single beam in the horizontal direction so as to measure the distances to points on the surface of an object; and a processing device including a classifier structured to be capable of identifying the kind of the object based on point cloud data that corresponds to a single scan line acquired by the distance sensor. The classifier is implemented based on a learned model generated by machine learning. The machine learning is executed using multiple items of point cloud data that correspond to multiple scan lines obtained by measuring a predetermined object by means of a LiDAR (Light Detection and Ranging) including the multiple scan lines in the vertical direction.</p>
<heading id="h0009">[ADVANTAGE OF THE PRESENT INVENTION]</heading>
<p id="p0007" num="0007">With the present invention, an object can be identified based on point cloud data that corresponds to a single scan line. Furthermore, this allows improvement in the efficiency of machine learning.</p>
<heading id="h0010">[BRIEF DESCRIPTION OF THE DRAWINGS]</heading><!-- EPO <DP n="3"> -->
<p id="p0008" num="0008">
<ul id="ul0002" list-style="none" compact="compact">
<li><figref idref="f0001">Fig. 1</figref> is a block diagram showing an object identification system according to an embodiment;</li>
<li><figref idref="f0002">Fig. 2</figref> is a block diagram showing an example configuration of a classifier;</li>
<li><figref idref="f0003">Fig. 3</figref> is a block diagram showing a learning system according to an embodiment;</li>
<li><figref idref="f0004">Figs. 4A through 4D</figref> are diagrams respectively showing multiple items of point cloud data for a pedestrian, bicycle, automobile, and utility pole acquired by means of a distance sensor;</li>
<li><figref idref="f0005">Fig. 5</figref> is a flowchart of learning by means of a learning system;</li>
<li><figref idref="f0006">Fig. 6</figref> is a diagram showing a distance sensor according to an embodiment;</li>
<li><figref idref="f0006">Fig. 7</figref> is a block diagram showing an automobile provided with the object identification system; and</li>
<li><figref idref="f0007">Fig. 8</figref> is an automotive lamp provided with the object identification system.</li>
</ul></p>
<heading id="h0011">[BEST MODE FOR CARRYING OUT THE INVENTION]</heading>
<heading id="h0012">OVERVIEW OF THE EMBODIMENTS</heading>
<p id="p0009" num="0009">An embodiment disclosed in the present specification relates to a vehicular object identification system. The vehicular object identification system includes: a distance sensor structured to scan a single beam in the horizontal direction so as to measure the distances to points on the surface of an object; and a processing device including a classifier structured to be capable of identifying the kind of the object based on point cloud data that corresponds to a single scan line acquired by the distance sensor. The classifier is implemented based on a learned model generated by machine learning. The machine learning is executed using multiple items of point cloud data that correspond to multiple scan lines obtained by measuring<!-- EPO <DP n="4"> --> a predetermined object by means of a LiDAR (Light Detection and Ranging) including the multiple scan lines in the vertical direction.</p>
<p id="p0010" num="0010">The object identification system allows the kind of an object to be judged using a single scan line. In a case in which the same distance sensor as that to be used in the object identification system is used in learning, in a case in which there is a difference between the height at which the distance sensor is used in learning and the height at which the distance sensor is mounted on the vehicle, such an arrangement has the potential to cause degradation in the object recognition rate. In order to solve this problem, with an arrangement in which training data is acquired while changing the height at which the distance sensor is set for training data acquisition, such an arrangement has a problem of an increased cost for data acquisition. In order to solve such a problem, a LiDAR that supports multiple scan lines and which differs from the distance sensor mounted on the vehicle is used in learning. Specifically, the multiple scan lines are each associated with the single scan line of the distance sensor so as to provide the training data, thereby providing improvement in the efficiency of data acquisition. In addition, the point cloud data that corresponds to the scan lines arranged at different heights is employed as the training data. This allows an object to be identified independent of the height of an emitted beam from the distance sensor.</p>
<p id="p0011" num="0011">Also, the distance sensor may include: a light source; a scanning device including a motor and a mirror attached to the motor and structured to reflect emitted light of the light source, in which the scanning device is structured such that probe light, which is light reflected by the mirror, can be scanned according to the rotation of the motor; a photosensor structured to detect return light, which<!-- EPO <DP n="5"> --> is the probe light reflected from a point on an object; and a processor structured to detect the distance to the point on the object based on the output of the photosensor. With such a distance sensor, the scanning device is configured as a combination of a commonplace motor and mirrors arranged in a fan structure. This provides the distance sensor with a lower cost.</p>
<heading id="h0013">EMBODIMENTS</heading>
<p id="p0012" num="0012">Description will be made below regarding the present invention based on preferred embodiments with reference to the drawings. The same or similar components, members, and processes are denoted by the same reference numerals, and redundant description thereof will be omitted as appropriate. The embodiments have been described for exemplary purposes only, and are by no means intended to restrict the present invention. Also, it is not necessarily essential for the present invention that all the features or a combination thereof be provided as described in the embodiments.</p>
<p id="p0013" num="0013"><figref idref="f0001">Fig. 1</figref> is a block diagram showing an object identification system 10 according to an embodiment. The object identification system 10 is mounted on a vehicle such as an automobile, motorcycle, or the like. The object identification system 10 judges the kind (category) of an object OBJ that is present in the surroundings of the vehicle.</p>
<p id="p0014" num="0014">The object identification system 10 mainly includes a distance sensor 20 and a processing device 40. The distance sensor 20 scans a single beam in the horizontal direction so as to measure the distances to points P on the surface of the object OBJ. The distance sensor 20 generates a single item of point group data PCD that corresponds to a single scan line SL.</p>
<p id="p0015" num="0015">Each item of point cloud data PCD includes the distance information to multiple sampling points P along the<!-- EPO <DP n="6"> --> scan line SL. The distance sensor 20 is not restricted in particular. However, in a case in which there is a need to identify an object with small irregularities, such as a pedestrian, with high precision, a LiDAR is preferably employed. It should be noted that typical LiDARs support multiple scan lines in the vertical direction. In contrast, the object identification system 10 according to the present embodiment supports only a single scan line.</p>
<p id="p0016" num="0016">The processing device 40 includes a classifier 42 that is capable of classifying the kind of the object OBJ based on a single item of point cloud data PCD that corresponds to a single scan line SL acquired by the distance sensor 20. The classifier 42 is structured using machine learning as described later. The data format of the point group data PCD is not restricted in particular. The data format of the point cloud data PCD may be a rectangular coordinate system or a polar coordinate system.</p>
<p id="p0017" num="0017">The processing device 40 outputs output data OUT that indicates the kind of the object OBJ. Also, the output data OUT may indicate the probability with which the object OBJ included in the point cloud data PCD matches each of multiple categories. It should be noted that the present invention is not restricted to such an arrangement. Examples of such kinds (categories) of the object include a pedestrian, bicycle, automobile, utility pole, etc. Regarding a pedestrian, a pedestrian as viewed from the front, a pedestrian as viewed from the rear, and a pedestrian as viewed from the side may be classified and defined as the same kind of object. The same can be said of an automobile and a bicycle. In the present embodiment, this definition is employed.</p>
<p id="p0018" num="0018">The processing device 40 may be provided as a combination of a processor (hardware component) such as a Central Processing Unit (CPU), Graphics Processing Unit (GPU),<!-- EPO <DP n="7"> --> microcontroller, or the like, and a software program to be executed by the processor (hardware component). The processing device 40 may be configured as a combination of multiple processors.</p>
<p id="p0019" num="0019"><figref idref="f0002">Fig. 2</figref> is a block diagram showing an example configuration of the classifier 42. The classifier 42 may be configured employing a neural network NN. The neural network NN is configured including an input layer 50, three intermediate layers (hidden layers) 52, and an output layer 54.</p>
<p id="p0020" num="0020">The number of units of the input layer 50 is determined according to the number of sample points for each line, which is designed to be 5,200. There are three intermediate layers with the number of units designed to be 200, 100, and 50, respectively. In the intermediate layers 52, affine transformation and transformation using a sigmoid function are performed. In the output layer 54, probability calculation is performed using affine transformation and a softmax function.</p>
<p id="p0021" num="0021">The output layer 54 may be designed to support multiple categories (e.g., four categories: pedestrian (Human), automobile (Car), bicycle (Bicycle), and utility pole (Pole)). In this case, the output data OUT may include four items of data, i.e., Human, Car, Bicycle, and Pole, each indicating the probability that the object OBJ matches the corresponding category.</p>
<p id="p0022" num="0022">As the preprocessing for the neural network NN, extraction, shifting, and normalization are preferably performed.</p>
<p id="p0023" num="0023">Extraction is processing for removing the background so as to extract the object OBJ. Shifting is data shifting processing for shifting the object such that it is positioned at the center. Normalization is processing for dividing the distance data by a predetermined value. For<!-- EPO <DP n="8"> --> example, as the predetermined value, the distance (reference distance) between the distance sensor 20 and a predetermined portion of the object OBJ at the time of the learning may be employed. This processing normalizes the value of the point cloud data such that it becomes a value in the vicinity of 1.</p>
<p id="p0024" num="0024">The above is the basic configuration of the object identification system 10. With the object identification system 10, the kind of the object OBJ can be judged using a single scan line. As the number of scan lines becomes larger, the amount of calculation performed by the processing device becomes enormous. Such an arrangement requires a high-speed processor. With the present embodiment, this arrangement requires processing for only a single scan line of point cloud data, thereby allowing the amount of calculation to be reduced. This means that the processing device 40 can be configured as a low-cost microcontroller. This allows the object identification system 10 to be provided with a lower cost.</p>
<heading id="h0014">REGARDING LEARNING</heading>
<p id="p0025" num="0025">Next, description will be made regarding learning of the classifier 42. In a case in which the same sensor as that employed in the object identification system 10, i.e., the distance sensor 20, is used in the learning of the classifier 42, and in a case in which there is a difference in height between the distance sensor used in the learning and the distance sensor when it is mounted on the vehicle, such an arrangement has the potential to cause a problem of degradation in the object recognition rate.</p>
<p id="p0026" num="0026">In order to solve this problem, an approach is conceivable in which, in the learning, training data (which is also referred to as "learning data") is acquired while changing the height (or elevation/depression angle) of the distance sensor so as to change the height of the scan line. However, such an approach has a problem of an increased cost<!-- EPO <DP n="9"> --> of data acquisition.</p>
<p id="p0027" num="0027">In order to solve such a problem, with the present embodiment, training data is acquired by means of a LiDAR that supports multiple scan lines, which differs from the distance sensor 20 mounted on the vehicle. <figref idref="f0003">Fig. 3</figref> is a block diagram showing a learning system according to an embodiment.</p>
<p id="p0028" num="0028">A learning system 70 includes a LiDAR 72 and a computer 74. In the learning, a LiDAR (Light Detection and Ranging) 72 that supports multiple scan lines SL<sub>1</sub> through SL<sub>N</sub> in the vertical direction is used. <figref idref="f0003">Fig. 3</figref> shows an example in which there is a pedestrian (human) as the object OBJ. The LiDAR 72 generates multiple items of point cloud data PCD<sub>1</sub> through PCD<sub>N</sub> that correspond to the multiple scan lines. For example, the number of lines N supported by the LiDAR to be employed for the learning is preferably on the order of eight.</p>
<p id="p0029" num="0029"><figref idref="f0003">Fig. 3</figref> shows an example in which the pedestrian and the LiDAR 72 are positioned such that they face each other. However, it is preferable to acquire multiple items of point cloud data PCD<sub>1</sub> through PCD<sub>N</sub> for the pedestrian from multiple different directions while changing the orientation of the pedestrian.</p>
<p id="p0030" num="0030">The multiple items of point cloud data PCD<sub>1</sub> through PCD<sub>N</sub> are input to the computer 74. The computer 74 performs machine learning with the multiple items of point cloud data PCD<sub>1</sub> through PCD<sub>N</sub> as the training data so as to allow a given object (a pedestrian in this example) to be identified. With this, the object identification system 10 shown in <figref idref="f0001">Fig. 1</figref> is capable of judging the kind of the object based on the point cloud data that corresponds to a single scan line.</p>
<p id="p0031" num="0031">It should be noted that all the items of point cloud data PCD<sub>1</sub> through PCD<sub>N</sub> that correspond to the multiple<!-- EPO <DP n="10"> --> scan lines SL<sub>1</sub> through SL<sub>N</sub> are not necessarily required to be used as the training data. Also, only a part of the point cloud data, which corresponds to the multiple scan lines except for both ends of the scan lines (or except for the top scan line or bottom scan line) may be employed as the training data.</p>
<p id="p0032" num="0032">In a case in which the object identification system 10 is to be designed to be capable of identifying multiple kinds of objects, multiple sets of point cloud data may preferably be acquired by means of the LiDAR 72 while changing the kind of the object OBJ.</p>
<p id="p0033" num="0033">Finally, the classifier 42 of the object identification system 10 is implemented based on a learned model (trained model) generated by machine learning.</p>
<p id="p0034" num="0034"><figref idref="f0004">Figs. 4A through 4D</figref> are diagrams showing multiple items of point cloud data PCD<sub>1</sub> through PCD<sub>8</sub> respectively acquired by means of the distance sensor 20 for a pedestrian, bicycle, automobile, and utility pole. Upon receiving the point cloud data PCD<sub>i</sub> that corresponds to any one from among the scan lines SL<sub>i</sub> (i = 1 through 8), the classifier 42 implemented based on the learned model generated by the machine learning described above is capable of judging which one from among the multiple categories matches the point cloud data PCD<sub>i</sub> with a high probability.</p>
<p id="p0035" num="0035"><figref idref="f0005">Fig. 5</figref> is a flowchart showing the learning by the learning system 70. At least one predetermined object is measured using the LiDAR 72 that supports multipole scan lines in the vertical direction, and that differs from the distance sensor 20 (S100). With this, multiple items of point cloud data PCD<sub>1</sub> through PCD<sub>N</sub> that correspond to the multiple scan lines are generated for each object.</p>
<p id="p0036" num="0036">Subsequently, machine learning is performed with each of the multiple items of point cloud data PCD<sub>1</sub> through PCD<sub>N</sub> as training data so as to allow a given object to be<!-- EPO <DP n="11"> --> identified (S102). Subsequently, the classifier 42 is implemented based on a learned model generated by machine learning (S104).</p>
<p id="p0037" num="0037">The above is a description of the learning system 70 and the learning method. With the learning system 70 or the learning method, the multiple scan lines SL<sub>1</sub> through SL<sub>N</sub> of the LiDAR 72 are each associated with a single scan line of the distance sensor. This provides efficient data acquisition.</p>
<p id="p0038" num="0038">Furthermore, learning is performed using point cloud data acquired by scan lines having different heights. This enables object recognition independent of the height of the emitted beam of the distance sensor 20. That is to say, this means that the restriction on the height at which the distance sensor 20 is mounted on a vehicle is relaxed. Furthermore, this means that such an arrangement provides improved tolerance for pitching of a vehicle while traveling.</p>
<p id="p0039" num="0039">Next, description will be made regarding an example configuration of the distance sensor 20. <figref idref="f0006">Fig. 6</figref> is a block diagram showing a distance sensor 100 according to an embodiment. The distance sensor 100 includes a light source 110, a scanning device 120, a photosensor 130, and a processor 140. The light source 110 emits light L1 having an infrared spectrum, for example. The emitted light L1 of the light source 110 may be modulated with respect to time.</p>
<p id="p0040" num="0040">The scanning device 120 includes a motor 122 and one or multiple mirrors (which will be also referred to as "blades") 126. The mirrors 126 are configured to have a fan structure. The mirrors 126 are attached to a rotational shaft 124 of the motor 122 such that they reflect the emitted light L1 of the light source 110. The emission angle (which will also be referred to as a "scan angle") θ of probe light L2, which is light reflected from the mirrors 126, changes according to the positions of the mirrors 126 (i.e.,<!-- EPO <DP n="12"> --> rotational angle φ of the motor). Accordingly, by rotationally driving the motor 122, the probe light L2 can be scanned in the θ direction ranging between θ<sub>MIN</sub> and θ<sub>MAX</sub>. It should be noted that, in a case in which the number of mirrors 126 thus provided is two, one half-rotation of the motor 122 (mechanical angle of 180 degrees) corresponds to a single scan. Accordingly, the probe light L2 is scanned twice every time the motor 122 is rotated once. It should be noted that the number of the mirrors 126 is not restricted in particular.</p>
<p id="p0041" num="0041">The rotational angle φ of the motor 122 can be detected by means of a position detection mechanism such as a Hall sensor, optical encoder, or the like. Accordingly, the scan angle θ at each time point can be obtained based on the rotational angle φ. In a case in which a stepping motor is employed as the motor 122, the rotational angle φ can be controlled by an open-loop control operation, thereby allowing the position detection mechanism to be omitted.</p>
<p id="p0042" num="0042">The photosensor 130 detects return light L3 which is the probe light L2 reflected at a point P on an object OBJ. The processor 140 detects the distance to the point P on the object OBJ based on the output of the photosensor 130. The distance detection method or algorithm is not restricted in particular. Rather, known techniques may be employed. For example, the delay time from the emission of the probe light L2 to the reception of the return light by means of the photosensor 130, i.e., the time of flight (TOF), may be measured so as to acquire the distance.</p>
<p id="p0043" num="0043">The above is the basic configuration of the distance sensor 100. Next, description will be made regarding the operation thereof. The motor 122 is rotationally driven so as to change the scan angle θ of the probe light L2 in the order of θ<sub>1</sub>, θ<sub>2</sub>, .... In this operation,<!-- EPO <DP n="13"> --> the distance r<sub>i</sub> to the point P<sub>i</sub> on the surface of the object OBJ is measured at each scan angle θ<sub>i</sub> (i = 1, 2, ...). With this, data pairs (point cloud data) each configured as a pair of the scan angle θ<sub>i</sub> and the corresponding distance r<sub>i</sub>, can be acquired.</p>
<p id="p0044" num="0044">With such a distance sensor 100, the scanning device 120 can be configured as a combination of the motor 122 configured as a commonplace motor and the mirrors 126 arranged in a fan structure. This provides the distance sensor 100 with a lower cost.</p>
<p id="p0045" num="0045"><figref idref="f0006">Fig. 7</figref> is a block diagram showing an automobile provided with the object identification system 10. An automobile 300 is provided with headlamps 302L and 302R. At least one from among the headlamps 302L and 302R is provided with the object identification system 10 as a built-in component. Each headlamp 302 is positioned at a frontmost end of the vehicle body, which is most advantageous as a position where the distance sensor 100 is to be installed for detecting an object in the vicinity.</p>
<p id="p0046" num="0046"><figref idref="f0007">Fig. 8</figref> is a block diagram showing an automotive lamp 200 including the object detection system 400. The automotive lamp 200 forms a lamp system 310 together with an in-vehicle ECU 304. The automotive lamp 200 includes a light source 202, a lighting circuit 204, and an optical system 206. Furthermore, the automotive lamp 200 is provided with the object detection system 400. The object detection system 400 corresponds to the above-described object identification system 10, and includes the distance sensor 100 and a processing device 410. The distance sensor 100 corresponds to the distance sensor 20 shown in <figref idref="f0002">Fig. 2</figref>. The processing device 410 judges the presence or absence and the kind of an object OBJ in front of the vehicle based on point cloud data acquired by the distance sensor 100. The processing device 410 may include an identifying device obtained by machine<!-- EPO <DP n="14"> --> learning. The processing device 410 corresponds to the processing device 40 shown in <figref idref="f0002">Fig. 2</figref>.</p>
<p id="p0047" num="0047">Also, the information with respect to the object OBJ detected by the processing device 410 may be used to support the light distribution control operation of the automotive lamp 200. Specifically, a lamp ECU 208 generates a suitable light distribution pattern based on the information with respect to the kind of the object OBJ and the position thereof generated by the processing device 410. The lighting circuit 204 and the optical system 206 operate so as to provide the light distribution pattern generated by the lamp ECU 208.</p>
<p id="p0048" num="0048">Also, the information with respect to the object OBJ detected by the processing device 410 may be transmitted to the in-vehicle ECU 304. The in-vehicle ECU may support autonomous driving based on the information thus transmitted.</p>
<p id="p0049" num="0049">Description has been made above regarding the present invention with reference to the embodiments. The above-described embodiments have been described for exemplary purposes only, and are by no means intended to be interpreted restrictively. Rather, it can be readily conceived by those skilled in this art that various modifications may be made by making various combinations of the aforementioned components or processes, which are also encompassed in the technical scope of the present invention. Description will be made below regarding such modifications.</p>
<p id="p0050" num="0050">In an embodiment, the object may be defined as a different kind (category) for each orientation as viewed from the user's vehicle. That is to say, the same object is identified as a different kind according to the orientation thereof, e.g., whether or not the object is positioned with a face-to-face orientation with respect to the user's vehicle. This is because such identification is advantageous in estimating the object OBJ moving direction.<!-- EPO <DP n="15"> --></p>
<p id="p0051" num="0051">The processing device 40 may be configured of only a hardware component using an FPGA or the like.</p>
<p id="p0052" num="0052">Description has been made regarding the present invention with reference to the embodiments using specific terms. However, the above-described embodiments show only an aspect of the mechanisms and applications of the present invention. Rather, various modifications and various changes in the layout can be made without departing from the spirit and scope of the present invention defined in appended claims.</p>
<heading id="h0015">[INDUSTRIAL APPLICABILITY]</heading>
<p id="p0053" num="0053">The present invention relates to an object identification system.</p>
<heading id="h0016">[DESCRIPTION OF THE REFERENCE NUMERALS]</heading>
<p id="p0054" num="0054">10 object identification system, 20 distance sensor, 40 processing device, 42 classifier, 50 input layer, 52 intermediate layer, 54 output layer, 70 learning system, 72 LiDAR, 74 computer, 100 distance sensor, 110 light source, 120 scanning device, 122 motor, 124 rotational shaft, 126 mirror, 130 photosensor, 140 processor, 200 automotive lamp, 202 light source, 204 lighting circuit, 206 optical system, 300 automobile, 302 headlamp, 304 in-vehicle ECU, 310 lamp system, vehicle ECU, 310 lamp system, 400 object detection system, 410 processing device.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="16"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A vehicular object identification system comprising:
<claim-text>a distance sensor structured to scan a single beam in a horizontal direction so as to measure distances to points on a surface of an object; and</claim-text>
<claim-text>a processing device comprising a classifier structured to be capable of identifying a kind of the object based on point cloud data that corresponds to a single scan line acquired by the distance sensor,</claim-text>
<claim-text>wherein the classifier is implemented based on a learned model generated by machine learning,</claim-text>
<claim-text>and wherein the machine learning is executed using a plurality of items of point cloud data that correspond to a plurality of scan lines obtained by measuring a predetermined object by means of a LiDAR (Light Detection and Ranging) comprising the plurality of scan lines in a vertical direction.</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The object identification system according to claim 1, wherein the distance sensor comprises:
<claim-text>a light source;</claim-text>
<claim-text>a scanning device comprising a motor and a mirror attached to the motor and structured to reflect emitted light of the light source, wherein the scanning device is structured such that probe light, which is light reflected by the mirror, can be scanned according to a rotation of the motor;</claim-text>
<claim-text>a photosensor structured to detect return light, which is the probe light reflected from a point on an object; and</claim-text>
<claim-text>a processor structured to detect a distance to the point on the object based on an output of the photosensor.</claim-text></claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The object identification system according to claim<!-- EPO <DP n="17"> --> 1 or 2, wherein the classifier comprises a neural network.</claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>An automobile comprising the object identification system according to any one of claims 1 through 3.</claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>An automobile according to claim 4, wherein the distance sensor is built into a headlamp.</claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>An automotive lamp comprising the object identification system according to any one of claims 1 through 3.</claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>A method for a classifier structured to be capable of identifying a kind of an object based on point cloud data that corresponds to a single scan line acquired by a distance sensor,<br/>
wherein the method comprises:
<claim-text>measuring a predetermined object using a LiDAR (Light Detection and Ranging) structured as a component that differs from the distance sensor, and structured to support a plurality of scan lines in a vertical direction;</claim-text>
<claim-text>executing machine learning with a plurality of items of point cloud data that correspond to the plurality of scan lines as training data, so as to allow the object to be identified; and</claim-text>
<claim-text>implementing the classifier based on a learned model generated by the machine learning.</claim-text></claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>A processing device comprising a classifier structured to scan a single beam in a horizontal direction, to receive point cloud data that corresponds to a single scan line acquired by a distance sensor structured to measure distances to points on a surface of an object, and to be capable of identifying a kind of the object based on the<!-- EPO <DP n="18"> --> point cloud data,
<claim-text>wherein the classifier is implemented based on a learned model generated by machine learning,</claim-text>
<claim-text>and wherein the machine learning is executed using a plurality of items of point cloud data that correspond to the plurality of scan lines obtained by measuring a predetermined object by means of a LiDAR (Light Detection and Ranging) that supports a plurality of scan lines in a vertical direction.</claim-text></claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="19"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="126" he="212" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="20"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="152" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="21"> -->
<figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="116" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="22"> -->
<figure id="f0004" num="4A,4B,4C,4D"><img id="if0004" file="imgf0004.tif" wi="159" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="23"> -->
<figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="159" he="113" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="24"> -->
<figure id="f0006" num="6,7"><img id="if0006" file="imgf0006.tif" wi="157" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="25"> -->
<figure id="f0007" num="8"><img id="if0007" file="imgf0007.tif" wi="155" he="111" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="163" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="163" he="233" type="tif"/><doc-page id="srep0003" file="srep0003.tif" wi="163" he="233" type="tif"/></search-report-data>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="JP2017056935A"><document-id><country>JP</country><doc-number>2017056935</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0001">[0003]</crossref></li>
<li><patcit id="ref-pcit0002" dnum="JP2009098023A"><document-id><country>JP</country><doc-number>2009098023</doc-number><kind>A</kind></document-id></patcit><crossref idref="pcit0002">[0003]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
