<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP19905206A1" file="EP19905206NWA1.xml" lang="en" country="EP" doc-number="3889544" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3889544</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121><B121EP>published in accordance with Art. 153(4) EPC</B121EP></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>19905206.9</B210><B220><date>20191228</date></B220><B240><B241><date>20210701</date></B241></B240><B250>zh</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>201811634006</B310><B320><date>20181229</date></B320><B330><ctry>CN</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>G01C  11/04        20060101AFI20200703BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>G06T   7/55        20170101ALI20200703BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>G06T   7/73        20170101ALI20200703BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>B64D  47/08        20060101ALI20200703BHEP        </text></classification-ipcr><classification-ipcr sequence="5"><text>H04N  13/239       20180101ALI20200703BHEP        </text></classification-ipcr></B510EP><B540><B541>de</B541><B542>TIEFENBILDVERARBEITUNGSVERFAHREN UND -VORRICHTUNG UND UNBEMANNTES LUFTFAHRZEUG</B542><B541>en</B541><B542>DEPTH IMAGE PROCESSING METHOD AND DEVICE, AND UNMANNED AERIAL VEHICLE</B542><B541>fr</B541><B542>PROCÉDÉ ET DISPOSITIF DE TRAITEMENT D'IMAGE DE PROFONDEUR ET VÉHICULE AÉRIEN SANS PILOTE</B542></B540><B590><B598>3</B598></B590></B500><B700><B710><B711><snm>Autel Robotics Co., Ltd.</snm><iid>101741103</iid><irf>P29994WOEP</irf><adr><str>9th Floor, Building B1 
Zhiyuan 
No.1001, Xueyuan Road 
Xili Street 
Nanshan</str><city>Shenzhen, Guangdong 518055</city><ctry>CN</ctry></adr></B711></B710><B720><B721><snm>LI, Zhaozao</snm><adr><str>9th Floor, Building B1, Zhiyuan, No.1001, Xueyuan 
Road, Xili Street, Nanshan</str><city>Shenzhen, Guangdong 518055</city><ctry>CN</ctry></adr></B721></B720><B740><B741><snm>Gulde &amp; Partner</snm><iid>101079545</iid><adr><str>Patent- und Rechtsanwaltskanzlei mbB 
Wallstraße 58/59</str><city>10179 Berlin</city><ctry>DE</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP><B860><B861><dnum><anum>CN2019129562</anum></dnum><date>20191228</date></B861><B862>zh</B862></B860><B870><B871><dnum><pnum>WO2020135797</pnum></dnum><date>20200702</date><bnum>202027</bnum></B871></B870></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">Embodiments of the present invention relate to a method, apparatus and UAV for processing a depth map. The method for processing a depth map includes the following steps: S1: correcting an image of a target area that is collected by an image collection apparatus; S2: performing binocular matching on the image to obtain a depth map of the target area; and S3: acquiring a distribution of obstacles around an UAV according to the depth map. The method further includes: acquiring execution times of the foregoing steps before executing the steps; and establishing at least two threads and at least one ring queue according to the execution times of the steps, and executing the steps by the at least two threads to reduce a total execution time. By executing the steps for processing the depth map by the at least two threads, each of the threads can obtain a processing result from other threads through the at least one ring queue. Through the mode of the ring queue and parallel multi-thread running, the problem of image processing blockage is solved, and a delay is reduced.<img id="iaf01" file="imgaf001.tif" wi="78" he="97" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001"><b>BACKGROUND</b></heading>
<heading id="h0002"><b>Technical Field</b></heading>
<p id="p0001" num="0001">Embodiments of the present invention relate to the technical field of unmanned aerial vehicles (UAV), and in particular, to a method, apparatus and UAV for processing a depth map.</p>
<heading id="h0003"><b>Related Art</b></heading>
<p id="p0002" num="0002">During an autonomous flight, the UAV needs to avoid obstacles. Therefore, it is necessary to detect positions of the obstacles, so that the UAV can take obstacle avoidance measures according to the positions of the obstacles. At present, the UAV mostly adopts a vision system (such as a monocular vision system, a binocular vision system, etc.) for obstacle position detection, which uses a camera apparatus to take images of an area surrounding the UAV and processes the images to determine location information of surrounding obstacles. Then, the UAV takes obstacle avoidance measures such as detour, deceleration or pause according to its own speed, posture and the location information of the obstacles to avoid the obstacles.</p>
<p id="p0003" num="0003">During implementation of the present invention, it is found that there are at least the following problems in the related art: when the UAV performs image processing and performs obstacle avoidance measures according to processing results, if a frame rate of image collection is relatively high, it is likely to cause image processing blockage, thus causing a large delay.<!-- EPO <DP n="2"> --></p>
<heading id="h0004"><b>SUMMARY</b></heading>
<p id="p0004" num="0004">An objective of embodiments of the present invention is to provide a method, apparatus and UAV for processing a depth map, which can alleviate the problem of image processing blockage and a large delay during using a vision system on the UAV.</p>
<p id="p0005" num="0005">In a first aspect, an embodiment of the present invention provides a method for processing a depth map, applicable to a controller of the UAV. The UAV further includes an image collection apparatus, the image collection apparatus being communicatively connected to the controller. The method includes the following steps:
<ul id="ul0001" list-style="none">
<li>S1: correcting an image of a target area that is collected by the image collection apparatus;</li>
<li>S2: performing binocular matching on the image to obtain a depth map of the target area; and</li>
<li>S3: acquiring a distribution of obstacles around the UAV according to the depth map; and</li>
<li>the method further includes:
<ul id="ul0002" list-style="none">
<li>acquiring an execution time of step S1, an execution time of step S2 and an execution time of step S3 before executing step S1, step S2 and step S3; and</li>
<li>establishing at least two threads and at least one ring queue according to the execution time of step S1, the execution time of step S2 and the execution time of step S3, and executing step S1, step S2 and step S3 by the at least two threads to reduce a total execution time, where</li>
<li>of two of the at least two threads that execute adjacent ones of the steps, a processing result from a thread executing a former step is transmitted to the ring queue, and a thread executing a latter step fetches the processing result from the ring queue and executes the latter step according to the processing result.</li>
</ul></li>
</ul></p>
<p id="p0006" num="0006">In some embodiments, the establishing at least two threads and at least one ring queue according to the execution time of step S1, the execution time of step S2 and the execution time of step S3, and executing step S1, step S2 and step S3 by the at least two threads to reduce a total execution time includes:<!-- EPO <DP n="3"> -->
<ul id="ul0003" list-style="none">
<li>determining whether a sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets a first preset condition; and</li>
<li>establishing a first thread, a second thread and a first ring queue, and executing step S1, step S2 and step S3 by the first thread and the second thread if the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets the first preset condition, where</li>
<li>of the first thread and the second thread, a processing result from a thread executing a former step is transmitted to the first ring queue, and a thread executing a latter step fetches the processing result from the first ring queue and executes the latter step according to the processing result.</li>
</ul></p>
<p id="p0007" num="0007">In some embodiments, the first preset condition is that:<br/>
the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 is greater than a preset value.</p>
<p id="p0008" num="0008">In some embodiments, the preset value is 1000/P, P being an image frame rate.</p>
<p id="p0009" num="0009">In some embodiments, the first thread executes two of step S1, step S2 and step S3, and the second thread executes one of step S1, step S2 and step S3; and<br/>
the establishing a first thread, a second thread and a first ring queue, and executing step S1, step S2 and step S3 by the first thread and the second thread if the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets the first preset condition includes:
<ul id="ul0004" list-style="none">
<li>determining whether a sum of the execution times of the two steps executed by the first thread meets a second preset condition; and</li>
<li>if so, establishing a third thread and a second ring queue, and executing the two steps respectively by the first thread and the third thread, where</li>
<li>of the first thread and the third thread, a processing result from a thread executing a former step is transmitted to the second ring queue, and a thread executing a latter step fetches the processing result from the second ring queue and executes the latter step according to the<!-- EPO <DP n="4"> --> processing result.</li>
</ul></p>
<p id="p0010" num="0010">In some embodiments, the second preset condition is that a sum of the execution times of the two steps executed by the first thread is greater than a preset value.</p>
<p id="p0011" num="0011">In some embodiments, the preset value is 1000/P, P being an image frame rate.</p>
<p id="p0012" num="0012">In some embodiments, the controller includes a hardware acceleration channel, and the image collection apparatus includes at least two sets of binocular units; and<br/>
the performing binocular matching on the image to obtain a depth map of the target area includes:
<ul id="ul0005" list-style="none">
<li>transmitting images collected by the at least two sets of binocular units to the hardware acceleration channel, and performing, by performing time division multiplexing on the hardware acceleration channel, binocular matching on the images collected by the at least two sets of binocular units, to obtain the depth map.</li>
<li>In some embodiments, the hardware acceleration channel includes at least two hardware acceleration channels, and the images collected by the at least two sets of binocular units include a set of images having a first resolution and a set of images having a second resolution, the second resolution being greater than the first resolution; and</li>
<li>the performing binocular matching on the image to obtain a depth map of the target area includes:
<ul id="ul0006" list-style="none">
<li>transmitting the set of images having the second resolution to one of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images; and</li>
<li>transmitting the set of images having the first resolution to the other of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images.</li>
</ul></li>
</ul></p>
<p id="p0013" num="0013">In some embodiments, the hardware acceleration channel includes four hardware acceleration channels, i.e., a first hardware acceleration channel, a second hardware acceleration channel, a third hardware acceleration channel and a fourth hardware<!-- EPO <DP n="5"> --> acceleration channel, respectively;
<ul id="ul0007" list-style="none">
<li>the image collection apparatus includes six sets of binocular units, a set of images collected by four of the six sets of binocular units being the set of images having the first resolution, and a set of images collected by two of the six sets of binocular units being the set of images having the second resolution;</li>
<li>the transmitting the set of images having the first resolution to the other of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images includes:
<ul id="ul0008" list-style="none">
<li>transmitting the set of images collected by the two sets of binocular units respectively to the first hardware acceleration channel and the second hardware acceleration channel; and</li>
<li>the transmitting the set of images having the second resolution to one of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images includes:<br/>
transmitting a set of images collected by two of the four sets of binocular units to the third hardware acceleration channel, and transmitting a set of images collected by the remaining two of the four sets of binocular units to the fourth hardware acceleration channel.</li>
</ul></li>
</ul></p>
<p id="p0014" num="0014">In a second aspect, an embodiment of the present invention provides an apparatus for processing a depth map, applicable to a controller of the UAV. The UAV further includes an image collection apparatus, the image collection apparatus being communicatively connected to the controller. The apparatus includes:
<ul id="ul0009" list-style="none">
<li>an image correction module, configured to: execute step S1 of correcting an image of a target area that is collected by the image collection apparatus;</li>
<li>a depth map acquisition module, configured to execute step S2 of performing binocular matching on the image to obtain a depth map of the target area; and</li>
<li>an obstacle distribution acquisition module, configured to execute step S3 of acquiring a distribution of obstacles around the UAV according to the depth map.</li>
</ul></p>
<p id="p0015" num="0015">The apparatus further includes:<!-- EPO <DP n="6"> -->
<ul id="ul0010" list-style="none">
<li>a time acquisition module, configured to: acquire an execution time of step S1, an execution time of step S2 and an execution time of step S3 before the execution of step S1, step S2 and step S3; and</li>
<li>a thread and ring queue establishing module, configured to establish at least two threads and at least one ring queue according to the execution time of step S1, the execution time of step S2 and the execution time of step S3, and execute step S1, step S2 and step S3 by the at least two threads to reduce a total execution time, where</li>
<li>of two of the at least two threads that execute adjacent ones of the steps, a processing result from a thread executing a former step is transmitted to the ring queue, and a thread executing a latter step fetches the processing result from the ring queue and executes the latter step according to the processing result.</li>
</ul></p>
<p id="p0016" num="0016">In some embodiments, the thread and ring queue establishing module includes:
<ul id="ul0011" list-style="none">
<li>a determination submodule, configured to determine whether a sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets a first preset condition; and</li>
<li>a thread and ring queue establishing submodule, configured to establish a first thread, a second thread and a first ring queue, and execute step S1, step S2 and step S3 by the first thread and the second thread if the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets a first preset condition, where</li>
<li>of the first thread and the second thread, a processing result from a thread executing a former step is transmitted to the first ring queue, and a thread executing a latter step fetches the processing result from the first ring queue and executes the latter step according to the processing result.</li>
</ul></p>
<p id="p0017" num="0017">In some embodiments, the first preset condition is that:<br/>
the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 is greater than a preset value.</p>
<p id="p0018" num="0018">In some embodiments, the preset value is 1000/P, P being an image frame rate.<!-- EPO <DP n="7"> --></p>
<p id="p0019" num="0019">In some embodiments, the first thread executes two of step S1, step S2 and step S3, and the second thread executes one of step S1, step S2 and step S3; and<br/>
the thread and ring queue establishing submodule is further configured to:
<ul id="ul0012" list-style="none">
<li>determine whether a sum of the execution times of the two steps executed by the first thread meets a second preset condition; and</li>
<li>if so, establish a third thread and a second ring queue, and executing the two steps respectively by the first thread and the third thread, where</li>
<li>of the first thread and the third thread, a processing result from a thread executing a former step is transmitted to the second ring queue, and a thread executing a latter step fetches the processing result from the second ring queue and executes the latter step according to the processing result.</li>
</ul></p>
<p id="p0020" num="0020">In some embodiments, the second preset condition is that a sum of the execution times of the two steps executed by the first thread is greater than a preset value.</p>
<p id="p0021" num="0021">In some embodiments, the preset value is 1000/P, P being an image frame rate.</p>
<p id="p0022" num="0022">In some embodiments, the controller includes a hardware acceleration channel, and the image collection apparatus includes at least two sets of binocular units; and<br/>
the depth map acquisition module is further configured to:
<ul id="ul0013" list-style="none">
<li>transmit images collected by the at least two sets of binocular units to the hardware acceleration channel, and perform, by performing time division multiplexing on the hardware acceleration channel, binocular matching on the images collected by the at least two sets of binocular units, to obtain the depth map.</li>
<li>In some embodiments, the hardware acceleration channel includes at least two hardware acceleration channels, and the images collected by the at least two sets of binocular units include a set of images having a first resolution and a set of images having a second resolution, the second resolution being greater than the first resolution; and</li>
<li>the depth map acquisition module is further configured to:<!-- EPO <DP n="8"> -->
<ul id="ul0014" list-style="none">
<li>transmit the set of images having the second resolution to one of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images; and</li>
<li>transmit the set of images having the first resolution to the other of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images.</li>
<li>In some embodiments, the hardware acceleration channel includes four hardware acceleration channels, i.e., a first hardware acceleration channel, a second hardware acceleration channel, a third hardware acceleration channel and a fourth hardware acceleration channel, respectively;</li>
<li>the image collection apparatus includes six sets of binocular units, a set of images collected by four of the six sets of binocular units being the set of images having the first resolution, and a set of images collected by two of the six sets of binocular units being the set of images having the second resolution;</li>
<li>the depth map acquisition module is further configured to:
<ul id="ul0015" list-style="none">
<li>transmit the set of images collected by the two sets of binocular units respectively to the first hardware acceleration channel and the second hardware acceleration channel; and</li>
<li>transmit a set of images collected by two of the four sets of binocular units to the third hardware acceleration channel, and transmit a set of images collected by the remaining two of the four sets of binocular units to the fourth hardware acceleration channel.</li>
</ul></li>
</ul></li>
</ul></p>
<p id="p0023" num="0023">In a third aspect, an embodiment of the present invention provides a UAV. The UAV includes:
<ul id="ul0016" list-style="none">
<li>a fuselage;</li>
<li>a wing connected to the fuselage;</li>
<li>a power apparatus, disposed on the wing;</li>
<li>an image collection apparatus disposed on the fuselage, the image collection apparatus being configured to acquire a target image of a target area of the UAV; and<!-- EPO <DP n="9"> --></li>
<li>a vision chip disposed in the fuselage, the vision chip being communicatively connected to the image collection apparatus, where</li>
<li>the vision chip includes:
<ul id="ul0017" list-style="none">
<li>at least one processor; and</li>
<li>a memory, communicatively connected to the at least one processor, the memory storing instructions executable by the at least one processor, the instructions, when being executed by the at least one processor, causing the at least one processor to perform the method.</li>
</ul></li>
</ul></p>
<p id="p0024" num="0024">In a fourth aspect, an embodiment of the present invention provides a non-volatile computer-readable storage medium, the computer-readable storage medium storing computer executable instructions which, when executed by a UAV, cause the UAV to execute the foregoing method.</p>
<p id="p0025" num="0025">According to the method, apparatus and UAV for processing a depth map of the embodiments of the present invention, at least two threads and at least one ring queue are established according to the execution times of all steps during depth map processing by the controller of the UAV, and the at least two threads execute all of the steps for processing the depth map. Each of the threads can obtain processing results from other threads through the at least one ring queue. Through the mode of the ring queue and parallel multi-thread running, the problem of image processing blockage is solved, and a delay is reduced.</p>
<heading id="h0005"><b>BRIEF DESCRIPTION OF THE DRAWINGS</b></heading>
<p id="p0026" num="0026">One or more embodiments are exemplarily described with reference to the corresponding figures in the accompanying drawings, and the descriptions are not to be construed as limiting the embodiments. Elements in the accompanying drawings that have same reference numerals are represented as similar elements, and unless otherwise particularly stated, the figures in the accompanying drawings are not drawn to scale.
<ul id="ul0018" list-style="none">
<li><figref idref="f0001">FIG. 1</figref> is a schematic diagram of an application scenario of a method and apparatus for processing a depth map according to an embodiment of the present invention.</li>
<li><figref idref="f0001">FIG. 2</figref> is a schematic structural diagram of hardware of an embodiment of a UAV<!-- EPO <DP n="10"> --> according to the present invention.</li>
<li><figref idref="f0002">FIG. 3</figref> is a schematic flowchart of an embodiment of the method for processing a depth map according to the present invention.</li>
<li><figref idref="f0003">FIG. 4</figref> is a schematic diagram of applying hardware acceleration channels in an embodiment of the method for processing a depth map according to the present invention.</li>
<li><figref idref="f0003">FIG. 5</figref> is a schematic structural diagram of an embodiment of the apparatus for processing a depth map according to the present invention.</li>
<li><figref idref="f0004">FIG. 6</figref> is a schematic structural diagram of an embodiment of the apparatus for processing a depth map according to the present invention.</li>
<li><figref idref="f0004">FIG. 7</figref> is a schematic structural diagram of hardware of a vision chip in an embodiment of the UAV according to the present invention.</li>
</ul></p>
<heading id="h0006"><b>DETAILED DESCRIPTION</b></heading>
<p id="p0027" num="0027">To make the objectives, technical solutions, and advantages of the embodiments of the present invention clearer, the following clearly and completely describes the technical solutions in the embodiments of the present invention with reference to the accompanying drawings in the embodiments of the present invention. Apparently, the described embodiments are merely some embodiments of the present invention rather than all of the embodiments. All other embodiments obtained by a person of ordinary skill in the art based on the embodiments of this application without creative efforts shall fall within the protection scope of this application.</p>
<p id="p0028" num="0028">A method, apparatus and UAV for processing a depth map provided in embodiments of the present invention are applicable to an application scenario shown in <figref idref="f0001">FIG. 1</figref>. The application scenario includes a UAV 100 and an obstacle 200. The UAV 100 may be a suitable UAV, including a fixed-wing UAV and a rotary-wing UAV, such as a helicopter, a quadrotor and an aircraft with other numbers of rotors and/or rotor configuration. The UAV 100 may further be other movable objects, such as a manned aircraft, a model airplane, an unmanned airship and an unmanned hot air balloon. The obstacle 200 may be, for example, a building, a mountain, a tree, a forest, a signal tower or other movable or non-movable<!-- EPO <DP n="11"> --> objects (only one obstacle is shown in <figref idref="f0001">FIG. 1</figref>, there may be more obstacles or no obstacle in actual application).</p>
<p id="p0029" num="0029">In some embodiments, referring to <figref idref="f0001">FIG. 2 (FIG. 2</figref> only shows a part of the composition of the UAV 100). The UAV 100 includes a fuselage 10, a wing connected to the fuselage 10, a power apparatus and a control system disposed on the fuselage 10. The power apparatus is configured to provide push force or lift force for the UAV 10 to fly. The control system is a central nervous system of the UAV 100 and may include a plurality of functional units, such as a flight control system, a vision system and other systems with specific functions. The vision system includes an image collection apparatus 30 and a vision chip 20, and the flight control system includes various sensors (such as a gyroscope and an accelerometer) and a flight control chip.</p>
<p id="p0030" num="0030">During autonomous flight, the UAV 100 needs to recognize and avoid the obstacle 200 in front by itself. The UAV 100 may detect location information of obstacles around the UAV 100 through the image collection apparatus 30 and the vision chip 20, and take obstacle avoidance measures according to the location information.</p>
<p id="p0031" num="0031">The image collection apparatus 30 is configured to acquire a target image of the target area, and may adopt, for example, a high-definition camera, a sport camera or the like. The vision chip 20 is communicatively connected to the image collection apparatus 30, may acquire the target image collected by the image collection apparatus 30, and perform image processing on the target image to obtain depth information of a corresponding area of the target image, thereby obtaining location information of the obstacle 200 around the UAV 100. The vision chip 20 can take obstacle avoidance measures according to the location information of the obstacle 200. The flight control chip controls the UAV 100 according to the obstacle avoidance measures. The obstacle avoidance measures include controlling the UAV to slow down, pause or the like. The vision chip 20 may further determine a distance of an obstacle and perform three-dimensional reconstruction according to the location information of the obstacle 200.</p>
<p id="p0032" num="0032">The image collection apparatus 30 may include at least one monocular unit or at least one binocular unit (the binocular unit is used as an example for description below). Each<!-- EPO <DP n="12"> --> binocular unit may obtain a set of target images, and each binocular unit collects the target images at a preset frame rate. The vision chip 20 needs to perform image processing on sets of target images obtained by the binocular units to obtain depth information corresponding to each of the sets of target images. The vision chip 20 also needs to obtain a distribution of obstacles 200 around the UAV 100 according to the depth information corresponding to each of the sets of target images. When the frame rate at which the target images are collected is relatively high, it may cause incapability of processing by the vision chip 20, which in turn hampers the processing of the sets of target images of an inputted source, causing image processing blockage and a large delay.</p>
<p id="p0033" num="0033">In order to solve the problem of image processing blockage and the large delay, in the embodiments of the present invention, at least two threads and at least one ring queue may be established according to the execution times of all steps during depth map processing by the controller of the UAV, and when a sum of the execution times of all steps is relatively large, the at least two threads execute all of the steps for processing the depth map. Each of the threads can obtain processing results from other threads through the at least one ring queue. Through the mode of the ring queue and parallel multi-thread running, the problem of image processing blockage is solved, and a delay is reduced.</p>
<p id="p0034" num="0034">In the foregoing embodiment, the vision chip 20 is disposed for the UAV 100 to obtain the obstacle avoidance measures of the UAV 100 according to the image acquired by the image collection apparatus 30. In some other embodiments, the UAV 100 may also use other controllers to implement the function of the vision chip 20.</p>
<p id="p0035" num="0035"><figref idref="f0002">FIG. 3</figref> is a schematic flowchart of a method for processing a depth map according to an embodiment of the present invention. The method is used in the controller of the UAV 100 shown in <figref idref="f0001">FIG. 1 or FIG. 2</figref>. In some of the embodiments, the controller may be the vision chip 20 of the UAV 100. As shown in <figref idref="f0002">FIG. 3</figref>, the method includes the following steps.</p>
<p id="p0036" num="0036">S1: Correct an image of a target area that is collected by the image collection apparatus.</p>
<p id="p0037" num="0037">The image collection apparatus may be a binocular unit, and the correcting an image collected by the image collection apparatus includes correcting the image and calibrating<!-- EPO <DP n="13"> --> each set of images collected by the binocular unit to acquire calibration parameters corresponding to each set of images.</p>
<p id="p0038" num="0038">S2: Perform binocular matching on the image to obtain a depth map of the target area.</p>
<p id="p0039" num="0039">That is, binocular matching is performed on each set of images to acquire a disparity map corresponding to each set of images, and depth information of an area corresponding to each set of images is acquired according to the disparity map and the calibration parameters.</p>
<p id="p0040" num="0040">S3: Acquire a distribution of obstacles around the UAV according to the depth map.</p>
<p id="p0041" num="0041">Data processing is performed according to the depth map to obtain the distribution of obstacles around the UAV, where the distribution of obstacles is, for example, location information of the obstacles, the distances of the obstacles, a three-dimensional map of a surrounding environment of the UAV and the like.</p>
<p id="p0042" num="0042">S4: Acquire an execution time of step S1, an execution time of step S2 and an execution time of step S3 before executing step S1, step S2 and step S3.</p>
<p id="p0043" num="0043">The execution time of all of the steps may be preset by a designer based on the basic knowledge of the processing time of each step. Alternatively, step S1, step S2 and step S3 are first executed for trial operation in the controller for a period of time, and the controller detects the execution times of the steps to obtain the execution time of step S1, the execution time of step S2 and the execution time of step S3.</p>
<p id="p0044" num="0044">S5: Establish at least two threads and at least one ring queue according to the execution time of step S1, the execution time of step S2 and the execution time of step S3, and respectively execute step S1, step S2 and step S3 by the at least two threads to reduce a total execution time, where of two of the at least two threads that execute adjacent ones of the steps, a processing result from a thread executing a former step is transmitted to the ring queue, and a thread executing a latter step fetches the processing result from the ring queue and executes the latter step according to the processing result.</p>
<p id="p0045" num="0045">It is determined, according to the obtained execution time of step S1, the execution time of step S2 and the execution time of step S3, whether the at least two threads are adopted to execute step S1, step S2 and step S3 in parallel. The number of threads and ring queues may<!-- EPO <DP n="14"> --> be determined according to the execution times of the above steps, for example, there may be two threads and one ring queue or three threads and two ring queues.</p>
<p id="p0046" num="0046">In some embodiments, if the total execution time of step S1, step S2 and step S3 is relatively long (for example, greater than 1000/P, where P is the image frame rate at which the image collection apparatus collects the image), image blockage may occur. In order to avoid the image blockage, at least two threads can be established to execute step S1, step S2 and step S3 in parallel. If two adjacent ones of the steps are executed by different threads, the thread executing a former step may transmit a processing result to the ring queue, and the thread executing a latter step acquires the processing result from the ring queue.</p>
<p id="p0047" num="0047">For example, the first thread, the second thread and the first ring queue may be set. The first thread executes two of step S1, step S2 and step S3, and the second thread executes one of step S1, step S2 and step S3. Alternatively, the second thread executes two of step S1, step S2 and step S3, and the first thread executes one of step S1, step S2 and step S3.</p>
<p id="p0048" num="0048">For example, the first thread executes step S1 and step S2, and the second thread executes step S3. The first thread transmits the processing result of step S2 to the first ring queue, and the second thread acquires the processing result from the first ring queue and executes step S3. In some other embodiments, alternatively, the first thread executes step S1, and the second thread executes step S2 and step S3, and so on.</p>
<p id="p0049" num="0049">Assuming that the execution time of step S1 is t1, the execution time of step S2 is t2 and the execution time of step S3 is t3, if t1+t2+t3 &gt; 1000/P, t1+t2 &lt; 1000/P and t3 &lt; 1000/P, the first thread executes step S1 and step S2, and the second thread executes step S3. After the two threads execute step S1, step S2 and step S3 in parallel, the total execution time max(t1+t2, t3) &lt; 1000/P, which reduces the total execution time, effectively avoiding image blockage.</p>
<p id="p0050" num="0050">In some embodiments, if a thread executes two adjacent ones of the steps, and the total execution time of the two steps is still relatively large (for example, greater than 1000/P), in order to further avoid image blockage, the two steps may also be executed separately by two threads.<!-- EPO <DP n="15"> --></p>
<p id="p0051" num="0051">For example, the first thread executes steps S1 and S2, and the second thread executes step S3. If t1+t2 &gt; 1000/P and t3 &lt; 1000/P, then a third thread and a second ring queue can be established. The second thread executes step S1, the third thread executes step S2, the first thread transmits the processing result of step S1 to a second ring queue, and the third thread fetches the processing result from the second ring queue and executes step S2. In some other embodiments, alternatively, the third thread executes step S1, and the first thread executes step S2.</p>
<p id="p0052" num="0052">In some other embodiments, if the execution time of one of step S1, step S2 and step S3 is relatively long, for example, t3 &gt; 1000/P, two or more threads may be further adopted to execute step S3.</p>
<p id="p0053" num="0053">In practical application, a plurality of sets of binocular units are usually adopted for depth detection, and the image binocular matching takes a long time. In some embodiments, in order to increase the operating speed, the binocular matching may be performed by a hardware acceleration channel disposed in the controller. The hardware acceleration channel is an apparatus composed of hardware and interface software that can increase the operating speed of the software.</p>
<p id="p0054" num="0054">In some of the embodiments, a hardware acceleration channel may be used to increase the operating speed. Frames of a set of images consecutively captured by the at least two sets of binocular units are sequentially transmitted to the hardware acceleration channel, and each set of images are processed by performing time division multiplexing on the hardware acceleration channel, so as to obtain depth information corresponding to each set of images. That is, a first set of images are transmitted to the hardware acceleration channel for processing, and upon completion of the processing, a second set of images are transmitted to the hardware acceleration channel for processing, and so on. The hardware acceleration channel is multiplexed by polling.</p>
<p id="p0055" num="0055">In some other embodiments, at least two hardware acceleration channels may be used to increase the operating speed. In the set of images obtained by the binocular units, binocular matching processing is performed, by using a hardware acceleration channel, on the set of images obtained by the high-resolution binocular unit and the set of images obtained by the<!-- EPO <DP n="16"> --> low-resolution binocular unit, to further increase the operating speed.</p>
<p id="p0056" num="0056">In some other embodiments, the set of images obtained by one high-resolution binocular unit may also be processed by using a hardware acceleration channel, and the set of images obtained by at least two low-resolution binocular units (for example, two binocular units or three binocular units) are processed by using the same hardware acceleration channel. The set of images obtained by at least two low-resolution binocular units share a hardware acceleration channel, which can make full and reasonable use of the hardware acceleration channel without affecting the operating speed of the software when the number of hardware acceleration channels is small. When the set of images obtained by at least two binocular units share one hardware acceleration channel, the sets of target images may be processed by using a method of performing time division multiplexing on the hardware acceleration channel.</p>
<p id="p0057" num="0057">For example below, an implementation of a UAV includes two pairs of binocular units having a resolution of 720P and four pairs of binocular units having a resolution of VGA, and there are four hardware acceleration channels in the controller. During specific implementation, as shown in <figref idref="f0003">FIG. 4</figref>, a high-resolution 720P binocular unit may use one hardware acceleration channel alone, and every two binocular units in the low-resolution VGA binocular unit share one hardware acceleration channel. A correspondence between the binocular unit and the hardware acceleration channel may be set in advance, so that the set of images obtained by each binocular unit can be transmitted to the corresponding hardware channel for processing.</p>
<p id="p0058" num="0058">Correspondingly, as shown in <figref idref="f0003">FIG. 5</figref>, an embodiment of the present invention further provides an apparatus for processing a depth map. The apparatus is used in the controller of the UAV 100 shown in <figref idref="f0001">FIG. 1 or FIG. 2</figref>. In some of the embodiments, the controller may be the vision chip 20 of the UAV 100. As shown in <figref idref="f0003">FIG. 5</figref>, the apparatus 500 for processing a depth map includes:
<ul id="ul0019" list-style="none">
<li>an image correction module 501, configured to execute step S1 of correcting an image of a target area that is collected by the image collection apparatus;<!-- EPO <DP n="17"> --></li>
<li>a depth map acquisition module 502, configured to execute step S2 of performing binocular matching on the image, so as to obtain a depth map of the target area; and</li>
<li>an obstacle distribution acquisition module 503, configured to execute step S3 of acquiring a distribution of obstacles around the UAV according to the depth map.</li>
</ul></p>
<p id="p0059" num="0059">The apparatus further includes:
<ul id="ul0020" list-style="none">
<li>a time acquisition module 504, configured to acquire an execution time of step S1, an execution time of step S2 and an execution time of step S3 before the execution of step S1, step S2 and step S3; and</li>
<li>a thread and ring queue establishing module 505, configured to establish at least two threads and at least one ring queue according to the execution time of step S1, the execution time of step S2 and the execution time of step S3, and execute step S1, step S2 and step S3 by the at least two threads to reduce a total execution time, where</li>
<li>of two of the at least two threads that execute adjacent ones of the steps, a processing result from a thread executing a former step is transmitted to the ring queue, and a thread executing a latter step fetches the processing result from the ring queue and executes the latter step according to the processing result.</li>
</ul></p>
<p id="p0060" num="0060">In the embodiments of the present invention, at least two threads and at least one ring queue are established according to the execution times of all steps during depth map processing by the controller of the UAV, and the at least two threads execute all of the steps for processing the depth map. Each of the threads can obtain processing results from other threads through the at least one ring queue. Through the mode of the ring queue and parallel multi-thread running, the problem of image processing blockage is solved, and a delay is reduced.</p>
<p id="p0061" num="0061">In some embodiments of the apparatus 500 for processing a depth map, as shown in <figref idref="f0004">FIG. 6</figref>, the thread and ring queue establishing module 505 includes:
<ul id="ul0021" list-style="none">
<li>a determination submodule 5051, configured to determine whether a sum of the execution times of step S1, step S2 and step S3 meets a first preset condition; and<!-- EPO <DP n="18"> --></li>
<li>a thread and ring queue establishing submodule 5052, configured to establish a first thread, a second thread and a first ring queue, and execute step S1, step S2 and step S3 by the first thread and the second thread if the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets a first preset condition, where</li>
<li>of the first thread and the second thread, a processing result from a thread executing a former step is transmitted to the first ring queue, and a thread executing a latter step fetches the processing result from the first ring queue and executes the latter step according to the processing result.</li>
</ul></p>
<p id="p0062" num="0062">In some embodiments of the apparatus 500 for processing a depth map, the first preset condition is that:<br/>
the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 is greater than a preset value.</p>
<p id="p0063" num="0063">In some embodiments of the apparatus 500 for processing a depth map, the preset value is 1000/P, P being an image frame rate.</p>
<p id="p0064" num="0064">In some embodiments of the apparatus 500 for processing a depth map, the first thread executes two of step S1, step S2 and step S3, and the second thread executes one of step S1, step S2 and step S3.</p>
<p id="p0065" num="0065">The thread and ring queue establishing submodule 5052 is further configured to:
<ul id="ul0022" list-style="none">
<li>determine whether a sum of the execution times of the two steps executed by the first thread meets a second preset condition; and</li>
<li>if so, establish a third thread and a second ring queue, and execute the two steps respectively by the first thread and the third thread, where</li>
<li>of the first thread and the third thread, a processing result from a thread executing a former step is transmitted to the second ring queue, and a thread executing a latter step fetches the processing result from the second ring queue and executes the latter step according to the processing result.</li>
</ul></p>
<p id="p0066" num="0066">In some embodiments of the apparatus 500 for processing a depth map, the second preset<!-- EPO <DP n="19"> --> condition is that a sum of the execution times of the two steps executed by the first thread is greater than a preset value.</p>
<p id="p0067" num="0067">In some embodiments of the apparatus 500 for processing a depth map, the preset value is 1000/P, P being an image frame rate.</p>
<p id="p0068" num="0068">In some embodiments of the apparatus 500 for processing a depth map, the controller includes a hardware acceleration channel, and the image collection apparatus includes at least two sets of binocular units; and<br/>
the depth map acquisition module 502 is further configured to:
<ul id="ul0023" list-style="none">
<li>transmit images collected by the at least two sets of binocular units to the hardware acceleration channel, and perform, by performing time division multiplexing on the hardware acceleration channel, binocular matching on the images collected by the at least two sets of binocular units, to obtain the depth map.</li>
<li>In some embodiments of the apparatus 500 for processing a depth map, the hardware acceleration channel includes at least two hardware acceleration channels, and the images collected by the at least two sets of binocular units include a set of images having a first resolution and a set of images having a second resolution, the second resolution being greater than the first resolution; and</li>
<li>the depth map acquisition module 502 is further configured to:
<ul id="ul0024" list-style="none">
<li>transmit the set of images having the second resolution to one of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images; and</li>
<li>transmit the set of images having the first resolution to the other of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images.</li>
</ul></li>
</ul></p>
<p id="p0069" num="0069">In some embodiments of the apparatus 500 for processing a depth map, the hardware acceleration channel includes four hardware acceleration channels, i.e., a first hardware acceleration channel, a second hardware acceleration channel, a third hardware acceleration<!-- EPO <DP n="20"> --> channel and a fourth hardware acceleration channel, respectively;
<ul id="ul0025" list-style="none">
<li>the image collection apparatus includes six sets of binocular units, a set of images collected by four of the six sets of binocular units being the set of images having the first resolution, and a set of images collected by two of the six sets of binocular units being the set of images having the second resolution;</li>
<li>the depth map acquisition module 502 is further configured to:
<ul id="ul0026" list-style="none">
<li>transmit the set of images collected by the two sets of binocular units respectively to the first hardware acceleration channel and the second hardware acceleration channel; and</li>
<li>transmit a set of images collected by two of the four sets of binocular units to the third hardware acceleration channel, and transmit a set of images collected by the remaining two of the four sets of binocular units to the fourth hardware acceleration channel.</li>
</ul></li>
</ul></p>
<p id="p0070" num="0070">The foregoing apparatus may perform the method provided in the embodiments of this application, and has the corresponding functional modules for performing the method and beneficial effects thereof. For technical details not described in detail in the apparatus embodiment, reference may be made to the method provided in the embodiments of this application.</p>
<p id="p0071" num="0071"><figref idref="f0004">FIG. 7</figref> is a schematic structural diagram of hardware of a vision chip 20 in an embodiment of the UAV 100. As shown in <figref idref="f0004">FIG. 7</figref>, the vision chip 20 includes:<br/>
one or more processors 21 and a memory 22. One processor 21 is used as an example in <figref idref="f0004">FIG. 7</figref>.</p>
<p id="p0072" num="0072">The processor 21 and the memory 22 may be connected through a bus or in other manners and are, for example, connected through a bus in <figref idref="f0004">FIG. 7</figref>.</p>
<p id="p0073" num="0073">As a non-volatile computer-readable storage medium, the memory 22 may be configured to store a non-volatile software program, a non-volatile computer-executable program and module, for example, a program instruction/module (for example, the image correction module 501, the depth map acquisition module 502, the obstacle distribution acquisition module 503, the time acquisition module 504 and the thread and ring queue establishing<!-- EPO <DP n="21"> --> module 505 shown in <figref idref="f0003">FIG. 5</figref>) corresponding to the method for processing a depth map in the embodiments of the present application. The processor 21 executes various functional applications and data processing of the UAV by executing the non-volatile software program, instructions and the module stored in the memory 22, to implement the method for processing a depth map in the foregoing method embodiment.</p>
<p id="p0074" num="0074">The memory 22 may include a program storage area and a data storage area. The program storage area may store an operating system and an application program that is required by at least one function. The data storage area may store data created according to use of the vision chip, and the like. In addition, the memory 22 may include a high speed random access memory, and may also include a non-volatile memory such as at least one magnetic disk storage device, a flash memory, or another non-volatile solid-state storage device. In some embodiments, the memory 22 optionally includes memories remotely disposed relative to the processor 21, and these remote memories may be connected to the UAV by using a network. Examples of the network include, but are not limited to, the Internet, an intranet, a local area network, a mobile communication network, and a combination thereof.</p>
<p id="p0075" num="0075">The one or more modules are stored in the memory 22. When executed by the one or more processors 21, the method for processing a depth map in any of the above method embodiments is executed. For example, the above method steps 101-105 in <figref idref="f0002">FIG. 3</figref> are executed, and functions of the modules 501-505 in <figref idref="f0003">FIG. 5</figref> and the modules 501-505 and 5051-5052 in <figref idref="f0004">FIG. 6</figref> are implemented.</p>
<p id="p0076" num="0076">The foregoing product may perform the method provided in the embodiments of the present application, and have the corresponding functional modules for performing the method and beneficial effects thereof. For technical details not described in detail in this embodiment, refer to the method provided in the embodiments of the present application.</p>
<p id="p0077" num="0077">An embodiment of the present application provides a non-transitory computer-readable storage medium storing computer-executable instructions that are executed by one or more processors, for example, the processor 21 in <figref idref="f0004">FIG. 7</figref>, so that the one or more processors may execute the method for processing a depth map in any of the foregoing method embodiments, for example, execute step 101 to step 105 in the foregoing method in <figref idref="f0002">FIG. 3</figref>, and implement<!-- EPO <DP n="22"> --> functions of the modules 501-505 in <figref idref="f0003">FIG. 5</figref> and the modules 501-505 and 5051-5052 in <figref idref="f0004">FIG. 6</figref>.</p>
<p id="p0078" num="0078">The foregoing described device embodiments are merely examples. The units described as separate parts may or may not be physically separate, and the parts displayed as units may or may not be physical units, may be located in one position, or may be distributed on a plurality of network units. Some or all of the modules may be selected according to actual needs to achieve the objectives of the solutions of the embodiments.</p>
<p id="p0079" num="0079">Through the description of the foregoing embodiments, a person skilled in the art may clearly understand that the embodiments may be implemented by software in combination with a universal hardware platform, and may certainly be implemented by hardware. A person of ordinary skill in the art may understand that, all or some of the processes of the method in the foregoing embodiments may be implemented by a computer program instructing relevant hardware. The program may be stored in a computer-readable storage medium. During execution of the program, the processes of the foregoing method embodiments may be included. The foregoing storage medium may be a magnetic disk, an optical disc, a read-only memory (ROM), a random access memory (RAM), or the like.</p>
<p id="p0080" num="0080">Finally, it should be noted that the foregoing embodiments are merely used for describing the technical solutions of the present invention, but are not intended to limit the present invention. Under the concept of the present invention, the technical features in the foregoing embodiments or different embodiments may be combined, the steps may be implemented in any sequence, and there may be many other changes in different aspects of the present invention as described above. For brevity, those are not provided in detail. Although the present invention is described in detail with reference to the foregoing embodiments, a person of ordinary skill in the art should understand that they may still make modifications to the technical solutions described in the foregoing embodiments or make equivalent replacements to some technical features thereof, without departing from the scope of the technical solutions of the embodiments of the present invention.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="23"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A method for processing a depth map, applicable to a controller of an unmanned aerial vehicle (UAV), the UAV further comprising an image collection apparatus, the image collection apparatus being communicatively connected to the controller, <b>characterized in that</b> the method comprises the following steps:
<claim-text>S1: correcting an image of a target area that is collected by the image collection apparatus;</claim-text>
<claim-text>S2: performing binocular matching on the image to obtain a depth map of the target area; and</claim-text>
<claim-text>S3: acquiring a distribution of obstacles around the UAV according to the depth map; and</claim-text>
<claim-text>the method further comprises:
<claim-text>acquiring an execution time of step S1, an execution time of step S2 and an execution time of step S3 before executing step S1, step S2 and step S3; and</claim-text>
<claim-text>establishing at least two threads and at least one ring queue according to the execution time of step S1, the execution time of step S2 and the execution time of step S3, and executing step S1, step S2 and step S3 by the at least two threads to reduce a total execution time, wherein</claim-text>
<claim-text>of two of the at least two threads that execute adjacent ones of the steps, a processing result from a thread executing a former step is transmitted to the ring queue, and a thread executing a latter step fetches the processing result from the ring queue and executes the latter step according to the processing result.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The method according to claim 1, <b>characterized in that</b> the establishing at least two threads and at least one ring queue according to the execution time of step S1, the execution time of step S2 and the execution time of step S3, and executing step S1, step S2 and step S3 by the at least two threads to reduce a total execution time comprises:<!-- EPO <DP n="24"> -->
<claim-text>determining whether a sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets a first preset condition; and</claim-text>
<claim-text>establishing a first thread, a second thread and a first ring queue, and executing step S1, step S2 and step S3 by the first thread and the second thread if the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets the first preset condition, wherein</claim-text>
<claim-text>of the first thread and the second thread, a processing result from a thread executing a former step is transmitted to the first ring queue, and a thread executing a latter step fetches the processing result from the first ring queue and executes the latter step according to the processing result.</claim-text></claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The method according to claim 2, <b>characterized in that</b> the first preset condition is that:<br/>
the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 is greater than a preset value.</claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>The method according to claim 3, <b>characterized in that</b> the preset value is 1000/P, P being an image frame rate.</claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>The method according to any one of claims 2 to 4, <b>characterized in that</b> the first thread executes two of step S1, step S2 and step S3, and the second thread executes one of step S1, step S2 and step S3; and<br/>
the establishing a first thread, a second thread and a first ring queue, and executing step S1, step S2 and step S3 by the first thread and the second thread if the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets the first preset condition comprises:
<claim-text>determining whether a sum of the execution times of the two steps executed by the first thread meets a second preset condition; and</claim-text>
<claim-text>if so, establishing a third thread and a second ring queue, and executing the two steps respectively by the first thread and the third thread, wherein<!-- EPO <DP n="25"> --></claim-text>
<claim-text>of the first thread and the third thread, a processing result from a thread executing a former step is transmitted to the second ring queue, and a thread executing a latter step fetches the processing result from the second ring queue and executes the latter step according to the processing result.</claim-text></claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>The method according to claim 5, <b>characterized in that</b> the second preset condition is that a sum of the execution times of the two steps executed by the first thread is greater than a preset value.</claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>The method according to claim 6, <b>characterized in that</b> the preset value is 1000/P, P being an image frame rate.</claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>The method according to any one of claims 1 to 7, <b>characterized in that</b> the controller comprises a hardware acceleration channel, and the image collection apparatus comprises at least two sets of binocular units; and<br/>
the performing binocular matching on the image to obtain a depth map of the target area comprises:<br/>
transmitting images collected by the at least two sets of binocular units to the hardware acceleration channel, and performing, by performing time division multiplexing on the hardware acceleration channel, binocular matching on the images collected by the at least two sets of binocular units, to obtain the depth map.</claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>The method according to any one of claims 1 to 7, <b>characterized in that</b> the hardware acceleration channel comprises at least two hardware acceleration channels, and the images collected by the at least two sets of binocular units comprise a set of images having a first resolution and a set of images having a second resolution, the second resolution being greater than the first resolution; and<br/>
the performing binocular matching on the image to obtain a depth map of the target area comprises:
<claim-text>transmitting the set of images having the second resolution to one of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images; and<!-- EPO <DP n="26"> --></claim-text>
<claim-text>transmitting the set of images having the first resolution to the other of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images.</claim-text></claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>The method according to claim 9, <b>characterized in that</b> the hardware acceleration channel comprises four hardware acceleration channels, i.e., a first hardware acceleration channel, a second hardware acceleration channel, a third hardware acceleration channel and a fourth hardware acceleration channel, respectively;
<claim-text>the image collection apparatus comprises six sets of binocular units, a set of images collected by four of the six sets of binocular units being the set of images having the first resolution, and a set of images collected by two of the six sets of binocular units being the set of images having the second resolution;</claim-text>
<claim-text>the transmitting the set of images having the first resolution to the other of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images comprises:
<claim-text>transmitting the set of images collected by the two sets of binocular units respectively to the first hardware acceleration channel and the second hardware acceleration channel; and</claim-text>
<claim-text>the transmitting the set of images having the second resolution to one of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images comprises:<br/>
transmitting a set of images collected by two of the four sets of binocular units to the third hardware acceleration channel, and transmitting a set of images collected by the remaining two of the four sets of binocular units to the fourth hardware acceleration channel.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>An apparatus for processing a depth map, applicable to a controller of a UAV, the UAV further comprising an image collection apparatus, the image collection apparatus being communicatively connected to the controller, <b>characterized in that</b> the apparatus comprises:
<claim-text>an image correction module, configured to execute step S1 of correcting an image of a target area that is collected by the image collection apparatus;<!-- EPO <DP n="27"> --></claim-text>
<claim-text>a depth map acquisition module, configured to execute step S2 of performing binocular matching on the image to obtain a depth map of the target area; and</claim-text>
<claim-text>an obstacle distribution acquisition module, configured to execute step S3 of acquiring a distribution of obstacles around the UAV according to the depth map; and</claim-text>
<claim-text>the apparatus further comprises:
<claim-text>a time acquisition module, configured to acquire an execution time of step S1, an execution time of step S2 and an execution time of step S3 before the execution of step S1, step S2 and step S3; and</claim-text>
<claim-text>a thread and ring queue establishing module, configured to establish at least two threads and at least one ring queue according to the execution time of step S1, the execution time of step S2 and the execution time of step S3, and execute step S1, step S2 and step S3 by the at least two threads to reduce a total execution time, wherein</claim-text>
<claim-text>of two of the at least two threads that execute adjacent ones of the steps, a processing result from a thread executing a former step is transmitted to the ring queue, and a thread executing a latter step fetches the processing result from the ring queue and executes the latter step according to the processing result.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0012" num="0012">
<claim-text>The apparatus according to claim 11, <b>characterized in that</b> the thread and ring queue establishing module comprises:
<claim-text>a determination submodule, configured to determine whether a sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets a first preset condition; and</claim-text>
<claim-text>a thread and ring queue establishing submodule, configured to establish a first thread, a second thread and a first ring queue, and execute step S1, step S2 and step S3 by the first thread and the second thread if the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 meets a first preset condition, wherein</claim-text>
<claim-text>of the first thread and the second thread, a processing result from a thread executing a former step is transmitted to the first ring queue, and a thread executing a latter step fetches<!-- EPO <DP n="28"> --> the processing result from the first ring queue and executes the latter step according to the processing result.</claim-text></claim-text></claim>
<claim id="c-en-0013" num="0013">
<claim-text>The apparatus according to claim 12, <b>characterized in that</b> the first preset condition is that:<br/>
the sum of the execution time of step S1, the execution time of step S2 and the execution time of step S3 is greater than a preset value.</claim-text></claim>
<claim id="c-en-0014" num="0014">
<claim-text>The apparatus according to claim 13, <b>characterized in that</b> the preset value is 1000/P, P being an image frame rate.</claim-text></claim>
<claim id="c-en-0015" num="0015">
<claim-text>The apparatus according to any one of claims 12 to 14, <b>characterized in that</b> the first thread executes two of step S1, step S2 and step S3, and the second thread executes one of step S1, step S2 and step S3; and<br/>
the thread and ring queue establishing submodule is further configured to:
<claim-text>determine whether a sum of the execution times of the two steps executed by the first thread meets a second preset condition; and</claim-text>
<claim-text>if so, establish a third thread and a second ring queue, and execute the two steps respectively by the first thread and the third thread, wherein</claim-text>
<claim-text>of the first thread and the third thread, a processing result from a thread executing a former step is transmitted to the second ring queue, and a thread executing a latter step fetches the processing result from the second ring queue and executes the latter step according to the processing result.</claim-text></claim-text></claim>
<claim id="c-en-0016" num="0016">
<claim-text>The apparatus according to claim 15, <b>characterized in that</b> the second preset condition is that a sum of the execution times of the two steps executed by the first thread is greater than a preset value.</claim-text></claim>
<claim id="c-en-0017" num="0017">
<claim-text>The apparatus according to claim 16, <b>characterized in that</b> the preset value is 1000/P, P being an image frame rate.</claim-text></claim>
<claim id="c-en-0018" num="0018">
<claim-text>The apparatus according to any one of claims 11 to 17, <b>characterized in that</b> the controller comprises a hardware acceleration channel, and the image collection apparatus<!-- EPO <DP n="29"> --> comprises at least two sets of binocular units; and<br/>
the depth map acquisition module is further configured to:<br/>
transmit images collected by the at least two sets of binocular units to the hardware acceleration channel, and perform, by performing time division multiplexing on the hardware acceleration channel, binocular matching on the images collected by the at least two sets of binocular units, to obtain the depth map.</claim-text></claim>
<claim id="c-en-0019" num="0019">
<claim-text>The apparatus according to any one of claims 11 to 17, <b>characterized in that</b> the hardware acceleration channel comprises at least two hardware acceleration channels, and the images collected by the at least two sets of binocular units comprise a set of images having a first resolution and a set of images having a second resolution, the second resolution being greater than the first resolution; and<br/>
the depth map acquisition module is further configured to:
<claim-text>transmit the set of images having the second resolution to one of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images; and</claim-text>
<claim-text>transmit the set of images having the first resolution to the other of the at least two hardware acceleration channels for binocular matching, so as to obtain a depth map corresponding to the set of images.</claim-text></claim-text></claim>
<claim id="c-en-0020" num="0020">
<claim-text>The apparatus according to claim 19, <b>characterized in that</b> the hardware acceleration channel comprises four hardware acceleration channels, i.e., a first hardware acceleration channel, a second hardware acceleration channel, a third hardware acceleration channel and a fourth hardware acceleration channel, respectively;
<claim-text>the image collection apparatus comprises six sets of binocular units, a set of images collected by four of the six sets of binocular units being the set of images having the first resolution, and a set of images collected by two of the six sets of binocular units being the set of images having the second resolution;</claim-text>
<claim-text>the depth map acquisition module is further configured to:<!-- EPO <DP n="30"> -->
<claim-text>transmit the set of images collected by the two sets of binocular units respectively to the first hardware acceleration channel and the second hardware acceleration channel; and</claim-text>
<claim-text>transmit a set of images collected by two of the four sets of binocular units to the third hardware acceleration channel, and transmit a set of images collected by the remaining two of the four sets of binocular units to the fourth hardware acceleration channel.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0021" num="0021">
<claim-text>A UAV, <b>characterized by</b> comprising:
<claim-text>a fuselage;</claim-text>
<claim-text>a wing connected to the fuselage;</claim-text>
<claim-text>a power apparatus disposed on the wing;</claim-text>
<claim-text>an image collection apparatus disposed on the fuselage, the image collection apparatus being configured to acquire a target image of a target area of the UAV; and</claim-text>
<claim-text>a vision chip disposed in the fuselage, the vision chip being communicatively connected to the image collection apparatus, wherein</claim-text>
<claim-text>the vision chip comprises:
<claim-text>at least one processor; and</claim-text>
<claim-text>a memory communicatively connected to the at least one processor, the memory storing instructions executable by the at least one processor, the instructions, when executed by the at least one processor, causing the at least one processor to perform the method according to any one of claims 1 to 10.</claim-text></claim-text></claim-text></claim>
<claim id="c-en-0022" num="0022">
<claim-text>A non-volatile computer-readable storage medium, <b>characterized in that</b> the computer-readable storage medium stores computer-executable instructions which, when executed by a UAV, cause the UAV to perform the method according to any one of claims 1 to 10.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="31"> -->
<figure id="f0001" num="1,2"><img id="if0001" file="imgf0001.tif" wi="79" he="124" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="32"> -->
<figure id="f0002" num="3"><img id="if0002" file="imgf0002.tif" wi="97" he="131" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> -->
<figure id="f0003" num="4,5"><img id="if0003" file="imgf0003.tif" wi="99" he="214" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> -->
<figure id="f0004" num="6,7"><img id="if0004" file="imgf0004.tif" wi="115" he="188" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="151" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="151" he="233" type="tif"/><doc-page id="srep0003" file="srep0003.tif" wi="151" he="233" type="tif"/></search-report-data>
</ep-patent-document>
