<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP19905815A1" file="EP19905815NWA1.xml" lang="en" country="EP" doc-number="3890330" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3890330</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121><B121EP>published in accordance with Art. 153(4) EPC</B121EP></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>19905815.7</B210><B220><date>20191225</date></B220><B240><B241><date>20210701</date></B241></B240><B250>zh</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>201811601097</B310><B320><date>20181226</date></B320><B330><ctry>CN</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>H04N  21/43        20110101AFI20200703BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>H04N  21/472       20110101ALI20200703BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>H04N  21/436       20110101ALI20200703BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>H04N  13/332       20180101ALI20200703BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>H04N  21/258       20130101 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="2"><text>H04N  21/43        20130101 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="3"><text>H04N  21/436       20130101 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="4"><text>H04N  21/81        20130101 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="5"><text>H04N  21/478       20130101 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="6"><text>H04N  13/332       20180501 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="7"><text>H04N  21/472       20130101 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="8"><text>H04N  21/462       20130101 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="9"><text>H04N  21/2387      20130101 LI20200725BCEP        </text></classification-cpc><classification-cpc sequence="10"><text>H04N  21/442       20130101 LI20200725BCEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>VERFAHREN, VORRICHTUNG UND SYSTEM ZUR SYNCHRONEN VIDEOANZEIGE</B542><B541>en</B541><B542>METHOD, DEVICE  SYSTEM FOR SYNCHRONOUSLY DISPLAYING VIDEO</B542><B541>fr</B541><B542>PROCÉDÉ, DISPOSITIF ET SYSTÈME D'AFFICHAGE SYNCHRONE D'UNE VIDÉO</B542></B540><B590><B598>1</B598></B590></B500><B700><B710><B711><snm>Huawei Technologies Co., Ltd.</snm><iid>101903059</iid><irf>219EP 2065 DL</irf><adr><str>Huawei Administration Building, 
Bantian, Longgang District</str><city>Shenzhen Guangdong 518129</city><ctry>CN</ctry></adr></B711></B710><B720><B721><snm>ZHAO, Pingping</snm><adr><str>Huawei Administration Building Bantian, Longgang 
District</str><city>Shenzhen, Guangdong 518129</city><ctry>CN</ctry></adr></B721></B720><B740><B741><snm>Pfenning, Meinig &amp; Partner mbB</snm><iid>100060642</iid><adr><str>Patent- und Rechtsanwälte 
Theresienhöhe 11a</str><city>80339 München</city><ctry>DE</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP><B860><B861><dnum><anum>CN2019128118</anum></dnum><date>20191225</date></B861><B862>zh</B862></B860><B870><B871><dnum><pnum>WO2020135461</pnum></dnum><date>20200702</date><bnum>202027</bnum></B871></B870></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">This application provides a method for video synchronous display. The method includes: playing, by user equipment, a first video; receiving, by the user equipment, an indication of starting synchronous display with a target device; in response to the synchronous display indication, obtaining, by the user equipment, status information of the first video, where the video status information is used by the target device and the user equipment to synchronously play an image of the first video; and sending, by the user equipment, the video status information to the target device, so that the target device synchronously plays the image of the first video based on the video status information. According to the method of the present invention, the user equipment does not need to transmit image information of a video image being played to the target device. This reduces a data bandwidth requirement and implementation difficulty in a screen-sharing process between the user equipment and the target device, and achieves better compatibility.<img id="iaf01" file="imgaf001.tif" wi="80" he="81" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001"><b>TECHNICAL FIELD</b></heading>
<p id="p0001" num="0001">This application relates to the field of video technologies, and in particular, to a video synchronous display technology.</p>
<heading id="h0002"><b>BACKGROUND</b></heading>
<p id="p0002" num="0002">With rapid development of a multimedia technology and a network technology, digital video grows rapidly. A VR (Virtual Reality, virtual reality) technology is increasingly mature. The VR technology provides immersive experience for a user in a computer by comprehensively using a computer graphics system, various control means, and interface devices. The widespread use of the VR technology allows the VR technology to be increasingly popular with and experienced by ordinary consumers.</p>
<p id="p0003" num="0003">Currently, a main VR-type device in the market is a virtual reality head mounted display (Head Mount Display, HMD), which is referred to as a VR head mounted display or VR glasses for short, is a product integrating a plurality of technologies such as a simulation technology, a computer graphics man-machine interface technology, a multimedia technology, a sensing technology, and a network technology, and is a new man-machine interaction means by using the computer and the latest sensor technology.</p>
<p id="p0004" num="0004">The VR glasses and a target device share the same screen. A VR video image experienced by the user by using the VR glasses can be synchronously displayed on a display screen of the target device, so that the image seen by the user can be shared with others.</p>
<p id="p0005" num="0005">The VR device and the target device may be connected through a video connection cable, so that the image of the VR device is output on the target device. This is a screen-sharing solution currently used by most VR devices. In this solution, a video output port needs to be added on the VR device, and the video image is output to the target device in a wired manner. This manner requires a wired connection. Therefore, this solution cannot be used without a wire,<!-- EPO <DP n="2"> --> increasing VR device hardware costs.</p>
<p id="p0006" num="0006">Miracast is a Wi-Fi CERTIFIED Miracast™ certification program announced by the Wi-Fi Alliance on September 19, 2012. After the VR device and the target device are authenticated by Miracast, and when systems of the VR device and the target device support a Miracast function, screen mirroring between the VR device and the target device can be implemented according to this solution, to synchronously display the picture of the VR device on the target device in a wireless manner. This solution requires that the VR device and the screen-shared device be authenticated by Miracast. This needs to be supported by a system-level function of the device. Therefore, this solution has high limitations.</p>
<heading id="h0003"><b>SUMMARY</b></heading>
<p id="p0007" num="0007">This application provides a method, a device, and a system for video synchronous display, to implement video synchronous display through signaling, reduce a requirement on a transmission bandwidth between screen-sharing devices and implementation difficulty, and improve user experience.</p>
<p id="p0008" num="0008">According to a first aspect, a method for video synchronous display is provided. The method is applied to a user equipment side that plays screen-sharing video content. The method includes: User equipment plays a first video; the user equipment receives an indication of starting synchronous display with a target device; in response to the synchronous display indication, the user equipment obtains status information of the first video, where the video status information is used by the target device and the user equipment to synchronously play a picture of the first video; and the user equipment sends the video status information to the target device, so that the target device synchronously plays the picture of the first video based on the video status information.</p>
<p id="p0009" num="0009">In a possible design, the video status information includes video identification information and video playing progress information.</p>
<p id="p0010" num="0010">In a possible design, the video status information further includes user posture data. In this design, the user posture data can also be synchronously transmitted to the target device, to achieve a better virtual reality synchronization effect.</p>
<p id="p0011" num="0011">In a possible design, the user equipment performs suppression calculation on the user posture data, and sends the user posture data based on a suppression calculation result. This design<!-- EPO <DP n="3"> --> can reduce the sent user posture data, save network traffic, and avoid impact caused by a slight action on posture data transmission.</p>
<p id="p0012" num="0012">In a possible design, the user equipment selects, based on a network connection manner between the user equipment and the target device, a near field manner or a far field manner to send the video status information to the target device, to more intelligently and flexibly transmit the video synchronization information.</p>
<p id="p0013" num="0013">In a possible design, the user equipment encapsulates the video status information and sends the encapsulated video status information to the target device. This ensures transmission of the synchronization information and compatibility.</p>
<p id="p0014" num="0014">In a possible design, the user equipment, when detecting that a video playing status changes, re-obtains the video status information of the first video and sends the video status information of the first video to the target device.</p>
<p id="p0015" num="0015">In a possible design, the user equipment, when detecting a playing progress synchronization instruction or a playing progress correction instruction, collects video playing progress information and video playing speed information, and sends the video playing progress information and the video playing speed information to the target device.</p>
<p id="p0016" num="0016">According to a second aspect, a method for video synchronous display is provided. The method is applied to a target device end for screen-sharing display, and the method includes: A target device receives status information that is of a first video and that is sent by user equipment, where the status information of the first video is status information that is collected by the user equipment and that is of the first video being played on the user equipment; and the target device synchronously plays a picture of the first video based on the video status information.</p>
<p id="p0017" num="0017">In a possible design, the video status information includes video identification information and video playing progress information, and the target device finds a corresponding first video file based on the video identification information, and locates, based on the video playing progress information, a corresponding playing position.</p>
<p id="p0018" num="0018">In a possible design, the video status information further includes user posture data, and the target device restores, based on the user posture data, a playing picture that is of the first video and that is at the corresponding position.</p>
<p id="p0019" num="0019">In a possible design, the target device may further initiate a playing progress correction operation based on the received video playing progress information and a video playing speed<!-- EPO <DP n="4"> --> information. For example, the target device determines whether there is a deviation between current playing progress and the received video playing progress of the user equipment. For example, the target device may determine whether the deviation is greater than a preset threshold (for example, 2 seconds). If the target device determines that the progress has a deviation, the target device corrects the progress of the currently played video, in other words, corrects the progress to the received video playing progress value of the user equipment; and the target device adjusts a playing speed to a received playing speed of the user equipment. Certainly, if the target device determines that the playing speed is the same as the playing speed of the user equipment, the target device may not need to adjust the playing speed. In this design, a periodic correction operation mechanism may be configured on the user equipment or the target device, to ensure accuracy of display synchronization between the user equipment and the target device.</p>
<p id="p0020" num="0020">According to a third aspect, user equipment for video synchronous display is provided. The user equipment includes: a video playing module, a video status information obtaining module, and a video status information sending module. The video playing module is configured to play a first video. The video status information obtaining module is configured to obtain status information of the first video after receiving an indication of starting synchronous display with a target device, where the video status information is used for the target device and the user equipment to synchronously play a picture of the first video. The video status information sending module is configured to send the obtained video status information to the target device, so that the target device synchronously plays the picture of the first video based on the video status information.</p>
<p id="p0021" num="0021">According to a fourth aspect, a target device for video synchronous display is provided. The target device includes: a video status information receiving module and a synchronous video playing module. The video status information receiving module is configured to receive status information that is of a first video and that is sent by user equipment. The status information of the first video is status information that is collected by the user equipment and that is of the first video being played on the user equipment. The synchronous video playing module is configured to synchronously play a picture of the first video on the target device based on the video status information.</p>
<p id="p0022" num="0022">According to a fifth aspect, an embodiment of this application provides a chip, including a processor and a memory. The memory is configured to store a computer-executable<!-- EPO <DP n="5"> --> instruction, and the processor is connected to the memory. When the chip runs, the processor executes the computer-executable instruction stored in the memory, to enable the chip to perform any one of the foregoing methods for video synchronous display.</p>
<p id="p0023" num="0023">According to a sixth aspect, an embodiment of this application provides a computer storage medium. The computer storage medium stores an instruction, and when the instruction is run on a computer, the computer is enabled to perform any one of the foregoing methods for video synchronous display.</p>
<p id="p0024" num="0024">According to a seventh aspect, an embodiment of this application provides a computer program product. The computer program product includes an instruction, and when the instruction is run on a computer, the computer is enabled to perform any one of the foregoing methods for video synchronous display.</p>
<p id="p0025" num="0025">In addition, for technical effects brought by any design manner in the second to the seventh aspects, refer to technical effects brought by different design methods in the first aspect. Details are not described herein again.</p>
<p id="p0026" num="0026">This application further provides a system for video synchronous display, including the foregoing user equipment and the target device. These systems components implement the foregoing methods corresponding the foregoing aspects.</p>
<p id="p0027" num="0027">It may be understood that any one of the device, computer storage medium, computer program product, chip, or system for positioning a terminal that is provided above is configured to implement the corresponding method provided above. Therefore, for beneficial effects that can be achieved by the device, computer storage medium, computer program product, chip, or system for positioning the terminal, refer to the beneficial effects of the corresponding method. Details are not described herein again.</p>
<heading id="h0004"><b>BRIEF DESCRIPTION OF DRAWINGS</b></heading>
<p id="p0028" num="0028">To describe the technical solutions in the embodiments of this application more clearly, the following briefly describes the accompanying drawings for describing the embodiments.
<ul id="ul0001" list-style="none" compact="compact">
<li><figref idref="f0001">FIG. 1</figref> is a schematic architectural diagram of a system for video synchronous display according to this application;</li>
<li><figref idref="f0002">FIG. 2</figref> is a flowchart of an embodiment of initiating, by user equipment, video<!-- EPO <DP n="6"> --> synchronous display according to this application;</li>
<li><figref idref="f0003">FIG. 2a</figref> is a schematic diagram of calculating a head-turning action of a user according to a spherical linear direction formula according to this application;</li>
<li><figref idref="f0004">FIG. 3</figref> is a schematic diagram of implementing communication between user equipment and a target device by using a registration and binding mechanism according to this application;</li>
<li><figref idref="f0005">FIG. 4</figref> is a flowchart of a method for triggering video synchronous display when a video playing status changes according to this application;</li>
<li><figref idref="f0006">FIG. 5</figref> is a flowchart of a playing progress correction method in a video synchronous display process according to this application;</li>
<li><figref idref="f0007">FIG. 6</figref> is a possible schematic structural diagram of user equipment according to this application;</li>
<li><figref idref="f0008">FIG. 7</figref> is a possible schematic structural diagram of a target device according to this application; and</li>
<li><figref idref="f0008">FIG. 8</figref> is a possible schematic structural diagram of user equipment or a target device according to this application.</li>
</ul></p>
<heading id="h0005"><b>DESCRIPTION OF EMBODIMENTS</b></heading>
<p id="p0029" num="0029">To make the technical problems resolved, the technical solutions used, and the technical effects achieved in this application clearer, the following describes the technical solutions in this application with reference to the accompanying drawings in the embodiments. The detailed descriptions provide various embodiments of a device and/or a process by using block diagrams, flowcharts, and/or examples. These block diagrams, flowcharts, and/or examples include one or more functions and/or operations, so persons in the art may understand that each function and/or operation in the block diagrams, the flowcharts, and/or the examples may be performed independently and/or jointly by using much hardware, software, and firmware, and/or any combination thereof.</p>
<p id="p0030" num="0030">An embodiment of the present invention provides a schematic diagram of a system for video synchronous display. As shown in <figref idref="f0001">FIG. 1</figref>, the system includes user equipment 101, a target device 102, a video server 103 (optional), and a synchronization information transmission network<!-- EPO <DP n="7"> --> 104. The user equipment 101 is connected to the target device 102 through the synchronization information transmission network 104. The synchronization information transmission network 104 may include a registration server (near field scenario) and an instruction forwarding server (far field scenario). The user equipment 101 and/or the target device 102 may store playable video content, or the user equipment 101 and/or the target device 102 may obtain video content from the video server 103 to play the video content. The following further describes a network entity in the embodiments of the present invention.</p>
<p id="p0031" num="0031">The user equipment 101 is a video playing device that has a video playing function and a network connection function. The user equipment 101 is a device that is currently being watched by a user and that can be operated. The user equipment 101 may play the video content stored by the user equipment 101, or may obtain the video content by connecting to the video server 103 and play the video content. The user equipment 101 may collect status information of a video currently played by the user equipment 101, and send the status information to the target device 102. The video status information is information according to which the target device 102 can synchronously play a same video picture as the current user equipment 101, for example, the video status information is identification information of the video content and video playing progress information, and may further include user posture data (for example, HMD posture data) and other video attributes. The user equipment 101 may further determine a network relationship with the target device 102, and select a proper network path to transmit the video status information. The user equipment 101 may be a virtual reality (VR) device, for example, a head-mounted display (HDM). An optical signal or even a binocular stereoscopic visual signal is sent to eyes by using various head-mounted displays, to implement a virtual reality effect.</p>
<p id="p0032" num="0032">The target device 102 is a device to which a screen is shared (or video synchronous display), receives, through the synchronization information transmission network 104, the video status information sent by the user equipment 101, finds corresponding video content based on the video status information to restore the video image. There may be one or more target devices 102, and the plurality of target devices 102 may synchronously display, based on the same video status information, the image that is the same as that of the user equipment 101.</p>
<p id="p0033" num="0033">The video server 103 provides a video service for the user equipment 101 and/or the target device 102, and provides video content or a video playing service based on a video ID or address requested by the user equipment 101 or the target device 102.<!-- EPO <DP n="8"> --></p>
<p id="p0034" num="0034">The synchronous information transmission network 104 is a network path for the user equipment 101 to transmit media status information to the target device 102.</p>
<p id="p0035" num="0035">In the near field scenario (located in a same network segment), the registration server provides a registration mechanism for the user equipment 101 and the target device 102 to carry out a video synchronous service, so that the user equipment can discover the target device that performs screen-shared display, to establish a connection between the user equipment 101 and the target device 102, and to implement a screen-sharing function. In this case, the user equipment 101 and the target device 102 directly transmit media status information data through the synchronization information transmission network 104. Internal network-based communication methods include but are not limited to DLNA, WebService, JavaSocket, and WebSocket.</p>
<p id="p0036" num="0036">In the far field scenario, the user equipment may forward the media status information to the target device by using the instruction forwarding server. Deployment manners of the instruction forwarding server include but is not limited to protocol-based manners such as IM Server, XMPP Server, Socket Server or Web Service.</p>
<p id="p0037" num="0037">The system for video synchronous display provided in the embodiment of the present invention does not need to implement screen-sharing display between the user equipment and the target device by mirroring the video content, in other words, the user equipment does not need to transmit image information of the video image being played to the target device. This reduces a data bandwidth requirement between the user equipment and the target device in a screen-sharing process. Because the user equipment and the target device can implement screen-sharing display as long as the user equipment and the target device have a network communication function, no additional hardware cost is required. Therefore, the user equipment and the target device have very good device compatibility. In addition, for the user equipment that can play the VR video content, because there is no image mirroring relationship between the user equipment and the screen-shared device in the system in this embodiment of the present invention, the user equipment (for example, VR glasses) may provide, based on a device capability of the user equipment, a VR effect of binocular output for a user who uses the user equipment, and may implement normal monocular output on the target device, so that a user who watches the target device can have better experience.</p>
<p id="p0038" num="0038"><figref idref="f0002">FIG. 2</figref> is a flowchart of an embodiment in which user equipment initiates video synchronization display with a target device based on the system shown in <figref idref="f0001">FIG. 1</figref>.</p>
<p id="p0039" num="0039">S201: User equipment plays a video.<!-- EPO <DP n="9"> --></p>
<p id="p0040" num="0040">That the user equipment plays the video may be that the user equipment plays video content, for example, the user equipment opens various video files. Alternatively, a user starts to play a game and enters a game image, in other words, the user equipment plays a game video rendered in real time. The video image may be generated or changed based on an action of the user (a game player).</p>
<p id="p0041" num="0041">The video content played by the user equipment may be video content stored in the user equipment, or may be a video obtained from a video server (or a game server) and played. For example, the video server is a streaming server.</p>
<p id="p0042" num="0042">S202: The user equipment receives an indication of performing synchronous display with the target device.</p>
<p id="p0043" num="0043">The indication of performing synchronous display may be generated through an operation performed by the user on the user equipment (namely, a video playing device). For example, screen-sharing display with the target device is triggered by a click by the user in a video playing interface or a VR application interface. Alternatively, the indication may be sent by the target device to the user equipment to request to perform synchronous display with the user equipment, or may be a default configuration of the user equipment.</p>
<p id="p0044" num="0044">S203: In response to the indication of synchronous display, the user equipment obtains status information of the video being played, namely, video status information.</p>
<p id="p0045" num="0045">After determining that synchronous display (screen-sharing display) needs to be performed with the target device, the user equipment collects the video status information of the video being played. The video status information is information that may be sent to the target device and according to which the target device may synchronously play the video image that is the same as that of the current user equipment, but does not include the video image or the video content (for example, encoding and decoding data of the video image).</p>
<p id="p0046" num="0046">If the video display image currently played by the user equipment is irrelevant to a posture, an action, or a viewing angle of the user (in other words, the current posture, the action, the viewing angle, a moving speed, or the like of the user does not affect the video content currently displayed), the video status information includes identifier information of the video content and video playing progress information. The identification information of the video content is used to enable the target device to find the same video file, and the video playing progress information is used to enable the target device to position playing progress of the video file to the same playing<!-- EPO <DP n="10"> --> position as that of the user equipment, to ensure that the user equipment and the target device present the same video image (including the beginning of the video) to the outside. Further, if playing speeds of the video by the user equipment and the target device are inconsistent or may be inconsistent, the video status information further includes playing speed information of the video. Specifically, the video status information may include: a media ID of the video being played, a name of the video, an address of the video being played, a position of a time point of the video being played, a playing speed of the video, luminance, a resolution, volume, subtitle information, and a specific parameter type of the video status information. A quantity or the name may also have other representation forms, and is not limited in the embodiment of the present invention.</p>
<p id="p0047" num="0047">Further, if the video content or image played by the user equipment may be affected by the current posture, the action, or the viewing angle of the user, for example, if different video content or images played by the VR glasses may be present based on different viewing angles of the head of the user (which may generally correspond to HMD posture data of the user), the status information of the video being played may further include current posture information of the user, for example, the HMD posture data of the user.</p>
<p id="p0048" num="0048">The user equipment may collect the video status information only once when initiating synchronization. This can save processing resources and network bandwidths because many default configurations of the user equipment and the target device are the same, for example, default video playing speeds are the same. Alternatively, after initiating synchronization, the user equipment periodically collects the video status information until the screen is not shared. Alternatively, when initiating synchronization, the user equipment collects all video status information, and subsequently collects only a portion of specific video status information, for example, when initiating synchronization, the user equipment collects all video status information and sends all the video status information to the target device, and subsequently periodically collects and sends only user posture information or playing progress information. The user equipment may collect the current posture information of the user when each frame of the video is rendered, or may collect the current posture information of the user in another period which is set as required.</p>
<p id="p0049" num="0049">S204: The user equipment sends the video status information to the target device.</p>
<p id="p0050" num="0050">The user equipment and the target device may be located in a subnet in a same IP gateway, namely, in a same network segment (which may be referred to as a near field scenario).<!-- EPO <DP n="11"> --> In the near field scenario, the user equipment directly transmits the video status information to the target device through an internal network. Communication protocols used for internal network-based communication include but are not limited to DLNA, WebService, JavaSocket, and WebSocket.</p>
<p id="p0051" num="0051">The user equipment and the target device may be located in different network segments or even different networks (which may be referred to as a far field scenario). In the far field scenario, the user equipment may forward the video status information to the target device by using a forwarding server. Deployment manners of the forwarding server include but are not limited to IM Server, XMPP Server, Socket Server and Web Service.</p>
<p id="p0052" num="0052">After obtaining the video status information, the user equipment may select, based on the target device, pre-registered information, or a selection of the user, a manner of sending the video status information to the target device. For example, in the near field scenario, the user equipment directly sends an instruction to the target device through an internal network; or in the far field scenario, the user equipment forwards an instruction to the target device by using the forwarding server.</p>
<p id="p0053" num="0053">The user equipment may collect and send the video status information at the same time, in other words, after collecting the video status information, the user equipment sends the collected video status information immediately or as soon as possible. Alternatively, a sending period may be configured based on a network condition. For example, when the video status information is collected every one millisecond, the sending period of the video status information is adjusted to 10 milliseconds if the network condition is poor, in other words, the latest collected video status information is sent every 10 milliseconds, and then the video status information may be updated every millisecond if the network transmission condition is improved or the network is switched to a better network.</p>
<p id="p0054" num="0054">Similarly, the target device may also flexibly configure or adjust a receiving period and a processing period of the video status information based on the network condition or a device capability of the target device. Details are not described herein in this embodiment. This flexible configuration can ensure a synchronization effect and reduce an amount of data transmitted on the network.</p>
<p id="p0055" num="0055">When the user equipment collects the user posture information, to ensure smooth network data transmission between the user equipment and the target device, the user equipment<!-- EPO <DP n="12"> --> may perform suppression calculation before sending the collected posture data. A calculation method may be calculating a three-dimensional spatial included angle formed by postures of the user at two consecutive time points, and only when the included angle is greater than a threshold, the user posture data (for example, HDM posture data) is sent to the target device. This avoids impact caused by a slight change in a user posture on data, to achieve a balance between performance and an effect.</p>
<p id="p0056" num="0056">For example, an HMD posture synchronization suppression algorithm may be used for processing the HMD posture data.</p>
<p id="p0057" num="0057">The calculation method is: obtaining two pieces of continuous posture data (x1, y1, z1, w1) and (x2, y2, z2, w2) that are detected by an HMD sensor and that describe current watching directions of the user, and calculating a three-dimensional spatial included angle between the two pieces of posture data. A method for calculating the spatial included angle is as follows: <maths id="math0001" num=""><math display="block"><mi mathvariant="normal">θ</mi><mo>=</mo><mi>arccos</mi><mfenced><mfrac><mrow><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>y</mi><mn>1</mn></msub><msub><mi>y</mi><mn>2</mn></msub><mo>+</mo><msub><mi>z</mi><mn>1</mn></msub><msub><mi>z</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>w</mi><mn>2</mn></msub></mrow><mrow><msqrt><mrow><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>y</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>z</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>w</mi><mn>1</mn><mn>2</mn></msubsup></mrow></msqrt><mo>+</mo><msqrt><mrow><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>y</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>z</mi><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>w</mi><mn>2</mn><mn>2</mn></msubsup></mrow></msqrt></mrow></mfrac></mfenced></math><img id="ib0001" file="imgb0001.tif" wi="106" he="17" img-content="math" img-format="tif"/></maths></p>
<p id="p0058" num="0058">If the spatial included angle is less than the threshold, the HMD posture data is suppressed according to a viewing angle calculation formula, and the HMD posture data does not need to be sent. The HDM posture data is sent to the target device only when the included angle is greater than the threshold.</p>
<p id="p0059" num="0059">The user equipment may further encapsulate the obtained video status information and then send the encapsulated video status information, to facilitate network transmission or receiving or parsing of the target device. The user equipment may add an indication or identification information to the encapsulated information, to indicate the target device to perform a screen-sharing operation based on the video status information. Certainly, the target device may also be configured to detect a portion of video status information, and initiate the screen-sharing operation. In this case, the indication or the identification information is unnecessary. For a specific encapsulation form, refer to descriptions in the following embodiments.</p>
<p id="p0060" num="0060">S205: The target device plays the video based on the video status information.</p>
<p id="p0061" num="0061">Specifically, the target device may perform the following actions.</p>
<p id="p0062" num="0062">(Optional) The target device checks validity of the received video status information. For example, synchronous display is allowed only after the target device determines that the user<!-- EPO <DP n="13"> --> equipment or the video content is valid. The target device may perform processing based on a portion or all of the video status information and a setting.</p>
<p id="p0063" num="0063">The target device requests a corresponding video server (for example, a CDN server) to obtain corresponding video content based on an identifier (or an address) of the video. Alternatively, the target device finds, based on the identifier or address of the video, the corresponding video content in the video content stored by the target device.</p>
<p id="p0064" num="0064">The target device restores the playing progress of the video based on the playing progress information in the video status information.</p>
<p id="p0065" num="0065">The target device restores the playing speed of the video based on the playing speed information in the video status information (Optional, for example, the user equipment and the target device may play the video at a default speed).</p>
<p id="p0066" num="0066">Optionally, the target device may further restore a playing picture or a playing status of the video on the user equipment based on other information in the video status information, for example, parameters such as the luminance, the resolution, the volume, and the subtitle information.</p>
<p id="p0067" num="0067">Further, if the video status information further includes the current posture information of the user, the target device further restores the played video based on the posture information. For example, after receiving the HMD sensor posture data, the target device renders a corresponding video picture based on the posture data, to restore the video picture watched by the user on the user equipment (such as the VR glasses). A specific restoration manner may be performing interpolation calculation on the posture data, to calculate an intermediate status and restore a head-turning action of the user. The HMD posture data is used as an example. After receiving the HMD sensor posture data, a screen-shared device renders the video picture based on the posture data, to restore the video picture watched by the user on the user equipment. In this embodiment of the present invention, to more vividly restore the head-turning action of the user watching the picture, a spherical linear interpolation method is used between two pieces of continuous sensor data, to restore the head-turning action of the user. As shown in <figref idref="f0003">FIG. 2a</figref>, if the two pieces of adjacent quaternion data generated by the head-turning action of the user are P (x1, y1, z1, w1), and q (x2, y2, z2, w2), the target device obtains an intermediate point t, and restores a posture of the user based on a value of the intermediate point t. A formula of a spherical linear direction is as follows:<!-- EPO <DP n="14"> --> <maths id="math0002" num=""><math display="block"><mi mathvariant="italic">Slerp</mi><mfenced><mi mathvariant="bold">p</mi><mi mathvariant="bold">q</mi><mi>t</mi></mfenced><mo>=</mo><mfrac><mrow><mi>sin</mi><mfenced open="[" close="]" separators=""><mfenced separators=""><mn>1</mn><mo>−</mo><mi>t</mi></mfenced><mi>θ</mi></mfenced><mi mathvariant="bold">p</mi><mo>+</mo><mi>sin</mi><mi>t</mi><mi>θ</mi><mi mathvariant="bold">q</mi></mrow><mrow><mi>sin</mi><mi>θ</mi></mrow></mfrac></math><img id="ib0002" file="imgb0002.tif" wi="82" he="10" img-content="math" img-format="tif"/></maths></p>
<p id="p0068" num="0068">The spatial direction of the time point t is p(t) = Slerp(p ,q, t). Note: Spherical linear interpolation (spherical linear interpolation) is referred to as Slerp for short.</p>
<p id="p0069" num="0069">In the method for video synchronous display according to the embodiment of the present invention, screen-sharing display does not need to be implemented in a mirroring manner, in other words, information of the video picture being played does not need to be transmitted to the target device. This reduces a data bandwidth requirement in a screen-sharing process. As long as the user equipment and the target device have the network communication function, a screen can be shared, and no additional hardware cost is required. Therefore, the user equipment and the target device have very good device compatibility. In addition, for the user equipment that can play the VR video content, because there is no picture mirroring relationship between the user equipment and the screen-shared device in the method in this embodiment of the present invention, the user equipment (for example, the VR glasses) may provide, based on a device capability of the user equipment, a VR effect of binocular output for a user who uses the user equipment, and may implement normal monocular output on the target device, so that a user who watches the target device can have better experience. In this embodiment of the present invention, the user posture data is further processed according to the suppression algorithm, to avoid impact caused by a slight change in a user posture on data transmission, and to achieve a balance between performance and the effect.</p>
<p id="p0070" num="0070">To enable the user equipment to discover the target device that performs screen-sharing display, and to establish a connection between the user equipment and the target device and implement a screen-sharing function, the following manner may be used for implementation.</p>
<p id="p0071" num="0071">In an implementation, target device information, for example, an IP address of the target device, is pre-configured on the user equipment, and the target device is also configured to accept a screen-sharing request of a specific user equipment and complete the screen-sharing display function based on an instruction of the specific user equipment. In this way, after the user equipment and the target device are started, the user equipment may directly initiate the screen-sharing function to the preset target device. This manner is applicable to a relatively fixed combination between a screen-sharing device and a screen-shared device, and can more quickly implement the screen-sharing connection.<!-- EPO <DP n="15"> --></p>
<p id="p0072" num="0072">This may also be implemented through a registration or binding mechanism. Referring to <figref idref="f0004">FIG. 3</figref>, if the user equipment and the target device are located in the same network segment (internal network, near field scenario), the user equipment and the target device perform registration on the internal network in advance. An example in which communication between the user equipment and the target device is implemented based on a DLNA protocol is used. A registration mechanism may be as follows.
<ul id="ul0002" list-style="none" compact="compact">
<li>S301: User equipment, as a control end (DMC), performs registration on an internal network.</li>
<li>S302: A target device, as a playing end (DMP), is registered on the internal network.</li>
<li>S303: Before using a screen-sharing function, the user equipment queries an available target device on the internal network. A DLNA network protocol is used as an example. The user equipment is the DMC end defined in a DLNA protocol stack, and detects whether the DMP end applicable to screen-sharing display is on the network.</li>
<li>S304: After finding the available target device on the internal network, the user equipment selects a near field scenario as a network transmission mode.</li>
</ul></p>
<p id="p0073" num="0073">If the near field scenario is unavailable, the user equipment may switch the near field scenario to a far field scenario to transmit playing status information. However, before the user equipment and the screen-shared device (the target device) use the screen-sharing function in the far field scenario, a binding relationship needs to be established first, to pair the user equipment and the target device. The following uses deployment of an instruction forwarding server based on an XMPP server as an example:
<ul id="ul0003" list-style="none" compact="compact">
<li>S305: The user equipment and the target device need to set device IDs of the user equipment and the target device, and separately establish the binding relationship between the user equipment and the target device by using the XMPP Server, so that the user equipment and the target device can identify each other. The binding relationship may be established in advance or may be established immediately.</li>
<li>S306: The user equipment and the target device separately establish a connection to the instruction forwarding server. The example in which the deployment of the instruction forwarding server based on the XMPP server is used. The user equipment and the target device separately establish an XMPP connection to the XMPP server based on the device IDs of the user equipment and the target device, to ensure that an XMPP screen-sharing service is available.<!-- EPO <DP n="16"> --></li>
<li>S307: The user equipment selects the far field scenario as the network transmission mode to implement screen-sharing display with the target device.</li>
</ul></p>
<p id="p0074" num="0074">The user equipment may support the connection in the near field scenario and far field scenario at the same time, to facilitate flexible selection based on a network status. By using a registration or binding mechanism, the user equipment and the target device can be discovered and connected in the near field scenario or far field scenario, and the user equipment and the target device can flexibly configure a screen-sharing relationship.</p>
<p id="p0075" num="0075"><figref idref="f0005">FIG. 4</figref> shows a method for video synchronous display according to this application. There is a main difference between this embodiment and the embodiment shown in <figref idref="f0002">FIG. 2</figref>. In this embodiment, a change in a video playing status triggers the user equipment to re-collect a portion or all of video playing status information and send the re-collected information to the target device. Steps are as follows.</p>
<p id="p0076" num="0076">S401: User equipment detects that a video playing status changes.</p>
<p id="p0077" num="0077">A manner of triggering the change in the video playing status may be that the user performs an operation on the user equipment to trigger the change in the video playing status. The manner of triggering the operation varies with a design of a video playing interface (for example, a VR application interface), for example, the manner of triggering the operation may be pressing, dragging, clicking, or even voice activation. The change in the video playing status may be: a pause, a progress change, a speed change (the user adjusts a playing speed), a replay, and the like. The video playing status may also be changed in a non-human manner, for example, the video is suddenly interrupted, or an advertisement is inserted in the video (except video content).</p>
<p id="p0078" num="0078">S402: The user equipment collects and transmits status information of a currently played video.</p>
<p id="p0079" num="0079">The re-collected video status information herein may be the same as or different from the video status information collected in the step S203. For example, the re-collected video status information may include only changed playing status information, for example, include only playing progress information. For a method for collecting and transmitting the video status information, refer to the steps S203 and S204. Details are not described herein again.</p>
<p id="p0080" num="0080">S403: The target device plays the video based on the video status information sent by the user equipment.</p>
<p id="p0081" num="0081">For details, refer to the foregoing step S205. Details are not described herein again. It<!-- EPO <DP n="17"> --> should be noted that the target device may determine a difference between the video status information received this time and the video status information received last time (the target device buffers the video status information received last time), and perform processing only based on a parameter that changes.</p>
<p id="p0082" num="0082">In this implementation, a proper screen-sharing response can be made to various playing status changes on the user equipment, to implement a more accurate screen-sharing effect.</p>
<p id="p0083" num="0083"><figref idref="f0006">FIG. 5</figref> shows a method for correcting playing progress in a video synchronous display process according to this application. There is a difference between this embodiment and the embodiment shown in <figref idref="f0002">FIG. 2</figref> or <figref idref="f0005">FIG. 4</figref>. In this embodiment, the target device may have a deviation between playing progress of the target device and current playing progress of the user equipment due to various reasons. The reason may be that: The user equipment does not send playing speed information to the target device after triggering synchronization with the target device; or even if the target device receives playing speed information, the target device does not perform a playing operation based on the playing speed information. Therefore, there is a subsequent playing progress deviation caused by different playing speeds. The reason may also be that the video playing status on the user equipment changes, but is not sensed by the target device. Therefore, a mechanism is required to correct inconsistent playing progress between the user equipment and the target device.</p>
<p id="p0084" num="0084">S501: User equipment collects video playing progress information and video playing speed information.</p>
<p id="p0085" num="0085">A mechanism for periodically performing a correction operation may be configured on the user equipment or the target device. For example, after video synchronization is initiated, the correction operation is triggered every one minute, to trigger collection of the progress information and the speed information (or only collection of the progress information). Alternatively, a dedicated button can be set in an interface for a user to trigger synchronization correction.</p>
<p id="p0086" num="0086">S502: The user equipment sends the video playing progress information and the video playing speed information to the target device.</p>
<p id="p0087" num="0087">After collecting the playing progress information and the playing speed information, the user equipment may further send an indication or identification information to the target device, to indicate the target device to correct the playing progress based on the playing progress information and the playing speed information.<!-- EPO <DP n="18"> --></p>
<p id="p0088" num="0088">S503: The target device initiates a playing progress correction operation based on the received video playing progress information and video playing speed information.</p>
<p id="p0089" num="0089">The target device may be configured to initiate the playing progress correction operation as long as the video playing progress information and the video playing speed information are received, or initiate the playing progress correction based on the indication or identifier information that is used for performing the playing progress correction and that is included in the received information.</p>
<p id="p0090" num="0090">The playing progress correction operation may specifically include the following steps.
<ul id="ul0004" list-style="none" compact="compact">
<li>S5031: The target device determines whether there is a deviation between the current playing progress and the received video playing progress of the user equipment, for example, the target device may determine whether the deviation is greater than a preset threshold (for example, 2 seconds).</li>
<li>S5032: If determining that the progress has the deviation, the target device corrects the progress of the currently played video, to be specific, corrects the progress to a received video playing progress value of the user equipment.</li>
<li>S5033: The target device adjusts the playing speed to the received playing speed of the user equipment. Certainly, if determining that the playing speed is the same as the playing speed of the user equipment, the target device may not need to adjust the playing speed.</li>
</ul></p>
<p id="p0091" num="0091">In the embodiments of the present invention, the periodic correction operation mechanism may be configured on the user equipment or the target device, to ensure accuracy of display synchronization between the user equipment and the target device.</p>
<p id="p0092" num="0092">The following describes a manner of encapsulating the video status information. In this embodiment of the present invention. In a near field (a DLNA network transmission mode) and a far field (an XMPP network transmission mode), the video status information may be transmitted by using a same encapsulation method. The video status information is encapsulated in the following scenarios.</p>
<p id="p0093" num="0093">In the embodiment for starting video synchronous display described in <figref idref="f0002">FIG. 2</figref>, the video status information may be encapsulated by using the following instruction format, as shown in Table 1.<!-- EPO <DP n="19"> -->
<tables id="tabl0001" num="0001">
<table frame="all">
<title><b>Table 1</b></title>
<tgroup cols="2">
<colspec colnum="1" colname="col1" colwidth="30mm"/>
<colspec colnum="2" colname="col2" colwidth="136mm"/>
<thead>
<row>
<entry valign="top">Parameter</entry>
<entry valign="top">Parameter description</entry></row></thead>
<tbody>
<row>
<entry>functionType</entry>
<entry>Action that the screen-shared device is indicated to perform Starting video synchronous display: startPlay</entry></row>
<row>
<entry>mediaCode</entry>
<entry>Unique identifier of media</entry></row>
<row rowsep="0">
<entry morerows="2" rowsep="1">playByBookmark</entry>
<entry>Whether the screen-shared device is indicated to play the video from a bookmark position</entry></row>
<row rowsep="0">
<entry>1: yes</entry></row>
<row>
<entry>0: no</entry></row>
<row>
<entry>playByTime</entry>
<entry>Time point at which the screen-shared device is indicated to play the video from the bookmark position</entry></row>
<row>
<entry>playUrl</entry>
<entry>URL that can be directly played on the screen-shared device</entry></row>
<row rowsep="0">
<entry morerows="1" rowsep="1">isVR</entry>
<entry>Indicating the screen-shared device to enter a VR screen-sharing status</entry></row>
<row>
<entry>1: enter the VR status</entry></row></tbody></tgroup>
</table>
</tables></p>
<p id="p0094" num="0094">An example of instruction encapsulation is as follows:<br/>
{"action": "functionCall", "functionType": "startPlay", "mediaCode": "2", "playByBookMark": "1", "playByTime": "180", "playUrl": "", "isVR": "1"}</p>
<p id="p0095" num="0095">The following instruction encapsulation includes an indication startPlay of starting a screen-sharing operation on the target device. If the target device is configured to start the screen-sharing operation immediately after receiving media status information (mediaCode, playByBookmark, playByTime, and playUrl in Table 1), the screen-sharing operation indication may not need to be included. It can be learned from the foregoing encapsulation example that, the video status information sent by the user equipment includes the media identifier information that mediacode is 2 and the playing progress information that the playing time point is 180. It may be further learned that, if the user equipment is a VR device, for example, the user equipment may collect user posture information (for example, HMD posture information), the encapsulated media status information further includes an indication "1: Enter the VR status" that indicates the target device to enter the VR screen-sharing status. This indicates that the target device needs to perform processing such as rendering on the played video picture based on the user posture information in<!-- EPO <DP n="20"> --> the video status information.</p>
<p id="p0096" num="0096">In the embodiment of the screen-sharing operation triggered by the change of the video playing status described in <figref idref="f0004">FIG. 3</figref>, the video status information may be encapsulated in the following instruction format, as shown in Table 2.
<tables id="tabl0002" num="0002">
<table frame="all">
<title>Table 2</title>
<tgroup cols="2">
<colspec colnum="1" colname="col1" colwidth="25mm"/>
<colspec colnum="2" colname="col2" colwidth="141mm"/>
<thead>
<row>
<entry valign="top">Parameter</entry>
<entry valign="top">Parameter description</entry></row></thead>
<tbody>
<row rowsep="0">
<entry>functionType</entry>
<entry>Action that the screen-shared device is indicated to perform</entry></row>
<row>
<entry/>
<entry>Playing status change: trickPlayControl</entry></row>
<row rowsep="0">
<entry>trickplayMode</entry>
<entry>Playing status at which the screen-shared device is indicated to be</entry></row>
<row rowsep="0">
<entry/>
<entry>0: play</entry></row>
<row rowsep="0">
<entry/>
<entry>1: suspended</entry></row>
<row rowsep="0">
<entry/>
<entry>2: fast-forward or rewind</entry></row>
<row rowsep="0">
<entry/>
<entry>3: seek</entry></row>
<row rowsep="0">
<entry/>
<entry>4: one-click to the beginning</entry></row>
<row rowsep="0">
<entry/>
<entry>5: one-click to the end</entry></row>
<row>
<entry/>
<entry>10: exit the playing</entry></row>
<row>
<entry>seekPostion</entry>
<entry>Playing position that is needed to be sought</entry></row>
<row>
<entry>fastSpeed</entry>
<entry>Fast-forward or rewind speed, which is needed only when the trick play status is 2 or 3 Value range: -32/-16/-8/-4/-2/2/4/8/16/32</entry></row>
<row rowsep="0">
<entry>isVR</entry>
<entry>Indicating the screen-shared device to enter a VR screen-sharing status</entry></row>
<row>
<entry/>
<entry>1: enter the VR status</entry></row></tbody></tgroup>
</table>
</tables></p>
<p id="p0097" num="0097">An example of instruction encapsulation is as follows:<br/>
{"action": "functionCall", " functionType": " trickPlayControl", " trickPlayMode": 2, " fastSpeed": 8}</p>
<p id="p0098" num="0098">It can be learned from the encapsulation example in Table 2 that, the playing status of the user equipment changes. "2: fast-forward or rewind" occurs, and the fast-forward speed is 8x.</p>
<p id="p0099" num="0099">In the embodiment of the method for correcting the playing progress in the video synchronous display process in <figref idref="f0005">FIG. 4</figref>, the video status information may be encapsulated in the following instruction format, as shown in Table 3.<!-- EPO <DP n="21"> -->
<tables id="tabl0003" num="0003">
<table frame="all">
<title><b>Table 3</b></title>
<tgroup cols="2">
<colspec colnum="1" colname="col1" colwidth="25mm"/>
<colspec colnum="2" colname="col2" colwidth="141mm"/>
<thead>
<row>
<entry valign="top">Parameter</entry>
<entry valign="top">Parameter description</entry></row></thead>
<tbody>
<row rowsep="0">
<entry>functionType</entry>
<entry>Action that the screen-shared device is indicated to perform</entry></row>
<row>
<entry/>
<entry>Playing status change: trickPlayControl</entry></row>
<row rowsep="0">
<entry>trickplayMode</entry>
<entry>Playing status at which the screen-shared device is indicated to be</entry></row>
<row rowsep="0">
<entry/>
<entry>0: play</entry></row>
<row rowsep="0">
<entry/>
<entry>1: suspended</entry></row>
<row rowsep="0">
<entry/>
<entry>2: fast-forward or rewind</entry></row>
<row rowsep="0">
<entry/>
<entry>3: seek</entry></row>
<row rowsep="0">
<entry/>
<entry>4: one-click to the beginning</entry></row>
<row rowsep="0">
<entry/>
<entry>5: one-click to the end</entry></row>
<row rowsep="0">
<entry/>
<entry>10: exit the playing</entry></row>
<row>
<entry/>
<entry>20: synchronous HMD posture</entry></row>
<row>
<entry>seekPostion</entry>
<entry>Playing position that is needed to be sought</entry></row>
<row>
<entry>fastSpeed</entry>
<entry>Fast-forward or rewind speed, which is needed only when the trick play status is 2 or 3 Value range: -32/-16/-8/-4/-2/2/4/8/16/32</entry></row>
<row rowsep="0">
<entry>isVR</entry>
<entry>Indicating the screen-shared device to enter a VR screen-sharing status</entry></row>
<row>
<entry/>
<entry>1: enter the VR status</entry></row></tbody></tgroup>
</table>
</tables></p>
<p id="p0100" num="0100">An example of instruction encapsulation is as follows:<br/>
{"action": "functionCall", " functionType": " trickPlayControl", " trickPlayMode": 2, 3, " seekPostion": "150", " fastSpeed": 2}</p>
<p id="p0101" num="0101">It can be learned from the encapsulation example in Table 3 that the user equipment triggers playing progress correction. The current playing progress is 150, and the playing speed is 2x (certainly, may also be a specific speed value). After receiving the information, the target device may correspondingly correct the playing progress.</p>
<p id="p0102" num="0102">In all embodiments of the present invention, the user equipment may further collect the user posture data. For example, the user equipment collects HMD posture data from the VR device, and also sends the user posture data to the target device. The target device processes a corresponding video display image based on the posture data. The encapsulation of the HMD<!-- EPO <DP n="22"> --> posture data may be shown in the following Table 4.
<tables id="tabl0004" num="0004">
<table frame="all">
<title><b>Table 4</b></title>
<tgroup cols="2">
<colspec colnum="1" colname="col1" colwidth="25mm"/>
<colspec colnum="2" colname="col2" colwidth="107mm"/>
<thead>
<row>
<entry valign="top">Parameter</entry>
<entry valign="top">Parameter description</entry></row></thead>
<tbody>
<row rowsep="0">
<entry>functionType</entry>
<entry>Action that the screen-shared device is indicated to perform</entry></row>
<row>
<entry/>
<entry>Playing status change: trickPlayControl</entry></row>
<row rowsep="0">
<entry>trickplayMode</entry>
<entry>Playing status at which the screen-shared device is indicated to be</entry></row>
<row>
<entry/>
<entry>20: synchronous HMD posture</entry></row>
<row rowsep="0">
<entry>isVR</entry>
<entry>Indicating the screen-shared device to enter a VR screen-sharing status</entry></row>
<row>
<entry/>
<entry>1: enter the VR status</entry></row>
<row rowsep="0">
<entry>Quaternion</entry>
<entry>HMD head mounted posture data, which is valid when trickplayMode is 20</entry></row>
<row>
<entry/>
<entry>Value format: x,y,z,w</entry></row></tbody></tgroup>
</table>
</tables></p>
<p id="p0103" num="0103">An example of instruction encapsulation is as follows:<br/>
{"action": "functionCall", "functionType": "trickPlayControl", "trickPlayMode": 20, isVR, "1", "quaternion": "0, 0, 0, 1"}</p>
<p id="p0104" num="0104">It can be learned from the encapsulation example in Table 4 that the user equipment collects user posture information (for example, HMD information). The encapsulated media status information includes an indication for indicating the target device to enter the VR screen-sharing status (certainly, if the target device is already in the VR status by default, no special indication is required), and further includes a specific four-tuple value of the HMD posture data: "0, 0, 0, 1".</p>
<p id="p0105" num="0105">Table 1-4 describes an encapsulation form or format of the video status information from perspectives of different embodiments. It may be understood that, during actual encapsulation, meanings, names, and quantities of specific parameters in Table 1-4 may be adjusted based on an actual requirement. For example, only some parameters may be encapsulated, a collection of all parameters in Table 1-4 may be encapsulated, or more parameters may be encapsulated. A plurality of parameters may also be bound through encapsulation, for example, the posture data is bound to the playing progress, to ensure accuracy of the posture synchronization image.</p>
<p id="p0106" num="0106">The foregoing mainly describes the solutions provided in the embodiments of this application from a perspective of interaction between the user equipment and the target device. It<!-- EPO <DP n="23"> --> may be understood that to implement the foregoing functions, the user equipment and the target device include corresponding hardware structures and/or software modules for performing the functions. A person skilled in the art should easily be aware that, in combination with units and algorithm steps of the examples described in the embodiments disclosed in this specification, this application may be implemented by hardware or a combination of hardware and computer software. Whether a function is performed by hardware or hardware driven by computer software depends on particular applications and design constraints of the technical solutions. A person skilled in the art may use different methods to implement the described functions for each particular application, but it should not be considered that the implementation goes beyond the scope of this application.</p>
<p id="p0107" num="0107">In the embodiments of this application, the user equipment and the target device may be divided into functional modules based on the foregoing method embodiments. For example, each functional module may be obtained through division based on each corresponding function, or two or more functions may be integrated into one processing module. The integrated module may be implemented in a form of hardware, or may be implemented in a form of a software functional module. It should be noted that, in this embodiment of this application, division into the modules is an example, and is merely a logical function division. In actual implementation, another division manner may be used. The following uses an example in which each functional module is obtained through division based on each corresponding function for description.</p>
<p id="p0108" num="0108"><figref idref="f0007">FIG. 6</figref> is a possible schematic structural diagram of the user equipment 600 in the foregoing embodiment. The user equipment 600 includes a video playing module 601, a video status information obtaining module 602, and a video status information sending module 603.</p>
<p id="p0109" num="0109">The video playing module 601 is configured to play a first video. The video status information obtaining module 602 is configured to obtain status information of the first video after receiving an indication of starting synchronous display with a target device, where the video status information is used for the target device and the user equipment to synchronously play an image of the first video. The video status information sending module 603 is configured to send the obtained video status information to the target device, so that the target device synchronously plays the image of the first video based on the video status information.</p>
<p id="p0110" num="0110">Optionally, the video status information includes video identification information and video playing progress information.<!-- EPO <DP n="24"> --></p>
<p id="p0111" num="0111">Optionally, the video status information further includes user posture data.</p>
<p id="p0112" num="0112">Optionally, that the video status information sending module is configured to send the video status information to the target device further includes that: the video status information sending module is configured to perform suppression calculation on the user posture data, and send the user posture data based on a suppression calculation result.</p>
<p id="p0113" num="0113">Optionally, the video status information sending module selects, based on a network connection manner between the user equipment and the target device, a near field manner or a far field manner to send the video status information to the target device.</p>
<p id="p0114" num="0114">Optionally, the video status information sending module is further configured to encapsulate the video status information and send the encapsulated video status information to the target device.</p>
<p id="p0115" num="0115">Optionally, the video status information obtaining module is further configured to, when detecting that the video play status changes, re-obtain the video status information of the first video, and send the re-obtained video status information to the target device.</p>
<p id="p0116" num="0116">Optionally, the video status information obtaining module is further configured to, when detecting a playing progress synchronization instruction or a playing progress correction instruction, collect video playing progress information and video playing speed information, send the video playing progress information and the video playing speed information to the target device.</p>
<p id="p0117" num="0117"><figref idref="f0008">FIG. 7</figref> is a possible schematic structural diagram of a target device 700 in the foregoing embodiments. The target device 700 includes a video status information receiving module 701 and a synchronous video playing module 702. The video status information receiving module 701 is configured to receive status information that is of a first video and that is sent by user equipment. The status information of the first video is status information that is collected by the user equipment and that is of the first video being played on the user equipment. The synchronous video playing module 702 is configured to synchronously play an image of the first video on the target device based on the video status information.</p>
<p id="p0118" num="0118">Optionally, the video status information includes video identification information and video playing progress information. The synchronous video playing module 702 is further configured to: find a first video file based on the video identification information, locate a corresponding playing position based on the video playing progress information, and play the first video file.<!-- EPO <DP n="25"> --></p>
<p id="p0119" num="0119">Optionally, the video status information further includes user posture data, and the synchronous video playing module 702 is further configured to restore, based on the user posture data, the image of the first video at the corresponding position.</p>
<p id="p0120" num="0120">All related content in all the foregoing embodiments may be referenced to function descriptions of functional modules corresponding to the user equipment and the target device. Details are not described herein again. Because the user equipment and the target device provided in the embodiments of this application are configured to perform the methods in all the foregoing embodiments, for technical effects that can be achieved by the user equipment and the target device, refer to the foregoing method embodiments. Details are not described herein again.</p>
<p id="p0121" num="0121">The "module" in <figref idref="f0007">FIG. 6</figref> and <figref idref="f0008">FIG. 7</figref> may be an application specific integrated circuit (Application Specific Integrated Circuit, ASIC), an electronic circuit, a processor that executes one or more software or firmware programs, a memory, a combinational logic circuit, and another component that provides the foregoing functions. When the integrated unit or module is implemented in the form of a software functional unit and sold or used as an independent product, the integrated unit may be stored in a computer-readable storage medium.</p>
<p id="p0122" num="0122"><figref idref="f0008">FIG. 8</figref> is another schematic structural diagram of user equipment or target device according to an embodiment of this application. The structure includes a processor 801, a memory 802, a transceiver 803, and a display 804. The processor 801 is connected to the memory 802 and the transceiver 803. For example, the processor 801 may be connected to the memory 802 and the transceiver 803 by using a bus.</p>
<p id="p0123" num="0123">The processor 801 may be configured to perform a corresponding function in the foregoing embodiment by the user equipment, or may be configured to support the target device in performing a corresponding function in the foregoing embodiment. The processor 801 may be a central processing unit (English: central processing unit, CPU), a network processor (English: network processor, NP), a hardware chip, or any combination thereof. The hardware chip may be an application-specific integrated circuit (English: application-specific integrated circuit, ASIC), a programmable logic device (English: programmable logic device, PLD), or a combination thereof. The PLD may be a complex programmable logic device (English: complex programmable logic device, CPLD), a field programmable gate array (English: field-programmable gate array, FPGA), a generic array logic (English: generic array logic, GAL), or any combination thereof.</p>
<p id="p0124" num="0124">The memory 802 is configured to store program code and the like. The memory 802<!-- EPO <DP n="26"> --> may include a volatile memory (English: volatile memory), for example, a random access memory (English: random access memory, RAM for short). The memory 502 may also include a nonvolatile memory (English: non-volatile memory), for example, a read-only memory (English: read-only memory, ROM for short), a flash memory (English: flash memory), a hard disk (English: hard disk drive, HDD for short), or a solid-state drive (English: solid-state drive, SSD for short). The memory 802 may further include a combination of memories of the foregoing types.</p>
<p id="p0125" num="0125">The transceiver 803 may be a communications module or a transceiver circuit, and is configured to transmit information such as data and signaling between the user equipment and the target device, or between the user equipment and the target device and another network unit such as a video server in the foregoing embodiments.</p>
<p id="p0126" num="0126">The processor 801 may invoke the program code to perform operations in the method embodiments shown in <figref idref="f0002 f0003 f0004 f0005 f0006">FIG. 2 to FIG. 5</figref>.</p>
<p id="p0127" num="0127">All or some of the foregoing embodiments may be implemented by using software, hardware, firmware, or any combination thereof. When software is used to implement the embodiments, the embodiments may be implemented completely or partially in a form of a computer program product. The computer program product includes one or more computer instructions. When the computer program instructions are loaded and executed on the computer, the procedure or functions according to the embodiments of the present invention are all or partially generated. The computer may be a general-purpose computer, a dedicated computer, a computer network, or other programmable apparatuses. The computer instruction may be stored in a computer-readable storage medium, or may be transmitted by using the computer-readable storage medium. The computer instructions may be transmitted from a website, computer, server, or data center to another website, computer, server, or data center in a wired (for example, a coaxial cable, an optical fiber, or a digital subscriber line (DSL)) or wireless (for example, infrared, radio, or microwave) manner. The computer-readable storage medium may be any available medium accessible to the computer. For example, the computer instruction may be stored or transmitted by using a magnetic medium (for example, a floppy disk, a hard disk, or a magnetic tape), an optical medium (for example, a DVD), or a semiconductor medium (for example, a solid state disk Solid State Disk (SSD)).</p>
<p id="p0128" num="0128">The foregoing descriptions are merely specific embodiments of the present invention, but are not intended to limit the protection scope of the present invention. Any modification or<!-- EPO <DP n="27"> --> replacement readily figured out by a person skilled in the art within the technical scope disclosed in the present invention shall fall within the protection scope of the present invention. Therefore, the protection scope of the present invention shall be subject to the protection scope of the claims.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="28"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A method for video synchronous display, wherein the method comprises:
<claim-text>playing, by user equipment, a first video;</claim-text>
<claim-text>receiving, by the user equipment, an indication of starting synchronous display with a target device;</claim-text>
<claim-text>in response to the synchronous display indication, obtaining, by the user equipment, status information of the first video, wherein the video status information is used by the target device and the user equipment to synchronously play an image of the first video; and</claim-text>
<claim-text>sending, by the user equipment, the video status information to the target device, so that the target device synchronously plays the image of the first video based on the video status information.</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The method according to claim 1, wherein the video status information comprises video identification information and video playing progress information.</claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The method according to claim 1 or 2, wherein the video status information further comprises user posture data.</claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>The method according to claim 3, wherein the sending, by the user equipment, the video status information to the target device further comprises: performing, by the user equipment, suppression calculation on the user posture data, and sending the user posture data based on a suppression calculation result.</claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>The method according to any one of claims 1 to 4, wherein the user equipment selects, based on a network connection manner between the user equipment and the target device, a near field manner or a far field manner to send the video status information to the target device.</claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>The method according to any one of claims 1 to 5, wherein the user equipment encapsulates the video status information and sends the encapsulated video status information to the target device.</claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>The method according to any one of claims 1 to 6, wherein the user equipment, when detecting that a video playing status changes, re-obtains the video status information of the first video and sends the video status information of the first video to the target device.</claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>The method according to any one of claims 1 to 7, wherein the user equipment, when detecting a playing progress synchronization instruction or a playing progress correction<!-- EPO <DP n="29"> --> instruction, collects video playing progress information and video playing speed information of the first video, and sends the video playing progress information and the video playing speed information of the first video to the target device.</claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>A method for video synchronous display, wherein the method comprises:
<claim-text>receiving, by a target device, status information that is of a first video and that is sent by user equipment, wherein the status information of the first video is status information that is collected by the user equipment and that is of the first video being played on the user equipment; and</claim-text>
<claim-text>synchronously playing, by the target device, an image of the first video based on the video status information.</claim-text></claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>The method according to claim 9, wherein the video status information comprises video identification information and video playing progress information, and the target device finds a corresponding first video file based on the video identification information, and locates, based on the video playing progress information, a playing position corresponding to the first video.</claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>The method according to claim 9 or 10, wherein the video status information further comprises user posture data, and the target device restores, based on the user posture data, a playing image that is of the first video and that is at the corresponding position.</claim-text></claim>
<claim id="c-en-0012" num="0012">
<claim-text>User equipment for video synchronous display, wherein the user equipment comprises: a video playing module, a video status information obtaining module, and a video status information sending module, wherein
<claim-text>the video playing module is configured to play a first video;</claim-text>
<claim-text>the video status information obtaining module is configured to obtain status information of the first video after receiving an indication of starting synchronous display with a target device, wherein the video status information is used for the target device and the user equipment to synchronously play an image of the first video; and</claim-text>
<claim-text>the video status information sending module is configured to send the obtained video status information to the target device, so that the target device synchronously plays the image of the first video based on the video status information.</claim-text></claim-text></claim>
<claim id="c-en-0013" num="0013">
<claim-text>The user equipment according to claim 12, wherein the video status information comprises video identification information and video playing progress information.</claim-text></claim>
<claim id="c-en-0014" num="0014">
<claim-text>The user equipment according to claim 12 or 13, wherein the video status information further comprises user posture data.<!-- EPO <DP n="30"> --></claim-text></claim>
<claim id="c-en-0015" num="0015">
<claim-text>The user equipment according to claim 14, wherein the video status information sending module is configured to send the video status information to the target device further comprises that the video status information sending module is configured to perform suppression calculation on the user posture data, and send the user posture data based on a suppression calculation result.</claim-text></claim>
<claim id="c-en-0016" num="0016">
<claim-text>The user equipment according to any one of claims 12 to 15, wherein the video status information sending module selects, based on a network connection manner between the user equipment and the target device, a near field manner or a far field manner to send the video status information to the target device.</claim-text></claim>
<claim id="c-en-0017" num="0017">
<claim-text>The user equipment according to any one of claims 12 to 16, wherein the video status information sending module is further configured to encapsulate the video status information and send the encapsulated video status information to the target device.</claim-text></claim>
<claim id="c-en-0018" num="0018">
<claim-text>The user equipment according to any one of claims 12 to 17, wherein the video status information obtaining module is further configured to, when detecting that a video playing status changes, re-obtain the video status information of the first video and send the video status information of the first video to the target device.</claim-text></claim>
<claim id="c-en-0019" num="0019">
<claim-text>The user equipment according to any one of claims 12 to 18, wherein the video status information obtaining module is further configured to, when detecting a playing progress synchronization instruction or a playing progress correction instruction, collect playing progress information and playing speed information of the first video, and send the playing progress information and the playing speed information of the first video to the target device.</claim-text></claim>
<claim id="c-en-0020" num="0020">
<claim-text>A target device for video synchronous display, wherein the target device comprises a video status information receiving module and a synchronous video playing module, wherein
<claim-text>the video status information receiving module is configured to receive status information that is of a first video and that is sent by user equipment, wherein the status information of the first video is status information that is collected by the user equipment and that is of the first video being played on the user equipment; and</claim-text>
<claim-text>the synchronous video playing module is configured to synchronously play an image of the first video on the target device based on the video status information.</claim-text></claim-text></claim>
<claim id="c-en-0021" num="0021">
<claim-text>The target device according to claim 20, wherein the video status information comprises video identification information and video playing progress information, and the synchronous video playing module is further configured to find a first video file based on the video<!-- EPO <DP n="31"> --> identification information, and locate a corresponding playing position based on the video playing progress information.</claim-text></claim>
<claim id="c-en-0022" num="0022">
<claim-text>The target device according to claim 20 or 21, wherein the video status information further comprises user posture data, and the synchronous video playing module is further configured to restore, based on the user posture data, a playing image that is of the first video and that is at the corresponding position.</claim-text></claim>
<claim id="c-en-0023" num="0023">
<claim-text>A system for video synchronous display, wherein the system comprises the user equipment according to any one of claims 12 to 19 and the target device according to any one of claims 20 to 22.</claim-text></claim>
<claim id="c-en-0024" num="0024">
<claim-text>A computer-readable storage medium storing a computer program, wherein when the program is executed by a processor, the method according to any one of claims 1 to 11 is implemented.</claim-text></claim>
<claim id="c-en-0025" num="0025">
<claim-text>User equipment for video synchronous display, comprising a memory, a processor, and a computer program that is stored in the memory and that is capable of running on the processor, wherein the processor executes the computer program to implement the method according to any one of claims 1 to 8.</claim-text></claim>
<claim id="c-en-0026" num="0026">
<claim-text>A target device for video synchronous display, comprising a memory, a processor, and a computer program that is stored in the memory and that is capable of running on the processor, wherein the processor executes the computer program to implement the method according to any one of claims 9 to 11.</claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="32"> -->
<figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="164" he="166" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> -->
<figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="136" he="121" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> -->
<figure id="f0003" num="2a"><img id="if0003" file="imgf0003.tif" wi="144" he="105" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> -->
<figure id="f0004" num="3"><img id="if0004" file="imgf0004.tif" wi="165" he="196" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> -->
<figure id="f0005" num="4"><img id="if0005" file="imgf0005.tif" wi="136" he="75" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> -->
<figure id="f0006" num="5"><img id="if0006" file="imgf0006.tif" wi="139" he="178" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> -->
<figure id="f0007" num="6"><img id="if0007" file="imgf0007.tif" wi="113" he="140" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> -->
<figure id="f0008" num="7,8"><img id="if0008" file="imgf0008.tif" wi="133" he="219" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="152" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="152" he="233" type="tif"/></search-report-data>
</ep-patent-document>
