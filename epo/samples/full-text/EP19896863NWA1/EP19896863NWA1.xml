<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE ep-patent-document PUBLIC "-//EPO//EP PATENT DOCUMENT 1.5.1//EN" "ep-patent-document-v1-5-1.dtd">
<!-- This XML data has been generated under the supervision of the European Patent Office -->
<ep-patent-document id="EP19896863A1" file="EP19896863NWA1.xml" lang="en" country="EP" doc-number="3890333" kind="A1" date-publ="20211006" status="n" dtd-version="ep-patent-document-v1-5-1">
<SDOBI lang="en"><B000><eptags><B001EP>ATBECHDEDKESFRGBGRITLILUNLSEMCPTIESILTLVFIROMKCYALTRBGCZEEHUPLSKBAHRIS..MTNORSMESMMAKHTNMD..........</B001EP><B005EP>J</B005EP><B007EP>BDM Ver 2.0.12 (4th of August) -  1100000/0</B007EP></eptags></B000><B100><B110>3890333</B110><B120><B121>EUROPEAN PATENT APPLICATION</B121><B121EP>published in accordance with Art. 153(4) EPC</B121EP></B120><B130>A1</B130><B140><date>20211006</date></B140><B190>EP</B190></B100><B200><B210>19896863.8</B210><B220><date>20191202</date></B220><B240><B241><date>20210630</date></B241></B240><B250>zh</B250><B251EP>en</B251EP><B260>en</B260></B200><B300><B310>201811536818</B310><B320><date>20181214</date></B320><B330><ctry>CN</ctry></B330></B300><B400><B405><date>20211006</date><bnum>202140</bnum></B405><B430><date>20211006</date><bnum>202140</bnum></B430></B400><B500><B510EP><classification-ipcr sequence="1"><text>H04N  21/44        20110101AFI20200619BHEP        </text></classification-ipcr><classification-ipcr sequence="2"><text>H04N  21/439       20110101ALI20200619BHEP        </text></classification-ipcr><classification-ipcr sequence="3"><text>H04N  21/472       20110101ALI20200619BHEP        </text></classification-ipcr><classification-ipcr sequence="4"><text>G10L  15/08        20060101ALI20200619BHEP        </text></classification-ipcr></B510EP><B520EP><classifications-cpc><classification-cpc sequence="1"><text>G10L  15/08        20130101 LI20200710BCEP        </text></classification-cpc><classification-cpc sequence="2"><text>H04N  21/439       20130101 LI20200710BCEP        </text></classification-cpc><classification-cpc sequence="3"><text>H04N  21/44        20130101 LI20200710BCEP        </text></classification-cpc><classification-cpc sequence="4"><text>H04N  21/472       20130101 LI20200710BCEP        </text></classification-cpc></classifications-cpc></B520EP><B540><B541>de</B541><B542>VIDEOSCHNEIDVERFAHREN UND -VORRICHTUNG, COMPUTERVORRICHTUNG UND SPEICHERMEDIUM</B542><B541>en</B541><B542>VIDEO CUTTING METHOD AND APPARATUS, COMPUTER DEVICE AND STORAGE MEDIUM</B542><B541>fr</B541><B542>PROCÉDÉ ET ET APPAREIL DE DÉCOUPAGE VIDÉO, DISPOSITIF INFORMATIQUE ET SUPPORT DE STOCKAGE</B542></B540><B590><B598>2</B598></B590></B500><B700><B710><B711><snm>One Connect Smart Technology Co., Ltd. (Shenzhen)</snm><iid>101835888</iid><irf>PAJRA20181362-PCT-EP</irf><adr><str>Qianhai Complex A201 
No.1 Qianwan 1st Road 
Qianhai Shenzhen-Hong Kong Cooperation Zone</str><city>Shenzhen, Guangdong 518052</city><ctry>CN</ctry></adr></B711></B710><B720><B721><snm>WANG, Zhenhua</snm><adr><str>Qianhai Complex A201, NO.1 Qianwan 1st Road  
Qianhai Shenzhen-Hong Kong Cooperation Zone</str><city>Shenzhen, Guangdong 518052</city><ctry>CN</ctry></adr></B721></B720><B740><B741><snm>De Arpe Tejero, Manuel</snm><iid>101893835</iid><adr><str>Arpe Patentes y Marcas 
Alcalá, 26, 5ª Planta</str><city>28014 Madrid</city><ctry>ES</ctry></adr></B741></B740></B700><B800><B840><ctry>AL</ctry><ctry>AT</ctry><ctry>BE</ctry><ctry>BG</ctry><ctry>CH</ctry><ctry>CY</ctry><ctry>CZ</ctry><ctry>DE</ctry><ctry>DK</ctry><ctry>EE</ctry><ctry>ES</ctry><ctry>FI</ctry><ctry>FR</ctry><ctry>GB</ctry><ctry>GR</ctry><ctry>HR</ctry><ctry>HU</ctry><ctry>IE</ctry><ctry>IS</ctry><ctry>IT</ctry><ctry>LI</ctry><ctry>LT</ctry><ctry>LU</ctry><ctry>LV</ctry><ctry>MC</ctry><ctry>MK</ctry><ctry>MT</ctry><ctry>NL</ctry><ctry>NO</ctry><ctry>PL</ctry><ctry>PT</ctry><ctry>RO</ctry><ctry>RS</ctry><ctry>SE</ctry><ctry>SI</ctry><ctry>SK</ctry><ctry>SM</ctry><ctry>TR</ctry></B840><B844EP><B845EP><ctry>BA</ctry></B845EP><B845EP><ctry>ME</ctry></B845EP></B844EP><B848EP><B849EP><ctry>KH</ctry></B849EP><B849EP><ctry>MA</ctry></B849EP><B849EP><ctry>MD</ctry></B849EP><B849EP><ctry>TN</ctry></B849EP></B848EP><B860><B861><dnum><anum>CN2019122472</anum></dnum><date>20191202</date></B861><B862>zh</B862></B860><B870><B871><dnum><pnum>WO2020119508</pnum></dnum><date>20200618</date><bnum>202025</bnum></B871></B870></B800></SDOBI>
<abstract id="abst" lang="en">
<p id="pa01" num="0001">Disclosed is a video cutting method, including: extracting to-be-identified video data from video stream data, and extracting image data and audio data from the to-be-identified video data; inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result; obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule; adding a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.<img id="iaf01" file="imgaf001.tif" wi="71" he="122" img-content="drawing" img-format="tif"/></p>
</abstract>
<description id="desc" lang="en"><!-- EPO <DP n="1"> -->
<heading id="h0001"><b>CROSS-REFERENCE TO RELATED APPLICATIONS</b></heading>
<p id="p0001" num="0001">This application claims priority to <patcit id="pcit0001" dnum="CN201811536818"><text>Chinese Patent Application 201811536818X, filed with the China National Intellectual Property Administration on December 14, 2018</text></patcit> and entitled "VIDEO CUTTING METHOD AND APPARATUS, COMPUTER DEVICE, AND STORAGE MEDIUM", which is incorporated herein by reference in its entirety.</p>
<heading id="h0002"><b>TECHNICAL FIELD</b></heading>
<p id="p0002" num="0002">This application relates to a video cutting method and apparatus, a computer device, and a storage medium.</p>
<heading id="h0003"><b>BACKGROUND</b></heading>
<p id="p0003" num="0003">With the development of multimedia technologies, there are vast applications of movies, television, news, socializing, education, and games that transmit information and resources in a video form, such as video chatting, video conferencing, video surveillance, and video dramas. Videos have become an important part of people's work, study, and life.</p>
<p id="p0004" num="0004">In video applications, cutting processing needs to be performed on videos in certain scenarios, such as television news extraction and sensitive information removal from recorded videos. The inventor is aware that, in current video cutting processing, manual marking is needed to determine locations on the timeline for cutting videos, and the video cutting processing efficiency is low.</p>
<heading id="h0004"><b>SUMMARY</b></heading>
<p id="p0005" num="0005">According to various embodiments disclosed in this application, a video cutting method and apparatus, a computer device, and a storage medium are provided.</p>
<p id="p0006" num="0006">A video cutting method includes:
<ul id="ul0001" list-style="none" compact="compact">
<li>extracting to-be-identified video data from video stream data, and extracting image data and audio data from the to-be-identified video data;<!-- EPO <DP n="2"> --></li>
<li>inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result;</li>
<li>obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule;</li>
<li>adding a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and</li>
<li>performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</li>
</ul></p>
<p id="p0007" num="0007">A video cutting apparatus includes:
<ul id="ul0002" list-style="none" compact="compact">
<li>an identified-data extraction module, configured to extract to-be-identified video data from video stream data, and extract image data and audio data from the to-be-identified video data;</li>
<li>a marking recognition processing module, configured to input the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and input the audio data into a preset marking speech recognition model to obtain a marking speech recognition result;</li>
<li>a marking result obtaining module, configured to obtain a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule;</li>
<li>a cutting identifier adding module, configured to add a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and</li>
<li>a video cutting module, configured to perform cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</li>
</ul></p>
<p id="p0008" num="0008">A computer device includes a memory and one or more processors, where the memory stores a computer-readable instruction, and when the computer-readable instruction is executed by the one or more processors, the one or more processors are enabled to perform the following steps:<!-- EPO <DP n="3"> -->
<ul id="ul0003" list-style="none" compact="compact">
<li>extracting to-be-identified video data from video stream data, and extracting image data and audio data from the to-be-identified video data;</li>
<li>inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result;</li>
<li>obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule;</li>
<li>adding a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and</li>
<li>performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</li>
</ul></p>
<p id="p0009" num="0009">One or more non-volatile computer-readable storage media that store a computer-readable instruction are provided. When the computer-readable instruction is executed by one or more processors, the one or more processors are enabled to perform the following steps:
<ul id="ul0004" list-style="none" compact="compact">
<li>extracting to-be-identified video data from video stream data, and extracting image data and audio data from the to-be-identified video data;</li>
<li>inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result;</li>
<li>obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule;</li>
<li>adding a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and</li>
<li>performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</li>
</ul></p>
<p id="p0010" num="0010">Details of one or more embodiments of this application are provided in the following accompanying drawings and descriptions. Other features and advantages of this application become clear from the specification, the accompanying drawings, and the claims.<!-- EPO <DP n="4"> --></p>
<heading id="h0005"><b>BRIEF DESCRIPTION OF DRAWINGS</b></heading>
<p id="p0011" num="0011">To describe the technical solutions in the embodiments of this application more clearly, the following briefly describes the accompanying drawings required for describing the embodiments. Apparently, the accompanying drawings in the following descriptions show merely some embodiments of this application, and a person of ordinary skill in the art may still derive other drawings from these accompanying drawings without creative efforts.
<ul id="ul0005" list-style="none" compact="compact">
<li><figref idref="f0001">FIG. 1</figref> is an application scenario diagram of a video cutting method according to one or more embodiments;</li>
<li><figref idref="f0001">FIG. 2</figref> is a schematic flowchart of a video cutting method according to one or more embodiments;</li>
<li><figref idref="f0002">FIG. 3</figref> is a schematic flowchart of responding to a marking-based cutting instruction according to one or more embodiments;</li>
<li><figref idref="f0003">FIG. 4</figref> is a schematic flowchart of a video cutting method according to another embodiment;</li>
<li><figref idref="f0004">FIG. 5</figref> is a structural diagram of a video cutting apparatus according to one or more embodiments; and</li>
<li><figref idref="f0004">FIG. 6</figref> is an internal structural diagram of a computer device according to one or more embodiments.</li>
</ul></p>
<heading id="h0006"><b>DESCRIPTION OF EMBODIMENTS</b></heading>
<p id="p0012" num="0012">To make the technical solutions and advantages of this application clearer and more comprehensible, the following further describes this application in detail with reference to the accompanying drawings and embodiments. It should be understood that the specific embodiments described herein are merely intended to explain this application, and are not intended to limit this application.</p>
<p id="p0013" num="0013">A video cutting method provided in this application may be applied to an application environment shown in <figref idref="f0001">FIG. 1</figref>. A recording device 102 communicates with a server 104 over a network. The recording device 102 performs video recording, and sends recorded video stream data to the server 104. The server 104 extracts image data and audio data from to-be-identified video data obtained from the video stream data, inputs the image data and the audio data respectively into<!-- EPO <DP n="5"> --> a corresponding preset marking behavior recognition model and preset marking speech recognition model, and then obtains a marking recognition result based on an obtained marking behavior recognition result, an obtained marking speech recognition result, and a preset marking triggering rule. When a type of the marking recognition result is a marking operation, a cutting point identifier is added to the to-be-identified video data, and finally the video stream data is cut based on the cutting point identifier to obtain video segment data.</p>
<p id="p0014" num="0014">The recording device 102 may be but is not limited to various video recording cameras or terminals with a video recording function, such as personal computers, laptops, smartphones, tablets, and portable wearable devices, and the server 104 may be implemented by a standalone server or a server cluster including multiple servers.</p>
<p id="p0015" num="0015">In an embodiment, as shown in <figref idref="f0001">FIG. 2</figref>, a video cutting method is provided. An example in which the method is applied to the server 104 in <figref idref="f0001">FIG. 1</figref> is used for description. The method includes the following steps.</p>
<p id="p0016" num="0016">Step S201: Extract to-be-identified video data from video stream data, and extract image data and audio data from the to-be-identified video data.</p>
<p id="p0017" num="0017">In this embodiment, the to-be-identified video data is extracted from the video stream data. The video stream data is video data that needs to be cut, and may be recorded by a recording device. For example, for a face-to-face verification process in the financial industry, the video stream data may be video data recorded in real time by a camera during video and audio recording. The to-be-identified video data is video data of a preset identification length. The identification length is set based on an actual requirement. Marking recognition may be performed on the to-be-identified video data to add a corresponding cutting point identifier. Performing marking recognition on the to-be-identified video data of the preset identification length allows cutting recorded video data in real time, thereby ensuring video cutting timeliness and improving video cutting efficiency.</p>
<p id="p0018" num="0018">Generally, the video data includes images and audio, and marking recognition can be performed on both the images and the audio. Specifically, when marking recognition is performed on the to-be-identified video data, the image data and the audio data are extracted from the to-be-identified video data, so as to separately perform recognition processing on both the image<!-- EPO <DP n="6"> --> data and the audio data in the to-be-identified video data. In this way, it can be identified whether a marking behavior occurs in the video images or whether a marking speech occurs in the video audio, thereby implementing marking recognition on the image behavior and the audio speech, and improving marking recognition accuracy.</p>
<p id="p0019" num="0019">Step S203: Input the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and input the audio data into a preset marking speech recognition model to obtain a marking speech recognition result.</p>
<p id="p0020" num="0020">After the image data and the audio data are extracted from the to-be-identified video data, the image data and the audio data are separately input into the corresponding marking behavior recognition model and marking speech recognition model for marking recognition. The marking behavior recognition model may be based on an artificial neural network algorithm, and may be obtained by training historical marking behavior data of a business person in a business system in a corresponding business scenario, for example, the historical marking behavior data may be marking behavior actions such as an applause action, a hands-up action, or a knocking action. The marking speech recognition model may be obtained by training historical marking speech data of a business person, for example, the historical marking speech data may be speech marking of keywords such as "first, second, and third".</p>
<p id="p0021" num="0021">In this embodiment, in one aspect, the image data is input into the preset marking behavior recognition model to perform marking behavior recognition and obtain the marking behavior recognition result; in the other aspect, the audio data is input into the preset marking speech recognition model to perform marking speech recognition and obtain the marking speech recognition result. Separately performing marking recognition on the image data and the audio data can increase diversity of marking operations, improve business process smoothness, and ensure video cutting accuracy.</p>
<p id="p0022" num="0022">Step S205: Obtain a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule.</p>
<p id="p0023" num="0023">After the marking behavior recognition result and the marking speech recognition result are obtained, the two results are combined to obtain the marking recognition result. Specifically, the<!-- EPO <DP n="7"> --> preset marking triggering rule is queried, where the marking triggering rule is set based on an actual business requirement. For example, the preset marking triggering rule may be set as follows: An OR operation is performed on the marking behavior recognition result and the marking speech recognition result. To be specific, if a type of either of the marking behavior recognition result and the marking speech recognition result is a marking operation, that is, when a cutting point identifier needs to be added, marking is triggered and the obtained marking recognition result is a marking operation. Alternatively, the preset marking triggering rule may be set as follows: An AND operation is performed on the marking behavior recognition result and the marking speech recognition result. To be specific, only when types of the marking behavior recognition result and the marking speech recognition result are each a marking operation, marking is triggered and the obtained marking recognition result is a marking operation.</p>
<p id="p0024" num="0024">Step S207: Add a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation.</p>
<p id="p0025" num="0025">After the marking recognition result is obtained, the type of the marking recognition result is determined. When the type of the marking recognition result is a marking operation, it indicates that the image data and/or the audio data in the to-be-identified video data have/has triggered marking, and the to-be-identified video data is in a video cutting location, and marking processing is performed on the to-be-identified video data. Specifically, a cutting point identifier may be added to the to-be-identified video data. The cutting point identifier is used to identify a cutting point for video cutting. During cutting of the video stream data, the cutting point identifier may be directly searched for to perform cutting processing.</p>
<p id="p0026" num="0026">In specific implementation, the cutting point identifier may be a cutting label. When the cutting point identifier is added to the to-be-identified video data, a key frame is determined from the to-be-identified video data based on a preset label adding rule. For example, the first frame in the to-be-identified video data is used as the key frame, and a cutting label is added to the key frame. The cutting label may include a cutting point sequence number and a cutting time value, but is not limited thereto.</p>
<p id="p0027" num="0027">Step S209: Perform cutting processing on the video stream data based on the cutting point<!-- EPO <DP n="8"> --> identifier to obtain video segment data.</p>
<p id="p0028" num="0028">When cutting processing is performed on the video stream data, the cutting point identifier in the video stream data is searched for, and cutting processing is performed based on the cutting point identifier to split the video stream data and obtain the video segment data.</p>
<p id="p0029" num="0029">In the foregoing video cutting method, the image data and the audio data are extracted from the to-be-identified video data obtained from the video stream data, the image data and the audio data are respectively input into the corresponding preset marking behavior recognition model and preset marking speech recognition model, and then the marking recognition result is obtained based on the obtained marking behavior recognition result, the obtained marking speech recognition result, and the preset marking triggering rule. When the type of the marking recognition result is a marking operation, a cutting point identifier is added to the to-be-identified video data, and finally the video stream data is cut based on the cutting point identifier to obtain the video segment data. In the video cutting process, marking recognition may be performed based on the image data and the audio data in the to-be-identified video data, and a cutting point identifier may be added. No manual marking operation is needed, thereby improving video cutting processing efficiency.</p>
<p id="p0030" num="0030">In some embodiments, the extracting to-be-identified video data from video stream data includes: obtaining the video stream data; determining a video stream identification length; and extracting the to-be-identified video data from the video stream data based on the video stream identification length.</p>
<p id="p0031" num="0031">Marking recognition processing cannot be directly performed on video stream data directly recorded by the recording device 102. The video stream data needs to be split into to-be-identified video data of a fixed identification length, and marking recognition is performed on the to-be-identified video data. In this embodiment, when the to-be-identified video data is extracted from the video stream data, in one aspect, the video stream data may be first obtained. Specifically, video stream data recorded in real time may be directly received from the recording device 102, or video stream data that has been recorded may be read from a preset memory. In the other aspect, the video stream identification length is determined, where the video stream identification length is set based on an actual requirement. For example, the video stream identification length may be set<!-- EPO <DP n="9"> --> based on input requirements of the marking behavior recognition model and the marking speech recognition model, or may be set based on processing resources of the server 104. After the video stream identification length is determined, the to-be-identified video data is extracted from the video stream data based on the video stream identification length. In specific applications, to-be-identified video data that meets the video stream identification length may be successively extracted from the video stream data, and then marking recognition processing may be subsequently performed on the extracted to-be-identified video data.</p>
<p id="p0032" num="0032">In an embodiment, the inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result includes: determining identification information of a business person corresponding to the to-be-identified video data; querying a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information; extracting image feature data from the image data, and extracting audio feature data from the audio data; and inputting the image feature data into the marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio feature data into the marking speech recognition model to obtain a marking speech recognition result.</p>
<p id="p0033" num="0033">In this embodiment, both the marking behavior recognition model and the marking speech recognition model are obtained by training historical marking data of each business person in the business system. Generally, in a video and audio recording process for face-to-face verification of businesses, different business systems may have different marking operation requirements, and different business persons may also have different marking operation habits.</p>
<p id="p0034" num="0034">Specifically, when the image data is input into the preset marking behavior recognition model to obtain the marking behavior recognition result, and the audio data is input into the preset marking speech recognition model to obtain the marking speech recognition result, the identification information of the business person corresponding to the to-be-identified video data is first determined. In applications, a recording device 102 is disposed in a service window of each business. The corresponding business person may be determined based on a source of the<!-- EPO <DP n="10"> --> to-be-identified video data, that is, the recording device 102, and identification information corresponding to the business person may be further queried. The identification information may be but is not limited to identity information that can uniquely identify the business person, such as an employee number and an employee name. After the identification information is determined, a preset marking behavior recognition model and a preset marking speech recognition model that correspond to the identification information are queried. The marking behavior recognition model and the marking speech recognition model are respectively obtained by training historical marking behavior data and historical marking speech data of the corresponding business person, and therefore provide highly specific marking recognition and have high recognition accuracy.</p>
<p id="p0035" num="0035">After the marking behavior recognition model and the marking speech recognition model are obtained, in one aspect, the image feature data is extracted from the image data, and the image feature data is input into the marking behavior recognition model to obtain the marking behavior recognition result. In the other aspect, the audio feature data is extracted from the audio data, and the audio feature data is input into the marking speech recognition model to obtain the marking speech recognition result. When marking recognition is performed on the image data and the audio data, feature extraction is performed and useless redundant information is filtered out to obtain the image feature data and the audio feature data, and subsequent marking recognition processing is performed to obtain the marking behavior recognition result and the marking speech recognition result.</p>
<p id="p0036" num="0036">In an embodiment, before the querying a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information, the method further includes: obtaining historical behavior image data and historical marking speech data from a business system; classifying the historical behavior image data and the historical marking speech data by business person to obtain historical behavior image data corresponding to each business person and historical marking speech data corresponding to each business person; training the historical behavior image data corresponding to each business person to obtain the marking behavior recognition model; and training the historical marking speech data corresponding to each business person to obtain the marking speech recognition model.<!-- EPO <DP n="11"> --></p>
<p id="p0037" num="0037">During the training of the marking behavior recognition model and the marking speech recognition model, the historical behavior image data and the historical marking speech data are first obtained from the business system. The historical behavior image data may be marking image data recorded during video and audio recording of each business person in the business system during face-to-face verification of businesses. For example, the historical behavior image data may include marking behaviors such as applause, hands-up, hands-crossing, and nodding. Similar to the historical behavior image data, the historical marking speech data may be keyword statements such as "the X<sup>th</sup> problem" and "OK, thank you". In specific applications, business persons have different personal habits, and their corresponding historical behavior image data and historical marking speech data also present different marking operations. Therefore, the historical behavior image data and the historical marking speech data are classified by business person to construct a corresponding marking behavior recognition model and marking speech recognition model for each business person.</p>
<p id="p0038" num="0038">Specifically, the historical behavior image data corresponding to each business person is trained to obtain the marking behavior recognition model; and the historical marking speech data corresponding to each business person is trained to obtain the marking speech recognition model. In specific implementation, the historical behavior image data may be divided into a training sample set and a test sample set. The training sample set is trained by using a supervised learning method to obtain a marking behavior model to be tested. Then, the test sample set is used to perform a recognition precision test on the marking behavior model to be tested, and the marking behavior recognition model is obtained after the recognition precision test succeeds. A training process of the marking speech recognition model is similar to that of the marking behavior recognition model.</p>
<p id="p0039" num="0039">In an embodiment, the obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule includes: querying a preset marking triggering rule, where the marking triggering rule includes a behavior triggering rule and a speech triggering rule; comparing the marking behavior recognition result with the behavior triggering rule to obtain a behavior triggering result; comparing the marking speech recognition result with the speech triggering rule to obtain a speech triggering result;<!-- EPO <DP n="12"> --> and obtaining the marking recognition result based on the behavior triggering result and the speech triggering result.</p>
<p id="p0040" num="0040">After the marking behavior recognition result and the marking speech recognition result are obtained, the marking recognition result is obtained based on a marking triggering rule for an actual business requirement. Specifically, the preset marking triggering rule is queried, where the marking triggering rule is set based on an actual business requirement. Specifically, the marking triggering rule may be set based on a business type and a habit of a business person. For example, the marking triggering rule may be set as follows: It is considered that marking is triggered when an applause behavior of a business person is recognized in the image data or a key statement "the X<sup>th</sup> problem" is recognized in the audio data. The marking triggering rule includes the behavior triggering rule and the speech triggering rule, which respectively correspond to marking recognition on the image data and marking recognition on the audio data.</p>
<p id="p0041" num="0041">In one aspect, the marking behavior recognition result is compared with the behavior triggering rule to obtain the behavior triggering result; in the other aspect, the marking speech recognition result is compared with the speech triggering rule to obtain the speech triggering result. Finally, the marking recognition result is obtained by combining the behavior triggering result and the speech triggering result. For example, an OR operation may be performed on the behavior triggering result and the speech triggering result, that is, when a type of either of the behavior triggering result and the speech triggering result is a marking operation, a type of the obtained marking recognition result is a marking operation, and the cutting point identifier is added to the to-be-identified video data.</p>
<p id="p0042" num="0042">In an embodiment, as shown in <figref idref="f0002">FIG. 3</figref>, the method further includes steps of responding to a marking-based cutting instruction, specifically including:<br/>
Step S301: When receiving a marking-based cutting instruction, determine a cutting moment value of the marking-based cutting instruction.</p>
<p id="p0043" num="0043">In this embodiment, in addition to extracting the to-be-identified video data from the video stream data and performing marking recognition on the to-be-identified video data, manual marking may further be implemented in response to a marking-based cutting instruction sent from the outside. Specifically, when the marking-based cutting instruction is received, a cutting moment<!-- EPO <DP n="13"> --> value of the marking-based cutting instruction is determined. The marking-based cutting instruction may be sent from the outside, for example, sent by a business person by clicking a related marking button. The cutting moment value is a sending time of the marking-based cutting instruction, and reflects a location on a timeline in which a marking operation needs to be performed on the video stream data.</p>
<p id="p0044" num="0044">Step S303: Determine a to-be-cut video frame corresponding to the cutting moment value from the to-be-identified video data.</p>
<p id="p0045" num="0045">After the cutting moment value of the marking-based cutting instruction is determined, the to-be-cut video frame corresponding to the cutting moment value is determined from the to-be-identified video data. Generally, when the marking-based cutting instruction is sent from the outside, it indicates that a marking operation is needed for a video frame corresponding to the moment in the to-be-identified video data. The corresponding to-be-cut video frame may be determined from the timeline of the to-be-identified video data based on the cutting moment value of the marking-based cutting instruction.</p>
<p id="p0046" num="0046">Step S305: Add a cutting point identifier to the to-be-cut video frame.</p>
<p id="p0047" num="0047">After the to-be-cut video frame is determined, the cutting point identifier is added to the to-be-cut video frame, and the cutting point identifier is used to identify a cutting point for video cutting. During cutting of the video stream data, the cutting point identifier may be directly searched for to perform cutting processing.</p>
<p id="p0048" num="0048">Step S307: Return to the step of performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</p>
<p id="p0049" num="0049">After the cutting point identifier is added, the process returns to the step of performing cutting processing on the video stream data based on the cutting point identifier. The cutting point identifier in the video stream data is searched for, and then cutting processing is performed based on the cutting point identifier to split the video stream data to obtain the video segment data.</p>
<p id="p0050" num="0050">In this embodiment, in addition to performing marking recognition on the image data and the audio data in the to-be-identified video data, the marking-based cutting instruction sent from the outside is further received in real time, and video cutting processing is performed based on the<!-- EPO <DP n="14"> --> marking-based cutting instruction to implement external control on video cutting, thereby effectively increasing operation diversity of video cutting and improving video cutting processing efficiency.</p>
<p id="p0051" num="0051">In some embodiments, after the video segment data is obtained, the method further includes: extracting audio segment data from the video segment data; querying a preset speech recognition model; inputting the audio segment data into the speech recognition model to obtain transcription data of the video segment data; and determining a business type corresponding to the video segment data based on the transcription data, and storing the video segment data in a storage location corresponding to the business type.</p>
<p id="p0052" num="0052">In this embodiment, after the video segment data cut from the video stream data is obtained, the video segment data may be stored in the corresponding storage location based on the business type of the respective video segment data. Specifically, the audio segment data is extracted from the video segment data, where the audio segment data includes conversation data in the video segment data, and the business type corresponding to the video segment data may be determined based on the audio segment data. The preset speech recognition model is queried, where the speech recognition model may perform speech recognition on input speech data to obtain corresponding transcription data.</p>
<p id="p0053" num="0053">In this embodiment, the audio segment data is input into the speech recognition model to obtain the transcription data of the video segment data, where the transcription data may be data in the text form, and the business type corresponding to the video segment data may be determined based on the transcription data. In specific implementation, a business keyword may be extracted from the transcription data, and a corresponding business type is matched based on the obtained business keyword. After the business type corresponding to the video segment data is determined, the video segment data is stored in the storage location corresponding to the business type. For example, a preset storage location corresponding to the business type may be queried, and the video segment data may be stored in the storage location, thereby implementing automatic classification and storage of the video segment data.</p>
<p id="p0054" num="0054">In an embodiment, as shown in <figref idref="f0003">FIG. 4</figref>, a video cutting method is provided, including:<!-- EPO <DP n="15"> -->
<ul id="ul0006" list-style="none" compact="compact">
<li>Step S401: Obtain video stream data.</li>
<li>Step S402: Determine a video stream identification length.</li>
<li>Step S403: Extract to-be-identified video data from the video stream data based on the video stream identification length.</li>
<li>Step S404: Extract image data and audio data from the to-be-identified video data.</li>
</ul></p>
<p id="p0055" num="0055">In this embodiment, the server 104 receives video stream data sent by the recording device 102, determines a video stream identification length specified based on an actual requirement, successively extracts, based on the video stream identification length, to-be-identified video data that meets the video stream identification length from the video stream data, and then performs subsequent marking recognition processing on the extracted to-be-identified video data.</p>
<p id="p0056" num="0056">Step S405: Determine identification information of a business person corresponding to the to-be-identified video data.</p>
<p id="p0057" num="0057">Step S406: Query a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information.</p>
<p id="p0058" num="0058">Step S407: Extract image feature data from the image data, and extract audio feature data from the audio data.</p>
<p id="p0059" num="0059">Step S408: Input the image feature data into the marking behavior recognition model to obtain a marking behavior recognition result, and input the audio feature data into the marking speech recognition model to obtain a marking speech recognition result.</p>
<p id="p0060" num="0060">After the image data and the audio data are obtained, a corresponding business person is determined based on a source of the to-be-identified video data, that is, the recording device 102, and identification information corresponding to the business person is further queried, where the identification information is an employee number and/or an employee name. A corresponding preset marking behavior recognition model and preset marking speech recognition model that correspond to the identification information are queried. The marking behavior recognition model and the marking speech recognition model are respectively obtained by training historical marking behavior data and historical marking speech data of the corresponding business person, and therefore provide highly specific marking recognition and have high recognition accuracy. In one aspect, the image<!-- EPO <DP n="16"> --> feature data is extracted from the image data, and the image feature data is input into the marking behavior recognition model to obtain the marking behavior recognition result. In the other aspect, the audio feature data is extracted from the audio data, and the audio feature data is input into the marking speech recognition model to obtain the marking speech recognition result.</p>
<p id="p0061" num="0061">Step S409: Obtain a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule.</p>
<p id="p0062" num="0062">After the marking behavior recognition result and the marking speech recognition result are obtained, the marking recognition result is obtained based on a marking triggering rule for an actual business requirement. Specifically, the following may be included: querying a preset marking triggering rule, where the marking triggering rule includes a behavior triggering rule and a speech triggering rule; comparing the marking behavior recognition result with the behavior triggering rule to obtain a behavior triggering result; comparing the marking speech recognition result with the speech triggering rule to obtain a speech triggering result; and obtaining the marking recognition result based on the behavior triggering result and the speech triggering result.</p>
<p id="p0063" num="0063">Step S410: Add a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation.</p>
<p id="p0064" num="0064">Step S411: Perform cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</p>
<p id="p0065" num="0065">After the marking recognition result is obtained, a type of the marking recognition result is determined. When the type of the marking recognition result is a marking operation, it indicates that the to-be-identified video data is a cutting point, and marking processing is performed on the to-be-identified video data. Specifically, a cutting point identifier may be added to the to-be-identified video data. The cutting point identifier in the video stream data is searched for to perform cutting processing based on the cutting point identifier, so that the video stream data is split to obtain the video segment data.</p>
<p id="p0066" num="0066">Step S412: Extract audio segment data from the video segment data.</p>
<p id="p0067" num="0067">Step S413: Query a preset speech recognition model.</p>
<p id="p0068" num="0068">Step S414: Input the audio segment data into the speech recognition model to obtain<!-- EPO <DP n="17"> --> transcription data of the video segment data.</p>
<p id="p0069" num="0069">Step S415: Determine a business type corresponding to the video segment data based on the transcription data, and store the video segment data in a storage location corresponding to the business type.</p>
<p id="p0070" num="0070">In this embodiment, after the video segment data cut from the video stream data is obtained, the video segment data may be stored in the corresponding storage location based on the business type of the respective video segment data, thereby implementing automatic classification and storage of the video segment data.</p>
<p id="p0071" num="0071">It should be understood that although the steps in the flowcharts of <figref idref="f0001 f0002 f0003">FIG. 2 to FIG. 4</figref> are shown in sequence indicated by arrows, these steps are not necessarily performed in such sequences. Unless expressly stated herein, these steps are not limited to a strict execution sequence, and may be performed in another sequence. In addition, at least some of the steps in <figref idref="f0001 f0002 f0003">FIG. 2 to FIG. 4</figref> may include a plurality of substeps or stages. These substeps or stages are not necessarily performed and completed at the same moment, but may be performed at different moments. These substeps or stages are not necessarily performed in sequence, either, but may be performed in turn or alternately with other steps or at least some substeps or stages of the other steps.</p>
<p id="p0072" num="0072">In an embodiment, as shown in <figref idref="f0004">FIG. 5</figref>, a video cutting apparatus is provided, including an identified-data extraction module 501, a marking recognition processing module 503, a marking result obtaining module 505, a cutting identifier adding module 507, and a video cutting module 509.</p>
<p id="p0073" num="0073">The identified-data extraction module 501 is configured to extract to-be-identified video data from video stream data, and extract image data and audio data from the to-be-identified video data.</p>
<p id="p0074" num="0074">The marking recognition processing module 503 is configured to input the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and input the audio data into a preset marking speech recognition model to obtain a marking speech recognition result.</p>
<p id="p0075" num="0075">The marking result obtaining module 505 is configured to obtain a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a<!-- EPO <DP n="18"> --> preset marking triggering rule.</p>
<p id="p0076" num="0076">The cutting identifier adding module 507 is configured to add a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation.</p>
<p id="p0077" num="0077">The video cutting module 509 is configured to perform cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</p>
<p id="p0078" num="0078">In an embodiment, the identified-data extraction module 501 includes a video stream obtaining unit, an identification length determining unit, and an identified-data extraction unit. The video stream obtaining unit is configured to obtain the video stream data. The identification length determining unit is configured to determine a video stream identification length. The identified-data extraction unit is configured to extract the to-be-identified video data from the video stream data based on the video stream identification length.</p>
<p id="p0079" num="0079">In an embodiment, the marking recognition processing module 503 includes an identification determining unit, a recognition model query unit, a feature data extraction unit, and a marking recognition unit. The identification determining unit is configured to determine identification information of a business person corresponding to the to-be-identified video data. The recognition model query unit is configured to query a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information. The feature data extraction unit is configured to extract image feature data from the image data, and extract audio feature data from the audio data. The marking recognition unit is configured to input the image feature data into the marking behavior recognition model to obtain a marking behavior recognition result, and input the audio feature data into the marking speech recognition model to obtain a marking speech recognition result.</p>
<p id="p0080" num="0080">In an embodiment, the apparatus further includes a historical data obtaining module, a historical data classification module, a behavior recognition model training module, and a speech recognition model training module. The historical data obtaining module is configured to obtain historical behavior image data and historical marking speech data from a business system. The historical data classification module is configured to classify the historical behavior image data and the historical marking speech data by business person to obtain historical behavior image data<!-- EPO <DP n="19"> --> corresponding to each business person and historical marking speech data corresponding to each business person. The behavior recognition model training module is configured to train the historical behavior image data corresponding to each business person to obtain the marking behavior recognition model. The speech recognition model training module is configured to train the historical marking speech data corresponding to each business person to obtain the marking speech recognition model.</p>
<p id="p0081" num="0081">In an embodiment, the marking result obtaining module 505 includes a triggering rule query unit, a behavior comparison unit, a speech comparison unit, and a marking result obtaining unit. The triggering rule query unit is configured to query a preset marking triggering rule, where the marking triggering rule includes a behavior triggering rule and a speech triggering rule. The behavior comparison unit is configured to compare the marking behavior recognition result with the behavior triggering rule to obtain a behavior triggering result. The speech comparison unit is configured to compare the marking speech recognition result with the speech triggering rule to obtain a speech triggering result. The marking result obtaining unit is configured to obtain the marking recognition result based on the behavior triggering result and the speech triggering result.</p>
<p id="p0082" num="0082">In an embodiment, the apparatus further includes a cutting instruction receiving module, a to-be-cut frame determining module, an identifier adding module, and a cutting processing module. The cutting instruction receiving module is configured to, when receiving a marking-based cutting instruction, determine a cutting moment value of the marking-based cutting instruction. The to-be-cut frame determining module is configured to determine a to-be-cut video frame corresponding to the cutting moment value from the to-be-identified video data. The identifier adding module is configured to add a cutting point identifier to the to-be-cut video frame. The cutting processing module is configured to return to the step of performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</p>
<p id="p0083" num="0083">In an embodiment, the apparatus further includes an audio segment extraction module, a speech recognition model query module, a transcription data obtaining module, and a video segment storage module. The audio segment extraction module is configured to extract audio segment data from the video segment data. The speech recognition model query module is<!-- EPO <DP n="20"> --> configured to query a preset speech recognition model. The transcription data obtaining module is configured to input the audio segment data into the speech recognition model to obtain transcription data of the video segment data. The video segment storage module is configured to determine a business type corresponding to the video segment data based on the transcription data, and store the video segment data in a storage location corresponding to the business type.</p>
<p id="p0084" num="0084">For specific definitions of the video cutting apparatus, reference may be made to the foregoing definitions of the video cutting method, and details are not described herein again. All or some of the modules in the foregoing video cutting apparatus may be implemented by using software, hardware, and a combination thereof. Each of the foregoing modules may be embedded in or independent of a processor in a computer device in the form of hardware, or may be stored in a memory in the computer device in the form of software to enable the processor to conveniently invoke and execute operations corresponding to each of the foregoing modules.</p>
<p id="p0085" num="0085">In an embodiment, a computer device is provided. The computer device may be a server. An internal structural diagram of the computer device may be shown in <figref idref="f0004">FIG. 6</figref>. The computer device includes a processor, a memory, and a network interface that are connected to each other by using a system bus. The processor of the computer device is configured to provide computing and control capabilities. The memory of the computer device includes a non-volatile storage medium and an internal memory. The non-volatile storage medium stores an operating system and a computer-readable instruction. The internal memory provides an environment for running the operating system and the computer-readable instruction in the non-volatile storage medium. The network interface of the computer device is configured to communicate with an external terminal through a network connection. The computer-readable instruction is executed by the processor to implement a video cutting method.</p>
<p id="p0086" num="0086">A person skilled in the art can understand that the structure shown in <figref idref="f0004">FIG. 6</figref> is merely a block diagram of a partial structure related to the solution of this application, and does not constitute a limitation on the computer device to which the solution of this application is applied. Specifically, the computer device may include more or fewer components than those shown in the figure, or combine some components, or have different component arrangements.<!-- EPO <DP n="21"> --></p>
<p id="p0087" num="0087">A computer device is provided, and includes a memory and one or more processors. The memory stores a computer-readable instruction. When the computer-readable instruction is executed by the one or more processors, the steps of the video cutting method in any one of the embodiments of this application are implemented.</p>
<p id="p0088" num="0088">One or more non-volatile storage media that store a computer-readable instruction are provided. When the computer-readable instruction is executed by one or more processors, the one or more processors are enabled to implement the steps of the video cutting method in any one of the embodiments of this application.</p>
<p id="p0089" num="0089">A person of ordinary skill in the art may understand that all or some of the processes in the methods in the foregoing embodiments may be implemented by a computer-readable instruction instructing related hardware. The computer-readable instruction may be stored in a non-volatile computer-readable storage medium, and when the computer-readable instruction is executed, the processes in the embodiments of the methods may be implemented. Any reference to a memory, a storage, a database, or other media used in the various embodiments provided by this application may include a non-volatile memory and/or a volatile memory. The non-volatile memory may include a Read-Only Memory (ROM), a Programmable ROM (PROM), an Electrically Programmable ROM (EPROM), an Electrically Erasable Programmable ROM (EEPROM), or a flash memory. The volatile memory may include a Random Access Memory (RAM) or an external cache memory. As description rather than limitation, the RAM can be obtained in a plurality of forms, such as a static RAM (SRAM), a dynamic RAM (DRAM), a synchronous DRAM (SDRAM), a double data rate SDRAM (DDRSDRAM), an enhanced SDRAM (ESDRAM), a Synchlink DRAM (SLDRAM), a Rambus direct RAM (RDRAM), a direct Rambus dynamic RAM (DRDRAM), and a Rambus dynamic RAM (RDRAM).</p>
<p id="p0090" num="0090">The technical features of the foregoing embodiments may be randomly combined. For brevity of description, not all possible combinations of the technical features of the foregoing embodiments are described. However, all the combinations of these technical features should be considered to be within the scope of this specification provided that the combinations are not contradictory.</p>
<p id="p0091" num="0091">The foregoing embodiments are merely several implementations of this application, and<!-- EPO <DP n="22"> --> description of the implementations is relatively specific and detailed, but shall not be understood as a limitation on the scope of the present invention. It should be noted that a person of ordinary skill in the art may make several variations and improvements without departing from the concept of this application, and the variations and improvements shall fall within the protection scope of this application. Therefore, the protection scope of this application patent shall be subject to the appended claims.</p>
</description>
<claims id="claims01" lang="en"><!-- EPO <DP n="23"> -->
<claim id="c-en-0001" num="0001">
<claim-text>A video cutting method, comprising:
<claim-text>extracting to-be-identified video data from video stream data, and extracting image data and audio data from the to-be-identified video data;</claim-text>
<claim-text>inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result;</claim-text>
<claim-text>obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule;</claim-text>
<claim-text>adding a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and</claim-text>
<claim-text>performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</claim-text></claim-text></claim>
<claim id="c-en-0002" num="0002">
<claim-text>The method according to claim 1, wherein the extracting to-be-identified video data from video stream data comprises:
<claim-text>obtaining the video stream data;</claim-text>
<claim-text>determining a video stream identification length; and</claim-text>
<claim-text>extracting the to-be-identified video data from the video stream data based on the video stream identification length.</claim-text></claim-text></claim>
<claim id="c-en-0003" num="0003">
<claim-text>The method according to claim 1, wherein the inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result comprises:
<claim-text>determining identification information of a business person corresponding to the to-be-identified video data;</claim-text>
<claim-text>querying a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information;</claim-text>
<claim-text>extracting image feature data from the image data, and extracting audio feature data from the<!-- EPO <DP n="24"> --> audio data; and</claim-text>
<claim-text>inputting the image feature data into the marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio feature data into the marking speech recognition model to obtain a marking speech recognition result.</claim-text></claim-text></claim>
<claim id="c-en-0004" num="0004">
<claim-text>The method according to claim 3, wherein before the querying a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information, the method further comprises:
<claim-text>obtaining historical behavior image data and historical marking speech data from a business system;</claim-text>
<claim-text>classifying the historical behavior image data and the historical marking speech data by business person to obtain historical behavior image data corresponding to each business person and historical marking speech data corresponding to each business person;</claim-text>
<claim-text>training the historical behavior image data corresponding to each business person to obtain the marking behavior recognition model; and</claim-text>
<claim-text>training the historical marking speech data corresponding to each business person to obtain the marking speech recognition model.</claim-text></claim-text></claim>
<claim id="c-en-0005" num="0005">
<claim-text>The method according to claim 1, wherein the obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule comprises:
<claim-text>querying a preset marking triggering rule, wherein the marking triggering rule comprises a behavior triggering rule and a speech triggering rule;</claim-text>
<claim-text>comparing the marking behavior recognition result with the behavior triggering rule to obtain a behavior triggering result;</claim-text>
<claim-text>comparing the marking speech recognition result with the speech triggering rule to obtain a speech triggering result; and</claim-text>
<claim-text>obtaining the marking recognition result based on the behavior triggering result and the speech triggering result.</claim-text></claim-text></claim>
<claim id="c-en-0006" num="0006">
<claim-text>The method according to claim 5, wherein the obtaining the marking recognition result based on the behavior triggering result and the speech triggering result comprises:<!-- EPO <DP n="25"> -->
<claim-text>performing an OR operation on the behavior triggering result and the speech triggering result to obtain the marking recognition result.</claim-text></claim-text></claim>
<claim id="c-en-0007" num="0007">
<claim-text>The method according to claim 1, wherein the adding a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation comprises:
<claim-text>determining the type of the marking recognition result;</claim-text>
<claim-text>querying a preset label adding rule when the type of the marking recognition result is a marking operation; and</claim-text>
<claim-text>determining a key frame from the to-be-identified video data based on the label adding rule, and adding a cutting label to the key frame, wherein the cutting point identifier comprises the cutting label.</claim-text></claim-text></claim>
<claim id="c-en-0008" num="0008">
<claim-text>The method according to any one of claims 1 to 7, further comprising:
<claim-text>when receiving a marking-based cutting instruction, determining a cutting moment value of the marking-based cutting instruction;</claim-text>
<claim-text>determining a to-be-cut video frame corresponding to the cutting moment value from the to-be-identified video data;</claim-text>
<claim-text>adding a cutting point identifier to the to-be-cut video frame; and</claim-text>
<claim-text>returning to the step of performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</claim-text></claim-text></claim>
<claim id="c-en-0009" num="0009">
<claim-text>The method according to claim 8, wherein after the video segment data is obtained, the method further comprises:
<claim-text>extracting audio segment data from the video segment data;</claim-text>
<claim-text>querying a preset speech recognition model;</claim-text>
<claim-text>inputting the audio segment data into the speech recognition model to obtain transcription data of the video segment data; and</claim-text>
<claim-text>determining a business type corresponding to the video segment data based on the transcription data, and storing the video segment data in a storage location corresponding to the business type.</claim-text><!-- EPO <DP n="26"> --></claim-text></claim>
<claim id="c-en-0010" num="0010">
<claim-text>The method according to claim 9, wherein the determining a business type corresponding to the video segment data based on the transcription data, and storing the video segment data in a storage location corresponding to the business type comprises:
<claim-text>extracting a business keyword from the transcription data;</claim-text>
<claim-text>determining the business type corresponding to the video segment data based on the business keyword;</claim-text>
<claim-text>querying a preset storage location corresponding to the business type; and</claim-text>
<claim-text>storing the video segment data in the storage location.</claim-text></claim-text></claim>
<claim id="c-en-0011" num="0011">
<claim-text>A video cutting apparatus, comprising:
<claim-text>an identified-data extraction module, configured to extract to-be-identified video data from video stream data, and extract image data and audio data from the to-be-identified video data;</claim-text>
<claim-text>a marking recognition processing module, configured to input the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and input the audio data into a preset marking speech recognition model to obtain a marking speech recognition result;</claim-text>
<claim-text>a marking result obtaining module, configured to obtain a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule;</claim-text>
<claim-text>a cutting identifier adding module, configured to add a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and</claim-text>
<claim-text>a video cutting module, configured to perform cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</claim-text></claim-text></claim>
<claim id="c-en-0012" num="0012">
<claim-text>The apparatus according to claim 11, wherein the identified-data extraction module comprises:
<claim-text>a video stream obtaining unit, configured to obtain the video stream data;</claim-text>
<claim-text>an identification length determining unit, configured to determine a video stream identification length; and</claim-text>
<claim-text>an identified-data extraction unit, configured to extract the to-be-identified video data from the<!-- EPO <DP n="27"> --> video stream data based on the video stream identification length.</claim-text></claim-text></claim>
<claim id="c-en-0013" num="0013">
<claim-text>The apparatus according to claim 11, wherein the marking recognition processing module comprises:
<claim-text>an identification determining unit, configured to determine identification information of a business person corresponding to the to-be-identified video data;</claim-text>
<claim-text>a recognition model query unit, configured to query a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information;</claim-text>
<claim-text>a feature data extraction unit, configured to extract image feature data from the image data, and extract audio feature data from the audio data; and</claim-text>
<claim-text>a marking recognition unit, configured to input the image feature data into the marking behavior recognition model to obtain a marking behavior recognition result, and input the audio feature data into the marking speech recognition model to obtain a marking speech recognition result.</claim-text></claim-text></claim>
<claim id="c-en-0014" num="0014">
<claim-text>The apparatus according to claim 13, wherein the apparatus further comprises:
<claim-text>a historical data obtaining module, configured to obtain historical behavior image data and historical marking speech data from a business system;</claim-text>
<claim-text>a historical data classification module, configured to classify the historical behavior image data and the historical marking speech data by business person to obtain historical behavior image data corresponding to each business person and historical marking speech data corresponding to each business person;</claim-text>
<claim-text>a behavior recognition model training module, configured to train the historical behavior image data corresponding to each business person to obtain the marking behavior recognition model; and</claim-text>
<claim-text>a speech recognition model training module, configured to train the historical marking speech data corresponding to each business person to obtain the marking speech recognition model.</claim-text></claim-text></claim>
<claim id="c-en-0015" num="0015">
<claim-text>A computer device, comprising a memory and one or more processors, wherein the memory stores a computer-readable instruction, and when the computer-readable instruction is<!-- EPO <DP n="28"> --> executed by the one or more processors, the one or more processors are enabled to perform the following steps:
<claim-text>extracting to-be-identified video data from video stream data, and extracting image data and audio data from the to-be-identified video data;</claim-text>
<claim-text>inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result;</claim-text>
<claim-text>obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule;</claim-text>
<claim-text>adding a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and</claim-text>
<claim-text>performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</claim-text></claim-text></claim>
<claim id="c-en-0016" num="0016">
<claim-text>The computer device according to claim 15, wherein when executing the computer-readable instruction, the processor further performs the following steps:
<claim-text>obtaining the video stream data;</claim-text>
<claim-text>determining a video stream identification length; and</claim-text>
<claim-text>extracting the to-be-identified video data from the video stream data based on the video stream identification length.</claim-text></claim-text></claim>
<claim id="c-en-0017" num="0017">
<claim-text>The computer device according to claim 15, wherein when executing the computer-readable instruction, the processor further performs the following steps:
<claim-text>determining identification information of a business person corresponding to the to-be-identified video data;</claim-text>
<claim-text>querying a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information;</claim-text>
<claim-text>extracting image feature data from the image data, and extracting audio feature data from the audio data; and<!-- EPO <DP n="29"> --></claim-text>
<claim-text>inputting the image feature data into the marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio feature data into the marking speech recognition model to obtain a marking speech recognition result.</claim-text></claim-text></claim>
<claim id="c-en-0018" num="0018">
<claim-text>One or more non-volatile computer-readable storage media that store a computer-readable instruction, wherein when the computer-readable instruction is executed by one or more processors, the one or more processors are enabled to perform the following steps:
<claim-text>extracting to-be-identified video data from video stream data, and extracting image data and audio data from the to-be-identified video data;</claim-text>
<claim-text>inputting the image data into a preset marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio data into a preset marking speech recognition model to obtain a marking speech recognition result;</claim-text>
<claim-text>obtaining a marking recognition result based on the marking behavior recognition result, the marking speech recognition result, and a preset marking triggering rule;</claim-text>
<claim-text>adding a cutting point identifier to the to-be-identified video data when a type of the marking recognition result is a marking operation; and</claim-text>
<claim-text>performing cutting processing on the video stream data based on the cutting point identifier to obtain video segment data.</claim-text></claim-text></claim>
<claim id="c-en-0019" num="0019">
<claim-text>The storage medium according to claim 18, wherein when executing the computer-readable instruction, the processor further performs the following steps:
<claim-text>obtaining the video stream data;</claim-text>
<claim-text>determining a video stream identification length; and</claim-text>
<claim-text>extracting the to-be-identified video data from the video stream data based on the video stream identification length.</claim-text></claim-text></claim>
<claim id="c-en-0020" num="0020">
<claim-text>The storage medium according to claim 18, wherein when executing the computer-readable instruction, the processor further performs the following steps:
<claim-text>determining identification information of a business person corresponding to the to-be-identified video data;<!-- EPO <DP n="30"> --></claim-text>
<claim-text>querying a preset marking behavior recognition model and a preset marking speech recognition model separately corresponding to the identification information;</claim-text>
<claim-text>extracting image feature data from the image data, and extracting audio feature data from the audio data; and</claim-text>
<claim-text>inputting the image feature data into the marking behavior recognition model to obtain a marking behavior recognition result, and inputting the audio feature data into the marking speech recognition model to obtain a marking speech recognition result.</claim-text></claim-text></claim>
</claims>
<drawings id="draw" lang="en"><!-- EPO <DP n="31"> -->
<figure id="f0001" num="1,2"><img id="if0001" file="imgf0001.tif" wi="157" he="171" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="32"> -->
<figure id="f0002" num="3"><img id="if0002" file="imgf0002.tif" wi="165" he="86" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> -->
<figure id="f0003" num="4"><img id="if0003" file="imgf0003.tif" wi="165" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> -->
<figure id="f0004" num="5,6"><img id="if0004" file="imgf0004.tif" wi="89" he="207" img-content="drawing" img-format="tif"/></figure>
</drawings>
<search-report-data id="srep" lang="en" srep-office="EP" date-produced=""><doc-page id="srep0001" file="srep0001.tif" wi="161" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="161" he="233" type="tif"/><doc-page id="srep0003" file="srep0003.tif" wi="161" he="233" type="tif"/></search-report-data>
<ep-reference-list id="ref-list">
<heading id="ref-h0001"><b>REFERENCES CITED IN THE DESCRIPTION</b></heading>
<p id="ref-p0001" num=""><i>This list of references cited by the applicant is for the reader's convenience only. It does not form part of the European patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be excluded and the EPO disclaims all liability in this regard.</i></p>
<heading id="ref-h0002"><b>Patent documents cited in the description</b></heading>
<p id="ref-p0002" num="">
<ul id="ref-ul0001" list-style="bullet">
<li><patcit id="ref-pcit0001" dnum="CN201811536818"><document-id><country>CN</country><doc-number>201811536818</doc-number><date>20181214</date></document-id></patcit><crossref idref="pcit0001">[0001]</crossref></li>
</ul></p>
</ep-reference-list>
</ep-patent-document>
